{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Seungjae Lee (이승재)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    We use elemental PyTorch to implement linear regression here. However, in most actual applications, abstractions such as <code>nn.Module</code> or <code>nn.Linear</code> are used.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(x) = Wx + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ cost(W, b) = \\frac{1}{m} \\sum^m_{i=1} \\left( H(x^{(i)}) - y^{(i)} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - $H(x)$: 주어진 $x$ 값에 대해 예측을 어떻게 할 것인가\n",
    " - $cost(W, b)$: $H(x)$ 가 $y$ 를 얼마나 잘 예측했는가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x77dfe45eac70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use fake data for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 PyTorch는 NCHW 형태이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1, requires_grad=True)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "b = torch.zeros(1, requires_grad=True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(x) = Wx + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ cost(W, b) = \\frac{1}{m} \\sum^m_{i=1} \\left( H(x^{(i)}) - y^{(i)} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.],\n",
      "        [-2.],\n",
      "        [-3.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis - y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [4.],\n",
      "        [9.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((hypothesis - y_train) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m optimizer.zero_grad()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mcost\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/PyTorch/.venv/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/PyTorch/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/PyTorch/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1764], requires_grad=True)\n",
      "tensor([0.0755], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the hypothesis is now better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1333],\n",
      "        [0.2267],\n",
      "        [0.3200]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6927, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Full Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, we will be training on the dataset for multiple epochs. This can be done simply with loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.093, b: 0.040 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.873, b: 0.289 Cost: 0.012043\n",
      "Epoch  200/1000 W: 0.900, b: 0.227 Cost: 0.007442\n",
      "Epoch  300/1000 W: 0.921, b: 0.179 Cost: 0.004598\n",
      "Epoch  400/1000 W: 0.938, b: 0.140 Cost: 0.002842\n",
      "Epoch  500/1000 W: 0.951, b: 0.110 Cost: 0.001756\n",
      "Epoch  600/1000 W: 0.962, b: 0.087 Cost: 0.001085\n",
      "Epoch  700/1000 W: 0.970, b: 0.068 Cost: 0.000670\n",
      "Epoch  800/1000 W: 0.976, b: 0.054 Cost: 0.000414\n",
      "Epoch  900/1000 W: 0.981, b: 0.042 Cost: 0.000256\n",
      "Epoch 1000/1000 W: 0.985, b: 0.033 Cost: 0.000158\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level Implementation with `nn.Module`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we had this fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 linear regression 모델을 만들면 되는데, 기본적으로 PyTorch의 모든 모델은 제공되는 `nn.Module`을 inherit 해서 만들게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 `__init__`에서는 사용할 레이어들을 정의하게 됩니다. 여기서 우리는 linear regression 모델을 만들기 때문에, `nn.Linear` 를 이용할 것입니다. 그리고 `forward`에서는 이 모델이 어떻게 입력값에서 출력값을 계산하는지 알려줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 생성해서 예측값 $H(x)$를 구해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0739],\n",
      "        [0.5891],\n",
      "        [1.1044]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 mean squared error (MSE) 로 cost를 구해보자. MSE 역시 PyTorch에서 기본적으로 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0739],\n",
      "        [0.5891],\n",
      "        [1.1044]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = F.mse_loss(hypothesis, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1471, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 주어진 cost를 이용해 $H(x)$ 의 $W, b$ 를 바꾸어서 cost를 줄여봅니다. 이때 PyTorch의 `torch.optim` 에 있는 `optimizer` 들 중 하나를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Full Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Linear Regression 코드를 이해했으니, 실제로 코드를 돌려 피팅시켜보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.101, b: 0.508 Cost: 4.630286\n",
      "Epoch  100/1000 W: 0.713, b: 0.653 Cost: 0.061555\n",
      "Epoch  200/1000 W: 0.774, b: 0.514 Cost: 0.038037\n",
      "Epoch  300/1000 W: 0.822, b: 0.404 Cost: 0.023505\n",
      "Epoch  400/1000 W: 0.860, b: 0.317 Cost: 0.014525\n",
      "Epoch  500/1000 W: 0.890, b: 0.250 Cost: 0.008975\n",
      "Epoch  600/1000 W: 0.914, b: 0.196 Cost: 0.005546\n",
      "Epoch  700/1000 W: 0.932, b: 0.154 Cost: 0.003427\n",
      "Epoch  800/1000 W: 0.947, b: 0.121 Cost: 0.002118\n",
      "Epoch  900/1000 W: 0.958, b: 0.095 Cost: 0.001309\n",
      "Epoch 1000/1000 W: 0.967, b: 0.075 Cost: 0.000809\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "W=tensor([0.], requires_grad=True)\n",
      "b=tensor([0.], requires_grad=True)\n",
      "=======================0==================\n",
      "hypothesis=\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n",
      "loss=4.666666507720947\n",
      "before backward() : W=tensor([0.], requires_grad=True), W.grad=None, b=tensor([0.], requires_grad=True), b.grad=None\n",
      "after  backward() : W=tensor([0.], requires_grad=True), W.grad=tensor([-9.3333]), b=tensor([0.], requires_grad=True), b.grad=tensor([-4.])\n",
      "after  step() : W=tensor([0.0933], requires_grad=True), W.grad=tensor([-9.3333]), b=tensor([0.0400], requires_grad=True), b.grad=tensor([-4.])\n",
      "=======================1==================\n",
      "hypothesis=\n",
      "tensor([[0.1333],\n",
      "        [0.2267],\n",
      "        [0.3200]], grad_fn=<AddBackward0>)\n",
      "loss=3.6927406787872314\n",
      "before backward() : W=tensor([0.0933], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0400], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.0933], requires_grad=True), W.grad=tensor([-8.3022]), b=tensor([0.0400], requires_grad=True), b.grad=tensor([-3.5467])\n",
      "after  step() : W=tensor([0.1764], requires_grad=True), W.grad=tensor([-8.3022]), b=tensor([0.0755], requires_grad=True), b.grad=tensor([-3.5467])\n",
      "=======================2==================\n",
      "hypothesis=\n",
      "tensor([[0.2518],\n",
      "        [0.4282],\n",
      "        [0.6045]], grad_fn=<AddBackward0>)\n",
      "loss=2.9228851795196533\n",
      "before backward() : W=tensor([0.1764], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0755], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.1764], requires_grad=True), W.grad=tensor([-7.3855]), b=tensor([0.0755], requires_grad=True), b.grad=tensor([-3.1436])\n",
      "after  step() : W=tensor([0.2502], requires_grad=True), W.grad=tensor([-7.3855]), b=tensor([0.1069], requires_grad=True), b.grad=tensor([-3.1436])\n",
      "=======================3==================\n",
      "hypothesis=\n",
      "tensor([[0.3571],\n",
      "        [0.6073],\n",
      "        [0.8575]], grad_fn=<AddBackward0>)\n",
      "loss=2.314336061477661\n",
      "before backward() : W=tensor([0.2502], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1069], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.2502], requires_grad=True), W.grad=tensor([-6.5704]), b=tensor([0.1069], requires_grad=True), b.grad=tensor([-2.7854])\n",
      "after  step() : W=tensor([0.3159], requires_grad=True), W.grad=tensor([-6.5704]), b=tensor([0.1348], requires_grad=True), b.grad=tensor([-2.7854])\n",
      "=======================4==================\n",
      "hypothesis=\n",
      "tensor([[0.4507],\n",
      "        [0.7666],\n",
      "        [1.0825]], grad_fn=<AddBackward0>)\n",
      "loss=1.8332923650741577\n",
      "before backward() : W=tensor([0.3159], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1348], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.3159], requires_grad=True), W.grad=tensor([-5.8458]), b=tensor([0.1348], requires_grad=True), b.grad=tensor([-2.4668])\n",
      "after  step() : W=tensor([0.3744], requires_grad=True), W.grad=tensor([-5.8458]), b=tensor([0.1594], requires_grad=True), b.grad=tensor([-2.4668])\n",
      "=======================5==================\n",
      "hypothesis=\n",
      "tensor([[0.5338],\n",
      "        [0.9082],\n",
      "        [1.2825]], grad_fn=<AddBackward0>)\n",
      "loss=1.4530338048934937\n",
      "before backward() : W=tensor([0.3744], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1594], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.3744], requires_grad=True), W.grad=tensor([-5.2015]), b=tensor([0.1594], requires_grad=True), b.grad=tensor([-2.1837])\n",
      "after  step() : W=tensor([0.4264], requires_grad=True), W.grad=tensor([-5.2015]), b=tensor([0.1813], requires_grad=True), b.grad=tensor([-2.1837])\n",
      "=======================6==================\n",
      "hypothesis=\n",
      "tensor([[0.6076],\n",
      "        [1.0340],\n",
      "        [1.4604]], grad_fn=<AddBackward0>)\n",
      "loss=1.1524409055709839\n",
      "before backward() : W=tensor([0.4264], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1813], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.4264], requires_grad=True), W.grad=tensor([-4.6287]), b=tensor([0.1813], requires_grad=True), b.grad=tensor([-1.9319])\n",
      "after  step() : W=tensor([0.4727], requires_grad=True), W.grad=tensor([-4.6287]), b=tensor([0.2006], requires_grad=True), b.grad=tensor([-1.9319])\n",
      "=======================7==================\n",
      "hypothesis=\n",
      "tensor([[0.6733],\n",
      "        [1.1459],\n",
      "        [1.6186]], grad_fn=<AddBackward0>)\n",
      "loss=0.9148194789886475\n",
      "before backward() : W=tensor([0.4727], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2006], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.4727], requires_grad=True), W.grad=tensor([-4.1194]), b=tensor([0.2006], requires_grad=True), b.grad=tensor([-1.7081])\n",
      "after  step() : W=tensor([0.5139], requires_grad=True), W.grad=tensor([-4.1194]), b=tensor([0.2177], requires_grad=True), b.grad=tensor([-1.7081])\n",
      "=======================8==================\n",
      "hypothesis=\n",
      "tensor([[0.7315],\n",
      "        [1.2454],\n",
      "        [1.7593]], grad_fn=<AddBackward0>)\n",
      "loss=0.7269739508628845\n",
      "before backward() : W=tensor([0.5139], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2177], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.5139], requires_grad=True), W.grad=tensor([-3.6666]), b=tensor([0.2177], requires_grad=True), b.grad=tensor([-1.5092])\n",
      "after  step() : W=tensor([0.5505], requires_grad=True), W.grad=tensor([-3.6666]), b=tensor([0.2328], requires_grad=True), b.grad=tensor([-1.5092])\n",
      "=======================9==================\n",
      "hypothesis=\n",
      "tensor([[0.7833],\n",
      "        [1.3338],\n",
      "        [1.8844]], grad_fn=<AddBackward0>)\n",
      "loss=0.5784736275672913\n",
      "before backward() : W=tensor([0.5505], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2328], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.5505], requires_grad=True), W.grad=tensor([-3.2640]), b=tensor([0.2328], requires_grad=True), b.grad=tensor([-1.3324])\n",
      "after  step() : W=tensor([0.5832], requires_grad=True), W.grad=tensor([-3.2640]), b=tensor([0.2461], requires_grad=True), b.grad=tensor([-1.3324])\n",
      "=======================10==================\n",
      "hypothesis=\n",
      "tensor([[0.8293],\n",
      "        [1.4124],\n",
      "        [1.9956]], grad_fn=<AddBackward0>)\n",
      "loss=0.4610734283924103\n",
      "before backward() : W=tensor([0.5832], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2461], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.5832], requires_grad=True), W.grad=tensor([-2.9061]), b=tensor([0.2461], requires_grad=True), b.grad=tensor([-1.1751])\n",
      "after  step() : W=tensor([0.6122], requires_grad=True), W.grad=tensor([-2.9061]), b=tensor([0.2578], requires_grad=True), b.grad=tensor([-1.1751])\n",
      "=======================11==================\n",
      "hypothesis=\n",
      "tensor([[0.8701],\n",
      "        [1.4823],\n",
      "        [2.0945]], grad_fn=<AddBackward0>)\n",
      "loss=0.36825668811798096\n",
      "before backward() : W=tensor([0.6122], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2578], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.6122], requires_grad=True), W.grad=tensor([-2.5878]), b=tensor([0.2578], requires_grad=True), b.grad=tensor([-1.0354])\n",
      "after  step() : W=tensor([0.6381], requires_grad=True), W.grad=tensor([-2.5878]), b=tensor([0.2682], requires_grad=True), b.grad=tensor([-1.0354])\n",
      "=======================12==================\n",
      "hypothesis=\n",
      "tensor([[0.9063],\n",
      "        [1.5444],\n",
      "        [2.1825]], grad_fn=<AddBackward0>)\n",
      "loss=0.2948717176914215\n",
      "before backward() : W=tensor([0.6381], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2682], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.6381], requires_grad=True), W.grad=tensor([-2.3049]), b=tensor([0.2682], requires_grad=True), b.grad=tensor([-0.9112])\n",
      "after  step() : W=tensor([0.6612], requires_grad=True), W.grad=tensor([-2.3049]), b=tensor([0.2773], requires_grad=True), b.grad=tensor([-0.9112])\n",
      "=======================13==================\n",
      "hypothesis=\n",
      "tensor([[0.9385],\n",
      "        [1.5996],\n",
      "        [2.2608]], grad_fn=<AddBackward0>)\n",
      "loss=0.23684652149677277\n",
      "before backward() : W=tensor([0.6612], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2773], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.6612], requires_grad=True), W.grad=tensor([-2.0533]), b=tensor([0.2773], requires_grad=True), b.grad=tensor([-0.8008])\n",
      "after  step() : W=tensor([0.6817], requires_grad=True), W.grad=tensor([-2.0533]), b=tensor([0.2853], requires_grad=True), b.grad=tensor([-0.8008])\n",
      "=======================14==================\n",
      "hypothesis=\n",
      "tensor([[0.9670],\n",
      "        [1.6487],\n",
      "        [2.3304]], grad_fn=<AddBackward0>)\n",
      "loss=0.19096271693706512\n",
      "before backward() : W=tensor([0.6817], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2853], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.6817], requires_grad=True), W.grad=tensor([-1.8296]), b=tensor([0.2853], requires_grad=True), b.grad=tensor([-0.7026])\n",
      "after  step() : W=tensor([0.7000], requires_grad=True), W.grad=tensor([-1.8296]), b=tensor([0.2923], requires_grad=True), b.grad=tensor([-0.7026])\n",
      "=======================15==================\n",
      "hypothesis=\n",
      "tensor([[0.9923],\n",
      "        [1.6923],\n",
      "        [2.3923]], grad_fn=<AddBackward0>)\n",
      "loss=0.15467612445354462\n",
      "before backward() : W=tensor([0.7000], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2923], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.7000], requires_grad=True), W.grad=tensor([-1.6308]), b=tensor([0.2923], requires_grad=True), b.grad=tensor([-0.6154])\n",
      "after  step() : W=tensor([0.7163], requires_grad=True), W.grad=tensor([-1.6308]), b=tensor([0.2985], requires_grad=True), b.grad=tensor([-0.6154])\n",
      "=======================16==================\n",
      "hypothesis=\n",
      "tensor([[1.0148],\n",
      "        [1.7311],\n",
      "        [2.4474]], grad_fn=<AddBackward0>)\n",
      "loss=0.1259755939245224\n",
      "before backward() : W=tensor([0.7163], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2985], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.7163], requires_grad=True), W.grad=tensor([-1.4539]), b=tensor([0.2985], requires_grad=True), b.grad=tensor([-0.5378])\n",
      "after  step() : W=tensor([0.7308], requires_grad=True), W.grad=tensor([-1.4539]), b=tensor([0.3039], requires_grad=True), b.grad=tensor([-0.5378])\n",
      "=======================17==================\n",
      "hypothesis=\n",
      "tensor([[1.0347],\n",
      "        [1.7655],\n",
      "        [2.4964]], grad_fn=<AddBackward0>)\n",
      "loss=0.10327144712209702\n",
      "before backward() : W=tensor([0.7308], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3039], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.7308], requires_grad=True), W.grad=tensor([-1.2967]), b=tensor([0.3039], requires_grad=True), b.grad=tensor([-0.4689])\n",
      "after  step() : W=tensor([0.7438], requires_grad=True), W.grad=tensor([-1.2967]), b=tensor([0.3086], requires_grad=True), b.grad=tensor([-0.4689])\n",
      "=======================18==================\n",
      "hypothesis=\n",
      "tensor([[1.0524],\n",
      "        [1.7962],\n",
      "        [2.5400]], grad_fn=<AddBackward0>)\n",
      "loss=0.085307277739048\n",
      "before backward() : W=tensor([0.7438], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3086], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.7438], requires_grad=True), W.grad=tensor([-1.1569]), b=tensor([0.3086], requires_grad=True), b.grad=tensor([-0.4077])\n",
      "after  step() : W=tensor([0.7554], requires_grad=True), W.grad=tensor([-1.1569]), b=tensor([0.3126], requires_grad=True), b.grad=tensor([-0.4077])\n",
      "=======================19==================\n",
      "hypothesis=\n",
      "tensor([[1.0680],\n",
      "        [1.8234],\n",
      "        [2.5788]], grad_fn=<AddBackward0>)\n",
      "loss=0.07108992338180542\n",
      "before backward() : W=tensor([0.7554], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3126], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.7554], requires_grad=True), W.grad=tensor([-1.0327]), b=tensor([0.3126], requires_grad=True), b.grad=tensor([-0.3532])\n",
      "after  step() : W=tensor([0.7657], requires_grad=True), W.grad=tensor([-1.0327]), b=tensor([0.3162], requires_grad=True), b.grad=tensor([-0.3532])\n",
      "=======================20==================\n",
      "hypothesis=\n",
      "tensor([[1.0819],\n",
      "        [1.8476],\n",
      "        [2.6133]], grad_fn=<AddBackward0>)\n",
      "loss=0.05983428657054901\n",
      "before backward() : W=tensor([0.7657], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3162], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.7657], requires_grad=True), W.grad=tensor([-0.9221]), b=tensor([0.3162], requires_grad=True), b.grad=tensor([-0.3049])\n",
      "after  step() : W=tensor([0.7749], requires_grad=True), W.grad=tensor([-0.9221]), b=tensor([0.3192], requires_grad=True), b.grad=tensor([-0.3049])\n",
      "=======================21==================\n",
      "hypothesis=\n",
      "tensor([[1.0941],\n",
      "        [1.8691],\n",
      "        [2.6440]], grad_fn=<AddBackward0>)\n",
      "loss=0.05091985687613487\n",
      "before backward() : W=tensor([0.7749], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3192], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.7749], requires_grad=True), W.grad=tensor([-0.8239]), b=tensor([0.3192], requires_grad=True), b.grad=tensor([-0.2619])\n",
      "after  step() : W=tensor([0.7832], requires_grad=True), W.grad=tensor([-0.8239]), b=tensor([0.3218], requires_grad=True), b.grad=tensor([-0.2619])\n",
      "=======================22==================\n",
      "hypothesis=\n",
      "tensor([[1.1050],\n",
      "        [1.8882],\n",
      "        [2.6713]], grad_fn=<AddBackward0>)\n",
      "loss=0.04385600984096527\n",
      "before backward() : W=tensor([0.7832], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3218], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.7832], requires_grad=True), W.grad=tensor([-0.7365]), b=tensor([0.3218], requires_grad=True), b.grad=tensor([-0.2237])\n",
      "after  step() : W=tensor([0.7905], requires_grad=True), W.grad=tensor([-0.7365]), b=tensor([0.3241], requires_grad=True), b.grad=tensor([-0.2237])\n",
      "=======================23==================\n",
      "hypothesis=\n",
      "tensor([[1.1146],\n",
      "        [1.9051],\n",
      "        [2.6956]], grad_fn=<AddBackward0>)\n",
      "loss=0.03825516626238823\n",
      "before backward() : W=tensor([0.7905], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3241], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.7905], requires_grad=True), W.grad=tensor([-0.6588]), b=tensor([0.3241], requires_grad=True), b.grad=tensor([-0.1898])\n",
      "after  step() : W=tensor([0.7971], requires_grad=True), W.grad=tensor([-0.6588]), b=tensor([0.3260], requires_grad=True), b.grad=tensor([-0.1898])\n",
      "=======================24==================\n",
      "hypothesis=\n",
      "tensor([[1.1231],\n",
      "        [1.9202],\n",
      "        [2.7173]], grad_fn=<AddBackward0>)\n",
      "loss=0.033810753375291824\n",
      "before backward() : W=tensor([0.7971], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3260], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.7971], requires_grad=True), W.grad=tensor([-0.5897]), b=tensor([0.3260], requires_grad=True), b.grad=tensor([-0.1596])\n",
      "after  step() : W=tensor([0.8030], requires_grad=True), W.grad=tensor([-0.5897]), b=tensor([0.3276], requires_grad=True), b.grad=tensor([-0.1596])\n",
      "=======================25==================\n",
      "hypothesis=\n",
      "tensor([[1.1306],\n",
      "        [1.9336],\n",
      "        [2.7366]], grad_fn=<AddBackward0>)\n",
      "loss=0.030280495062470436\n",
      "before backward() : W=tensor([0.8030], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3276], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8030], requires_grad=True), W.grad=tensor([-0.5283]), b=tensor([0.3276], requires_grad=True), b.grad=tensor([-0.1328])\n",
      "after  step() : W=tensor([0.8083], requires_grad=True), W.grad=tensor([-0.5283]), b=tensor([0.3289], requires_grad=True), b.grad=tensor([-0.1328])\n",
      "=======================26==================\n",
      "hypothesis=\n",
      "tensor([[1.1372],\n",
      "        [1.9455],\n",
      "        [2.7538]], grad_fn=<AddBackward0>)\n",
      "loss=0.027472922578454018\n",
      "before backward() : W=tensor([0.8083], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3289], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8083], requires_grad=True), W.grad=tensor([-0.4737]), b=tensor([0.3289], requires_grad=True), b.grad=tensor([-0.1090])\n",
      "after  step() : W=tensor([0.8130], requires_grad=True), W.grad=tensor([-0.4737]), b=tensor([0.3300], requires_grad=True), b.grad=tensor([-0.1090])\n",
      "=======================27==================\n",
      "hypothesis=\n",
      "tensor([[1.1430],\n",
      "        [1.9560],\n",
      "        [2.7691]], grad_fn=<AddBackward0>)\n",
      "loss=0.02523677609860897\n",
      "before backward() : W=tensor([0.8130], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3300], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8130], requires_grad=True), W.grad=tensor([-0.4251]), b=tensor([0.3300], requires_grad=True), b.grad=tensor([-0.0879])\n",
      "after  step() : W=tensor([0.8173], requires_grad=True), W.grad=tensor([-0.4251]), b=tensor([0.3309], requires_grad=True), b.grad=tensor([-0.0879])\n",
      "=======================28==================\n",
      "hypothesis=\n",
      "tensor([[1.1481],\n",
      "        [1.9654],\n",
      "        [2.7827]], grad_fn=<AddBackward0>)\n",
      "loss=0.023452267050743103\n",
      "before backward() : W=tensor([0.8173], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3309], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8173], requires_grad=True), W.grad=tensor([-0.3819]), b=tensor([0.3309], requires_grad=True), b.grad=tensor([-0.0692])\n",
      "after  step() : W=tensor([0.8211], requires_grad=True), W.grad=tensor([-0.3819]), b=tensor([0.3315], requires_grad=True), b.grad=tensor([-0.0692])\n",
      "=======================29==================\n",
      "hypothesis=\n",
      "tensor([[1.1527],\n",
      "        [1.9738],\n",
      "        [2.7949]], grad_fn=<AddBackward0>)\n",
      "loss=0.02202487923204899\n",
      "before backward() : W=tensor([0.8211], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3315], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8211], requires_grad=True), W.grad=tensor([-0.3435]), b=tensor([0.3315], requires_grad=True), b.grad=tensor([-0.0525])\n",
      "after  step() : W=tensor([0.8245], requires_grad=True), W.grad=tensor([-0.3435]), b=tensor([0.3321], requires_grad=True), b.grad=tensor([-0.0525])\n",
      "=======================30==================\n",
      "hypothesis=\n",
      "tensor([[1.1566],\n",
      "        [1.9811],\n",
      "        [2.8057]], grad_fn=<AddBackward0>)\n",
      "loss=0.020879851654171944\n",
      "before backward() : W=tensor([0.8245], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3321], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8245], requires_grad=True), W.grad=tensor([-0.3094]), b=tensor([0.3321], requires_grad=True), b.grad=tensor([-0.0377])\n",
      "after  step() : W=tensor([0.8276], requires_grad=True), W.grad=tensor([-0.3094]), b=tensor([0.3324], requires_grad=True), b.grad=tensor([-0.0377])\n",
      "=======================31==================\n",
      "hypothesis=\n",
      "tensor([[1.1601],\n",
      "        [1.9877],\n",
      "        [2.8153]], grad_fn=<AddBackward0>)\n",
      "loss=0.01995815522968769\n",
      "before backward() : W=tensor([0.8276], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3324], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8276], requires_grad=True), W.grad=tensor([-0.2790]), b=tensor([0.3324], requires_grad=True), b.grad=tensor([-0.0246])\n",
      "after  step() : W=tensor([0.8304], requires_grad=True), W.grad=tensor([-0.2790]), b=tensor([0.3327], requires_grad=True), b.grad=tensor([-0.0246])\n",
      "=======================32==================\n",
      "hypothesis=\n",
      "tensor([[1.1631],\n",
      "        [1.9935],\n",
      "        [2.8240]], grad_fn=<AddBackward0>)\n",
      "loss=0.01921296864748001\n",
      "before backward() : W=tensor([0.8304], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3327], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8304], requires_grad=True), W.grad=tensor([-0.2520]), b=tensor([0.3327], requires_grad=True), b.grad=tensor([-0.0129])\n",
      "after  step() : W=tensor([0.8329], requires_grad=True), W.grad=tensor([-0.2520]), b=tensor([0.3328], requires_grad=True), b.grad=tensor([-0.0129])\n",
      "=======================33==================\n",
      "hypothesis=\n",
      "tensor([[1.1658],\n",
      "        [1.9987],\n",
      "        [2.8316]], grad_fn=<AddBackward0>)\n",
      "loss=0.01860743947327137\n",
      "before backward() : W=tensor([0.8329], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3328], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8329], requires_grad=True), W.grad=tensor([-0.2279]), b=tensor([0.3328], requires_grad=True), b.grad=tensor([-0.0026])\n",
      "after  step() : W=tensor([0.8352], requires_grad=True), W.grad=tensor([-0.2279]), b=tensor([0.3329], requires_grad=True), b.grad=tensor([-0.0026])\n",
      "=======================34==================\n",
      "hypothesis=\n",
      "tensor([[1.1681],\n",
      "        [2.0033],\n",
      "        [2.8385]], grad_fn=<AddBackward0>)\n",
      "loss=0.018112413585186005\n",
      "before backward() : W=tensor([0.8352], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3329], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8352], requires_grad=True), W.grad=tensor([-0.2065]), b=tensor([0.3329], requires_grad=True), b.grad=tensor([0.0066])\n",
      "after  step() : W=tensor([0.8373], requires_grad=True), W.grad=tensor([-0.2065]), b=tensor([0.3328], requires_grad=True), b.grad=tensor([0.0066])\n",
      "=======================35==================\n",
      "hypothesis=\n",
      "tensor([[1.1701],\n",
      "        [2.0074],\n",
      "        [2.8446]], grad_fn=<AddBackward0>)\n",
      "loss=0.01770475134253502\n",
      "before backward() : W=tensor([0.8373], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3328], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8373], requires_grad=True), W.grad=tensor([-0.1875]), b=tensor([0.3328], requires_grad=True), b.grad=tensor([0.0147])\n",
      "after  step() : W=tensor([0.8392], requires_grad=True), W.grad=tensor([-0.1875]), b=tensor([0.3326], requires_grad=True), b.grad=tensor([0.0147])\n",
      "=======================36==================\n",
      "hypothesis=\n",
      "tensor([[1.1718],\n",
      "        [2.0110],\n",
      "        [2.8501]], grad_fn=<AddBackward0>)\n",
      "loss=0.017366236075758934\n",
      "before backward() : W=tensor([0.8392], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3326], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8392], requires_grad=True), W.grad=tensor([-0.1706]), b=tensor([0.3326], requires_grad=True), b.grad=tensor([0.0219])\n",
      "after  step() : W=tensor([0.8409], requires_grad=True), W.grad=tensor([-0.1706]), b=tensor([0.3324], requires_grad=True), b.grad=tensor([0.0219])\n",
      "=======================37==================\n",
      "hypothesis=\n",
      "tensor([[1.1733],\n",
      "        [2.0142],\n",
      "        [2.8550]], grad_fn=<AddBackward0>)\n",
      "loss=0.017082465812563896\n",
      "before backward() : W=tensor([0.8409], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3324], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8409], requires_grad=True), W.grad=tensor([-0.1556]), b=tensor([0.3324], requires_grad=True), b.grad=tensor([0.0283])\n",
      "after  step() : W=tensor([0.8424], requires_grad=True), W.grad=tensor([-0.1556]), b=tensor([0.3321], requires_grad=True), b.grad=tensor([0.0283])\n",
      "=======================38==================\n",
      "hypothesis=\n",
      "tensor([[1.1746],\n",
      "        [2.0170],\n",
      "        [2.8594]], grad_fn=<AddBackward0>)\n",
      "loss=0.016842052340507507\n",
      "before backward() : W=tensor([0.8424], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3321], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8424], requires_grad=True), W.grad=tensor([-0.1422]), b=tensor([0.3321], requires_grad=True), b.grad=tensor([0.0340])\n",
      "after  step() : W=tensor([0.8438], requires_grad=True), W.grad=tensor([-0.1422]), b=tensor([0.3318], requires_grad=True), b.grad=tensor([0.0340])\n",
      "=======================39==================\n",
      "hypothesis=\n",
      "tensor([[1.1756],\n",
      "        [2.0195],\n",
      "        [2.8633]], grad_fn=<AddBackward0>)\n",
      "loss=0.01663600653409958\n",
      "before backward() : W=tensor([0.8438], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3318], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8438], requires_grad=True), W.grad=tensor([-0.1303]), b=tensor([0.3318], requires_grad=True), b.grad=tensor([0.0390])\n",
      "after  step() : W=tensor([0.8451], requires_grad=True), W.grad=tensor([-0.1303]), b=tensor([0.3314], requires_grad=True), b.grad=tensor([0.0390])\n",
      "=======================40==================\n",
      "hypothesis=\n",
      "tensor([[1.1766],\n",
      "        [2.0217],\n",
      "        [2.8668]], grad_fn=<AddBackward0>)\n",
      "loss=0.016457131132483482\n",
      "before backward() : W=tensor([0.8451], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3314], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8451], requires_grad=True), W.grad=tensor([-0.1197]), b=tensor([0.3314], requires_grad=True), b.grad=tensor([0.0434])\n",
      "after  step() : W=tensor([0.8463], requires_grad=True), W.grad=tensor([-0.1197]), b=tensor([0.3310], requires_grad=True), b.grad=tensor([0.0434])\n",
      "=======================41==================\n",
      "hypothesis=\n",
      "tensor([[1.1773],\n",
      "        [2.0237],\n",
      "        [2.8700]], grad_fn=<AddBackward0>)\n",
      "loss=0.016299912706017494\n",
      "before backward() : W=tensor([0.8463], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3310], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8463], requires_grad=True), W.grad=tensor([-0.1102]), b=tensor([0.3310], requires_grad=True), b.grad=tensor([0.0473])\n",
      "after  step() : W=tensor([0.8474], requires_grad=True), W.grad=tensor([-0.1102]), b=tensor([0.3305], requires_grad=True), b.grad=tensor([0.0473])\n",
      "=======================42==================\n",
      "hypothesis=\n",
      "tensor([[1.1779],\n",
      "        [2.0254],\n",
      "        [2.8728]], grad_fn=<AddBackward0>)\n",
      "loss=0.016159800812602043\n",
      "before backward() : W=tensor([0.8474], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3305], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8474], requires_grad=True), W.grad=tensor([-0.1018]), b=tensor([0.3305], requires_grad=True), b.grad=tensor([0.0508])\n",
      "after  step() : W=tensor([0.8485], requires_grad=True), W.grad=tensor([-0.1018]), b=tensor([0.3300], requires_grad=True), b.grad=tensor([0.0508])\n",
      "=======================43==================\n",
      "hypothesis=\n",
      "tensor([[1.1785],\n",
      "        [2.0269],\n",
      "        [2.8754]], grad_fn=<AddBackward0>)\n",
      "loss=0.016033316031098366\n",
      "before backward() : W=tensor([0.8485], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3300], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8485], requires_grad=True), W.grad=tensor([-0.0944]), b=tensor([0.3300], requires_grad=True), b.grad=tensor([0.0538])\n",
      "after  step() : W=tensor([0.8494], requires_grad=True), W.grad=tensor([-0.0944]), b=tensor([0.3295], requires_grad=True), b.grad=tensor([0.0538])\n",
      "=======================44==================\n",
      "hypothesis=\n",
      "tensor([[1.1789],\n",
      "        [2.0283],\n",
      "        [2.8777]], grad_fn=<AddBackward0>)\n",
      "loss=0.015917709097266197\n",
      "before backward() : W=tensor([0.8494], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3295], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8494], requires_grad=True), W.grad=tensor([-0.0877]), b=tensor([0.3295], requires_grad=True), b.grad=tensor([0.0565])\n",
      "after  step() : W=tensor([0.8503], requires_grad=True), W.grad=tensor([-0.0877]), b=tensor([0.3289], requires_grad=True), b.grad=tensor([0.0565])\n",
      "=======================45==================\n",
      "hypothesis=\n",
      "tensor([[1.1792],\n",
      "        [2.0295],\n",
      "        [2.8797]], grad_fn=<AddBackward0>)\n",
      "loss=0.01581072434782982\n",
      "before backward() : W=tensor([0.8503], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3289], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8503], requires_grad=True), W.grad=tensor([-0.0818]), b=tensor([0.3289], requires_grad=True), b.grad=tensor([0.0589])\n",
      "after  step() : W=tensor([0.8511], requires_grad=True), W.grad=tensor([-0.0818]), b=tensor([0.3283], requires_grad=True), b.grad=tensor([0.0589])\n",
      "=======================46==================\n",
      "hypothesis=\n",
      "tensor([[1.1794],\n",
      "        [2.0305],\n",
      "        [2.8816]], grad_fn=<AddBackward0>)\n",
      "loss=0.015710653737187386\n",
      "before backward() : W=tensor([0.8511], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3283], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8511], requires_grad=True), W.grad=tensor([-0.0765]), b=tensor([0.3283], requires_grad=True), b.grad=tensor([0.0610])\n",
      "after  step() : W=tensor([0.8519], requires_grad=True), W.grad=tensor([-0.0765]), b=tensor([0.3277], requires_grad=True), b.grad=tensor([0.0610])\n",
      "=======================47==================\n",
      "hypothesis=\n",
      "tensor([[1.1796],\n",
      "        [2.0314],\n",
      "        [2.8833]], grad_fn=<AddBackward0>)\n",
      "loss=0.015616149641573429\n",
      "before backward() : W=tensor([0.8519], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3277], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8519], requires_grad=True), W.grad=tensor([-0.0718]), b=tensor([0.3277], requires_grad=True), b.grad=tensor([0.0628])\n",
      "after  step() : W=tensor([0.8526], requires_grad=True), W.grad=tensor([-0.0718]), b=tensor([0.3271], requires_grad=True), b.grad=tensor([0.0628])\n",
      "=======================48==================\n",
      "hypothesis=\n",
      "tensor([[1.1796],\n",
      "        [2.0322],\n",
      "        [2.8848]], grad_fn=<AddBackward0>)\n",
      "loss=0.015526079572737217\n",
      "before backward() : W=tensor([0.8526], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3271], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8526], requires_grad=True), W.grad=tensor([-0.0676]), b=tensor([0.3271], requires_grad=True), b.grad=tensor([0.0645])\n",
      "after  step() : W=tensor([0.8533], requires_grad=True), W.grad=tensor([-0.0676]), b=tensor([0.3264], requires_grad=True), b.grad=tensor([0.0645])\n",
      "=======================49==================\n",
      "hypothesis=\n",
      "tensor([[1.1797],\n",
      "        [2.0329],\n",
      "        [2.8862]], grad_fn=<AddBackward0>)\n",
      "loss=0.01543960440903902\n",
      "before backward() : W=tensor([0.8533], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3264], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8533], requires_grad=True), W.grad=tensor([-0.0639]), b=tensor([0.3264], requires_grad=True), b.grad=tensor([0.0659])\n",
      "after  step() : W=tensor([0.8539], requires_grad=True), W.grad=tensor([-0.0639]), b=tensor([0.3258], requires_grad=True), b.grad=tensor([0.0659])\n",
      "=======================50==================\n",
      "hypothesis=\n",
      "tensor([[1.1797],\n",
      "        [2.0336],\n",
      "        [2.8875]], grad_fn=<AddBackward0>)\n",
      "loss=0.015356022864580154\n",
      "before backward() : W=tensor([0.8539], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3258], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8539], requires_grad=True), W.grad=tensor([-0.0606]), b=tensor([0.3258], requires_grad=True), b.grad=tensor([0.0671])\n",
      "after  step() : W=tensor([0.8545], requires_grad=True), W.grad=tensor([-0.0606]), b=tensor([0.3251], requires_grad=True), b.grad=tensor([0.0671])\n",
      "=======================51==================\n",
      "hypothesis=\n",
      "tensor([[1.1796],\n",
      "        [2.0341],\n",
      "        [2.8886]], grad_fn=<AddBackward0>)\n",
      "loss=0.015274837613105774\n",
      "before backward() : W=tensor([0.8545], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3251], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8545], requires_grad=True), W.grad=tensor([-0.0576]), b=tensor([0.3251], requires_grad=True), b.grad=tensor([0.0682])\n",
      "after  step() : W=tensor([0.8551], requires_grad=True), W.grad=tensor([-0.0576]), b=tensor([0.3244], requires_grad=True), b.grad=tensor([0.0682])\n",
      "=======================52==================\n",
      "hypothesis=\n",
      "tensor([[1.1795],\n",
      "        [2.0346],\n",
      "        [2.8897]], grad_fn=<AddBackward0>)\n",
      "loss=0.015195600688457489\n",
      "before backward() : W=tensor([0.8551], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3244], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8551], requires_grad=True), W.grad=tensor([-0.0549]), b=tensor([0.3244], requires_grad=True), b.grad=tensor([0.0691])\n",
      "after  step() : W=tensor([0.8556], requires_grad=True), W.grad=tensor([-0.0549]), b=tensor([0.3237], requires_grad=True), b.grad=tensor([0.0691])\n",
      "=======================53==================\n",
      "hypothesis=\n",
      "tensor([[1.1793],\n",
      "        [2.0350],\n",
      "        [2.8906]], grad_fn=<AddBackward0>)\n",
      "loss=0.015117984265089035\n",
      "before backward() : W=tensor([0.8556], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3237], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8556], requires_grad=True), W.grad=tensor([-0.0526]), b=tensor([0.3237], requires_grad=True), b.grad=tensor([0.0700])\n",
      "after  step() : W=tensor([0.8562], requires_grad=True), W.grad=tensor([-0.0526]), b=tensor([0.3230], requires_grad=True), b.grad=tensor([0.0700])\n",
      "=======================54==================\n",
      "hypothesis=\n",
      "tensor([[1.1792],\n",
      "        [2.0353],\n",
      "        [2.8915]], grad_fn=<AddBackward0>)\n",
      "loss=0.015041700564324856\n",
      "before backward() : W=tensor([0.8562], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3230], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8562], requires_grad=True), W.grad=tensor([-0.0505]), b=tensor([0.3230], requires_grad=True), b.grad=tensor([0.0707])\n",
      "after  step() : W=tensor([0.8567], requires_grad=True), W.grad=tensor([-0.0505]), b=tensor([0.3223], requires_grad=True), b.grad=tensor([0.0707])\n",
      "=======================55==================\n",
      "hypothesis=\n",
      "tensor([[1.1790],\n",
      "        [2.0356],\n",
      "        [2.8923]], grad_fn=<AddBackward0>)\n",
      "loss=0.014966552145779133\n",
      "before backward() : W=tensor([0.8567], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3223], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8567], requires_grad=True), W.grad=tensor([-0.0486]), b=tensor([0.3223], requires_grad=True), b.grad=tensor([0.0713])\n",
      "after  step() : W=tensor([0.8571], requires_grad=True), W.grad=tensor([-0.0486]), b=tensor([0.3216], requires_grad=True), b.grad=tensor([0.0713])\n",
      "=======================56==================\n",
      "hypothesis=\n",
      "tensor([[1.1787],\n",
      "        [2.0359],\n",
      "        [2.8930]], grad_fn=<AddBackward0>)\n",
      "loss=0.014892381615936756\n",
      "before backward() : W=tensor([0.8571], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3216], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8571], requires_grad=True), W.grad=tensor([-0.0469]), b=tensor([0.3216], requires_grad=True), b.grad=tensor([0.0718])\n",
      "after  step() : W=tensor([0.8576], requires_grad=True), W.grad=tensor([-0.0469]), b=tensor([0.3209], requires_grad=True), b.grad=tensor([0.0718])\n",
      "=======================57==================\n",
      "hypothesis=\n",
      "tensor([[1.1785],\n",
      "        [2.0361],\n",
      "        [2.8937]], grad_fn=<AddBackward0>)\n",
      "loss=0.01481907069683075\n",
      "before backward() : W=tensor([0.8576], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3209], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8576], requires_grad=True), W.grad=tensor([-0.0454]), b=tensor([0.3209], requires_grad=True), b.grad=tensor([0.0722])\n",
      "after  step() : W=tensor([0.8581], requires_grad=True), W.grad=tensor([-0.0454]), b=tensor([0.3202], requires_grad=True), b.grad=tensor([0.0722])\n",
      "=======================58==================\n",
      "hypothesis=\n",
      "tensor([[1.1782],\n",
      "        [2.0363],\n",
      "        [2.8944]], grad_fn=<AddBackward0>)\n",
      "loss=0.014746460132300854\n",
      "before backward() : W=tensor([0.8581], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3202], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8581], requires_grad=True), W.grad=tensor([-0.0440]), b=tensor([0.3202], requires_grad=True), b.grad=tensor([0.0726])\n",
      "after  step() : W=tensor([0.8585], requires_grad=True), W.grad=tensor([-0.0440]), b=tensor([0.3194], requires_grad=True), b.grad=tensor([0.0726])\n",
      "=======================59==================\n",
      "hypothesis=\n",
      "tensor([[1.1779],\n",
      "        [2.0365],\n",
      "        [2.8950]], grad_fn=<AddBackward0>)\n",
      "loss=0.014674502424895763\n",
      "before backward() : W=tensor([0.8585], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3194], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8585], requires_grad=True), W.grad=tensor([-0.0428]), b=tensor([0.3194], requires_grad=True), b.grad=tensor([0.0729])\n",
      "after  step() : W=tensor([0.8589], requires_grad=True), W.grad=tensor([-0.0428]), b=tensor([0.3187], requires_grad=True), b.grad=tensor([0.0729])\n",
      "=======================60==================\n",
      "hypothesis=\n",
      "tensor([[1.1776],\n",
      "        [2.0366],\n",
      "        [2.8955]], grad_fn=<AddBackward0>)\n",
      "loss=0.014603150077164173\n",
      "before backward() : W=tensor([0.8589], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3187], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8589], requires_grad=True), W.grad=tensor([-0.0418]), b=tensor([0.3187], requires_grad=True), b.grad=tensor([0.0732])\n",
      "after  step() : W=tensor([0.8594], requires_grad=True), W.grad=tensor([-0.0418]), b=tensor([0.3180], requires_grad=True), b.grad=tensor([0.0732])\n",
      "=======================61==================\n",
      "hypothesis=\n",
      "tensor([[1.1773],\n",
      "        [2.0367],\n",
      "        [2.8960]], grad_fn=<AddBackward0>)\n",
      "loss=0.014532312750816345\n",
      "before backward() : W=tensor([0.8594], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3180], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8594], requires_grad=True), W.grad=tensor([-0.0408]), b=tensor([0.3180], requires_grad=True), b.grad=tensor([0.0734])\n",
      "after  step() : W=tensor([0.8598], requires_grad=True), W.grad=tensor([-0.0408]), b=tensor([0.3172], requires_grad=True), b.grad=tensor([0.0734])\n",
      "=======================62==================\n",
      "hypothesis=\n",
      "tensor([[1.1770],\n",
      "        [2.0368],\n",
      "        [2.8965]], grad_fn=<AddBackward0>)\n",
      "loss=0.014461967162787914\n",
      "before backward() : W=tensor([0.8598], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3172], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8598], requires_grad=True), W.grad=tensor([-0.0399]), b=tensor([0.3172], requires_grad=True), b.grad=tensor([0.0735])\n",
      "after  step() : W=tensor([0.8602], requires_grad=True), W.grad=tensor([-0.0399]), b=tensor([0.3165], requires_grad=True), b.grad=tensor([0.0735])\n",
      "=======================63==================\n",
      "hypothesis=\n",
      "tensor([[1.1767],\n",
      "        [2.0368],\n",
      "        [2.8970]], grad_fn=<AddBackward0>)\n",
      "loss=0.014392085373401642\n",
      "before backward() : W=tensor([0.8602], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3165], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8602], requires_grad=True), W.grad=tensor([-0.0391]), b=tensor([0.3165], requires_grad=True), b.grad=tensor([0.0737])\n",
      "after  step() : W=tensor([0.8606], requires_grad=True), W.grad=tensor([-0.0391]), b=tensor([0.3158], requires_grad=True), b.grad=tensor([0.0737])\n",
      "=======================64==================\n",
      "hypothesis=\n",
      "tensor([[1.1763],\n",
      "        [2.0369],\n",
      "        [2.8974]], grad_fn=<AddBackward0>)\n",
      "loss=0.014322616159915924\n",
      "before backward() : W=tensor([0.8606], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3158], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8606], requires_grad=True), W.grad=tensor([-0.0384]), b=tensor([0.3158], requires_grad=True), b.grad=tensor([0.0737])\n",
      "after  step() : W=tensor([0.8609], requires_grad=True), W.grad=tensor([-0.0384]), b=tensor([0.3150], requires_grad=True), b.grad=tensor([0.0737])\n",
      "=======================65==================\n",
      "hypothesis=\n",
      "tensor([[1.1760],\n",
      "        [2.0369],\n",
      "        [2.8978]], grad_fn=<AddBackward0>)\n",
      "loss=0.014253559522330761\n",
      "before backward() : W=tensor([0.8609], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3150], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8609], requires_grad=True), W.grad=tensor([-0.0378]), b=tensor([0.3150], requires_grad=True), b.grad=tensor([0.0738])\n",
      "after  step() : W=tensor([0.8613], requires_grad=True), W.grad=tensor([-0.0378]), b=tensor([0.3143], requires_grad=True), b.grad=tensor([0.0738])\n",
      "=======================66==================\n",
      "hypothesis=\n",
      "tensor([[1.1756],\n",
      "        [2.0369],\n",
      "        [2.8982]], grad_fn=<AddBackward0>)\n",
      "loss=0.014184891246259212\n",
      "before backward() : W=tensor([0.8613], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3143], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8613], requires_grad=True), W.grad=tensor([-0.0372]), b=tensor([0.3143], requires_grad=True), b.grad=tensor([0.0738])\n",
      "after  step() : W=tensor([0.8617], requires_grad=True), W.grad=tensor([-0.0372]), b=tensor([0.3135], requires_grad=True), b.grad=tensor([0.0738])\n",
      "=======================67==================\n",
      "hypothesis=\n",
      "tensor([[1.1752],\n",
      "        [2.0369],\n",
      "        [2.8986]], grad_fn=<AddBackward0>)\n",
      "loss=0.014116608537733555\n",
      "before backward() : W=tensor([0.8617], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3135], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8617], requires_grad=True), W.grad=tensor([-0.0367]), b=tensor([0.3135], requires_grad=True), b.grad=tensor([0.0739])\n",
      "after  step() : W=tensor([0.8621], requires_grad=True), W.grad=tensor([-0.0367]), b=tensor([0.3128], requires_grad=True), b.grad=tensor([0.0739])\n",
      "=======================68==================\n",
      "hypothesis=\n",
      "tensor([[1.1749],\n",
      "        [2.0369],\n",
      "        [2.8990]], grad_fn=<AddBackward0>)\n",
      "loss=0.0140486815944314\n",
      "before backward() : W=tensor([0.8621], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3128], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8621], requires_grad=True), W.grad=tensor([-0.0362]), b=tensor([0.3128], requires_grad=True), b.grad=tensor([0.0738])\n",
      "after  step() : W=tensor([0.8624], requires_grad=True), W.grad=tensor([-0.0362]), b=tensor([0.3121], requires_grad=True), b.grad=tensor([0.0738])\n",
      "=======================69==================\n",
      "hypothesis=\n",
      "tensor([[1.1745],\n",
      "        [2.0369],\n",
      "        [2.8993]], grad_fn=<AddBackward0>)\n",
      "loss=0.0139811085537076\n",
      "before backward() : W=tensor([0.8624], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3121], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8624], requires_grad=True), W.grad=tensor([-0.0358]), b=tensor([0.3121], requires_grad=True), b.grad=tensor([0.0738])\n",
      "after  step() : W=tensor([0.8628], requires_grad=True), W.grad=tensor([-0.0358]), b=tensor([0.3113], requires_grad=True), b.grad=tensor([0.0738])\n",
      "=======================70==================\n",
      "hypothesis=\n",
      "tensor([[1.1741],\n",
      "        [2.0369],\n",
      "        [2.8997]], grad_fn=<AddBackward0>)\n",
      "loss=0.013913878239691257\n",
      "before backward() : W=tensor([0.8628], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3113], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8628], requires_grad=True), W.grad=tensor([-0.0354]), b=tensor([0.3113], requires_grad=True), b.grad=tensor([0.0738])\n",
      "after  step() : W=tensor([0.8631], requires_grad=True), W.grad=tensor([-0.0354]), b=tensor([0.3106], requires_grad=True), b.grad=tensor([0.0738])\n",
      "=======================71==================\n",
      "hypothesis=\n",
      "tensor([[1.1737],\n",
      "        [2.0369],\n",
      "        [2.9000]], grad_fn=<AddBackward0>)\n",
      "loss=0.0138469859957695\n",
      "before backward() : W=tensor([0.8631], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3106], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8631], requires_grad=True), W.grad=tensor([-0.0351]), b=tensor([0.3106], requires_grad=True), b.grad=tensor([0.0737])\n",
      "after  step() : W=tensor([0.8635], requires_grad=True), W.grad=tensor([-0.0351]), b=tensor([0.3099], requires_grad=True), b.grad=tensor([0.0737])\n",
      "=======================72==================\n",
      "hypothesis=\n",
      "tensor([[1.1733],\n",
      "        [2.0368],\n",
      "        [2.9003]], grad_fn=<AddBackward0>)\n",
      "loss=0.0137804439291358\n",
      "before backward() : W=tensor([0.8635], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3099], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8635], requires_grad=True), W.grad=tensor([-0.0347]), b=tensor([0.3099], requires_grad=True), b.grad=tensor([0.0736])\n",
      "after  step() : W=tensor([0.8638], requires_grad=True), W.grad=tensor([-0.0347]), b=tensor([0.3091], requires_grad=True), b.grad=tensor([0.0736])\n",
      "=======================73==================\n",
      "hypothesis=\n",
      "tensor([[1.1730],\n",
      "        [2.0368],\n",
      "        [2.9006]], grad_fn=<AddBackward0>)\n",
      "loss=0.013714239001274109\n",
      "before backward() : W=tensor([0.8638], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3091], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8638], requires_grad=True), W.grad=tensor([-0.0344]), b=tensor([0.3091], requires_grad=True), b.grad=tensor([0.0736])\n",
      "after  step() : W=tensor([0.8642], requires_grad=True), W.grad=tensor([-0.0344]), b=tensor([0.3084], requires_grad=True), b.grad=tensor([0.0736])\n",
      "=======================74==================\n",
      "hypothesis=\n",
      "tensor([[1.1726],\n",
      "        [2.0367],\n",
      "        [2.9009]], grad_fn=<AddBackward0>)\n",
      "loss=0.01364834513515234\n",
      "before backward() : W=tensor([0.8642], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3084], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8642], requires_grad=True), W.grad=tensor([-0.0342]), b=tensor([0.3084], requires_grad=True), b.grad=tensor([0.0735])\n",
      "after  step() : W=tensor([0.8645], requires_grad=True), W.grad=tensor([-0.0342]), b=tensor([0.3076], requires_grad=True), b.grad=tensor([0.0735])\n",
      "=======================75==================\n",
      "hypothesis=\n",
      "tensor([[1.1722],\n",
      "        [2.0367],\n",
      "        [2.9012]], grad_fn=<AddBackward0>)\n",
      "loss=0.013582766056060791\n",
      "before backward() : W=tensor([0.8645], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3076], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8645], requires_grad=True), W.grad=tensor([-0.0339]), b=tensor([0.3076], requires_grad=True), b.grad=tensor([0.0734])\n",
      "after  step() : W=tensor([0.8649], requires_grad=True), W.grad=tensor([-0.0339]), b=tensor([0.3069], requires_grad=True), b.grad=tensor([0.0734])\n",
      "=======================76==================\n",
      "hypothesis=\n",
      "tensor([[1.1718],\n",
      "        [2.0366],\n",
      "        [2.9015]], grad_fn=<AddBackward0>)\n",
      "loss=0.01351753156632185\n",
      "before backward() : W=tensor([0.8649], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3069], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8649], requires_grad=True), W.grad=tensor([-0.0337]), b=tensor([0.3069], requires_grad=True), b.grad=tensor([0.0733])\n",
      "after  step() : W=tensor([0.8652], requires_grad=True), W.grad=tensor([-0.0337]), b=tensor([0.3062], requires_grad=True), b.grad=tensor([0.0733])\n",
      "=======================77==================\n",
      "hypothesis=\n",
      "tensor([[1.1714],\n",
      "        [2.0366],\n",
      "        [2.9018]], grad_fn=<AddBackward0>)\n",
      "loss=0.013452605344355106\n",
      "before backward() : W=tensor([0.8652], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3062], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8652], requires_grad=True), W.grad=tensor([-0.0335]), b=tensor([0.3062], requires_grad=True), b.grad=tensor([0.0731])\n",
      "after  step() : W=tensor([0.8655], requires_grad=True), W.grad=tensor([-0.0335]), b=tensor([0.3055], requires_grad=True), b.grad=tensor([0.0731])\n",
      "=======================78==================\n",
      "hypothesis=\n",
      "tensor([[1.1710],\n",
      "        [2.0365],\n",
      "        [2.9020]], grad_fn=<AddBackward0>)\n",
      "loss=0.013387982733547688\n",
      "before backward() : W=tensor([0.8655], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3055], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8655], requires_grad=True), W.grad=tensor([-0.0333]), b=tensor([0.3055], requires_grad=True), b.grad=tensor([0.0730])\n",
      "after  step() : W=tensor([0.8659], requires_grad=True), W.grad=tensor([-0.0333]), b=tensor([0.3047], requires_grad=True), b.grad=tensor([0.0730])\n",
      "=======================79==================\n",
      "hypothesis=\n",
      "tensor([[1.1706],\n",
      "        [2.0364],\n",
      "        [2.9023]], grad_fn=<AddBackward0>)\n",
      "loss=0.013323699124157429\n",
      "before backward() : W=tensor([0.8659], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3047], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8659], requires_grad=True), W.grad=tensor([-0.0331]), b=tensor([0.3047], requires_grad=True), b.grad=tensor([0.0729])\n",
      "after  step() : W=tensor([0.8662], requires_grad=True), W.grad=tensor([-0.0331]), b=tensor([0.3040], requires_grad=True), b.grad=tensor([0.0729])\n",
      "=======================80==================\n",
      "hypothesis=\n",
      "tensor([[1.1702],\n",
      "        [2.0364],\n",
      "        [2.9026]], grad_fn=<AddBackward0>)\n",
      "loss=0.01325971633195877\n",
      "before backward() : W=tensor([0.8662], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3040], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8662], requires_grad=True), W.grad=tensor([-0.0329]), b=tensor([0.3040], requires_grad=True), b.grad=tensor([0.0727])\n",
      "after  step() : W=tensor([0.8665], requires_grad=True), W.grad=tensor([-0.0329]), b=tensor([0.3033], requires_grad=True), b.grad=tensor([0.0727])\n",
      "=======================81==================\n",
      "hypothesis=\n",
      "tensor([[1.1698],\n",
      "        [2.0363],\n",
      "        [2.9028]], grad_fn=<AddBackward0>)\n",
      "loss=0.013196020387113094\n",
      "before backward() : W=tensor([0.8665], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3033], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8665], requires_grad=True), W.grad=tensor([-0.0328]), b=tensor([0.3033], requires_grad=True), b.grad=tensor([0.0726])\n",
      "after  step() : W=tensor([0.8668], requires_grad=True), W.grad=tensor([-0.0328]), b=tensor([0.3025], requires_grad=True), b.grad=tensor([0.0726])\n",
      "=======================82==================\n",
      "hypothesis=\n",
      "tensor([[1.1694],\n",
      "        [2.0362],\n",
      "        [2.9031]], grad_fn=<AddBackward0>)\n",
      "loss=0.013132649473845959\n",
      "before backward() : W=tensor([0.8668], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3025], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8668], requires_grad=True), W.grad=tensor([-0.0326]), b=tensor([0.3025], requires_grad=True), b.grad=tensor([0.0725])\n",
      "after  step() : W=tensor([0.8672], requires_grad=True), W.grad=tensor([-0.0326]), b=tensor([0.3018], requires_grad=True), b.grad=tensor([0.0725])\n",
      "=======================83==================\n",
      "hypothesis=\n",
      "tensor([[1.1690],\n",
      "        [2.0362],\n",
      "        [2.9033]], grad_fn=<AddBackward0>)\n",
      "loss=0.013069580309092999\n",
      "before backward() : W=tensor([0.8672], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3018], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8672], requires_grad=True), W.grad=tensor([-0.0325]), b=tensor([0.3018], requires_grad=True), b.grad=tensor([0.0723])\n",
      "after  step() : W=tensor([0.8675], requires_grad=True), W.grad=tensor([-0.0325]), b=tensor([0.3011], requires_grad=True), b.grad=tensor([0.0723])\n",
      "=======================84==================\n",
      "hypothesis=\n",
      "tensor([[1.1686],\n",
      "        [2.0361],\n",
      "        [2.9036]], grad_fn=<AddBackward0>)\n",
      "loss=0.01300681009888649\n",
      "before backward() : W=tensor([0.8675], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3011], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8675], requires_grad=True), W.grad=tensor([-0.0323]), b=tensor([0.3011], requires_grad=True), b.grad=tensor([0.0722])\n",
      "after  step() : W=tensor([0.8678], requires_grad=True), W.grad=tensor([-0.0323]), b=tensor([0.3004], requires_grad=True), b.grad=tensor([0.0722])\n",
      "=======================85==================\n",
      "hypothesis=\n",
      "tensor([[1.1682],\n",
      "        [2.0360],\n",
      "        [2.9038]], grad_fn=<AddBackward0>)\n",
      "loss=0.012944362126290798\n",
      "before backward() : W=tensor([0.8678], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.3004], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8678], requires_grad=True), W.grad=tensor([-0.0322]), b=tensor([0.3004], requires_grad=True), b.grad=tensor([0.0720])\n",
      "after  step() : W=tensor([0.8681], requires_grad=True), W.grad=tensor([-0.0322]), b=tensor([0.2996], requires_grad=True), b.grad=tensor([0.0720])\n",
      "=======================86==================\n",
      "hypothesis=\n",
      "tensor([[1.1678],\n",
      "        [2.0359],\n",
      "        [2.9041]], grad_fn=<AddBackward0>)\n",
      "loss=0.012882198207080364\n",
      "before backward() : W=tensor([0.8681], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2996], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8681], requires_grad=True), W.grad=tensor([-0.0321]), b=tensor([0.2996], requires_grad=True), b.grad=tensor([0.0719])\n",
      "after  step() : W=tensor([0.8685], requires_grad=True), W.grad=tensor([-0.0321]), b=tensor([0.2989], requires_grad=True), b.grad=tensor([0.0719])\n",
      "=======================87==================\n",
      "hypothesis=\n",
      "tensor([[1.1674],\n",
      "        [2.0359],\n",
      "        [2.9043]], grad_fn=<AddBackward0>)\n",
      "loss=0.012820329517126083\n",
      "before backward() : W=tensor([0.8685], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2989], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8685], requires_grad=True), W.grad=tensor([-0.0319]), b=tensor([0.2989], requires_grad=True), b.grad=tensor([0.0717])\n",
      "after  step() : W=tensor([0.8688], requires_grad=True), W.grad=tensor([-0.0319]), b=tensor([0.2982], requires_grad=True), b.grad=tensor([0.0717])\n",
      "=======================88==================\n",
      "hypothesis=\n",
      "tensor([[1.1670],\n",
      "        [2.0358],\n",
      "        [2.9046]], grad_fn=<AddBackward0>)\n",
      "loss=0.012758764438331127\n",
      "before backward() : W=tensor([0.8688], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2982], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8688], requires_grad=True), W.grad=tensor([-0.0318]), b=tensor([0.2982], requires_grad=True), b.grad=tensor([0.0716])\n",
      "after  step() : W=tensor([0.8691], requires_grad=True), W.grad=tensor([-0.0318]), b=tensor([0.2975], requires_grad=True), b.grad=tensor([0.0716])\n",
      "=======================89==================\n",
      "hypothesis=\n",
      "tensor([[1.1666],\n",
      "        [2.0357],\n",
      "        [2.9048]], grad_fn=<AddBackward0>)\n",
      "loss=0.012697489000856876\n",
      "before backward() : W=tensor([0.8691], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2975], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8691], requires_grad=True), W.grad=tensor([-0.0317]), b=tensor([0.2975], requires_grad=True), b.grad=tensor([0.0714])\n",
      "after  step() : W=tensor([0.8694], requires_grad=True), W.grad=tensor([-0.0317]), b=tensor([0.2968], requires_grad=True), b.grad=tensor([0.0714])\n",
      "=======================90==================\n",
      "hypothesis=\n",
      "tensor([[1.1662],\n",
      "        [2.0356],\n",
      "        [2.9050]], grad_fn=<AddBackward0>)\n",
      "loss=0.012636511586606503\n",
      "before backward() : W=tensor([0.8694], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2968], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8694], requires_grad=True), W.grad=tensor([-0.0316]), b=tensor([0.2968], requires_grad=True), b.grad=tensor([0.0712])\n",
      "after  step() : W=tensor([0.8697], requires_grad=True), W.grad=tensor([-0.0316]), b=tensor([0.2961], requires_grad=True), b.grad=tensor([0.0712])\n",
      "=======================91==================\n",
      "hypothesis=\n",
      "tensor([[1.1658],\n",
      "        [2.0355],\n",
      "        [2.9053]], grad_fn=<AddBackward0>)\n",
      "loss=0.012575832195580006\n",
      "before backward() : W=tensor([0.8697], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2961], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8697], requires_grad=True), W.grad=tensor([-0.0315]), b=tensor([0.2961], requires_grad=True), b.grad=tensor([0.0711])\n",
      "after  step() : W=tensor([0.8701], requires_grad=True), W.grad=tensor([-0.0315]), b=tensor([0.2954], requires_grad=True), b.grad=tensor([0.0711])\n",
      "=======================92==================\n",
      "hypothesis=\n",
      "tensor([[1.1654],\n",
      "        [2.0355],\n",
      "        [2.9055]], grad_fn=<AddBackward0>)\n",
      "loss=0.012515433132648468\n",
      "before backward() : W=tensor([0.8701], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2954], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8701], requires_grad=True), W.grad=tensor([-0.0314]), b=tensor([0.2954], requires_grad=True), b.grad=tensor([0.0709])\n",
      "after  step() : W=tensor([0.8704], requires_grad=True), W.grad=tensor([-0.0314]), b=tensor([0.2947], requires_grad=True), b.grad=tensor([0.0709])\n",
      "=======================93==================\n",
      "hypothesis=\n",
      "tensor([[1.1650],\n",
      "        [2.0354],\n",
      "        [2.9057]], grad_fn=<AddBackward0>)\n",
      "loss=0.012455344200134277\n",
      "before backward() : W=tensor([0.8704], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2947], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8704], requires_grad=True), W.grad=tensor([-0.0313]), b=tensor([0.2947], requires_grad=True), b.grad=tensor([0.0708])\n",
      "after  step() : W=tensor([0.8707], requires_grad=True), W.grad=tensor([-0.0313]), b=tensor([0.2939], requires_grad=True), b.grad=tensor([0.0708])\n",
      "=======================94==================\n",
      "hypothesis=\n",
      "tensor([[1.1646],\n",
      "        [2.0353],\n",
      "        [2.9060]], grad_fn=<AddBackward0>)\n",
      "loss=0.012395528145134449\n",
      "before backward() : W=tensor([0.8707], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2939], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8707], requires_grad=True), W.grad=tensor([-0.0312]), b=tensor([0.2939], requires_grad=True), b.grad=tensor([0.0706])\n",
      "after  step() : W=tensor([0.8710], requires_grad=True), W.grad=tensor([-0.0312]), b=tensor([0.2932], requires_grad=True), b.grad=tensor([0.0706])\n",
      "=======================95==================\n",
      "hypothesis=\n",
      "tensor([[1.1642],\n",
      "        [2.0352],\n",
      "        [2.9062]], grad_fn=<AddBackward0>)\n",
      "loss=0.012336008250713348\n",
      "before backward() : W=tensor([0.8710], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2932], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8710], requires_grad=True), W.grad=tensor([-0.0311]), b=tensor([0.2932], requires_grad=True), b.grad=tensor([0.0704])\n",
      "after  step() : W=tensor([0.8713], requires_grad=True), W.grad=tensor([-0.0311]), b=tensor([0.2925], requires_grad=True), b.grad=tensor([0.0704])\n",
      "=======================96==================\n",
      "hypothesis=\n",
      "tensor([[1.1638],\n",
      "        [2.0351],\n",
      "        [2.9064]], grad_fn=<AddBackward0>)\n",
      "loss=0.012276776134967804\n",
      "before backward() : W=tensor([0.8713], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2925], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8713], requires_grad=True), W.grad=tensor([-0.0311]), b=tensor([0.2925], requires_grad=True), b.grad=tensor([0.0703])\n",
      "after  step() : W=tensor([0.8716], requires_grad=True), W.grad=tensor([-0.0311]), b=tensor([0.2918], requires_grad=True), b.grad=tensor([0.0703])\n",
      "=======================97==================\n",
      "hypothesis=\n",
      "tensor([[1.1634],\n",
      "        [2.0351],\n",
      "        [2.9067]], grad_fn=<AddBackward0>)\n",
      "loss=0.01221780851483345\n",
      "before backward() : W=tensor([0.8716], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2918], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8716], requires_grad=True), W.grad=tensor([-0.0310]), b=tensor([0.2918], requires_grad=True), b.grad=tensor([0.0701])\n",
      "after  step() : W=tensor([0.8719], requires_grad=True), W.grad=tensor([-0.0310]), b=tensor([0.2911], requires_grad=True), b.grad=tensor([0.0701])\n",
      "=======================98==================\n",
      "hypothesis=\n",
      "tensor([[1.1631],\n",
      "        [2.0350],\n",
      "        [2.9069]], grad_fn=<AddBackward0>)\n",
      "loss=0.012159156613051891\n",
      "before backward() : W=tensor([0.8719], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2911], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8719], requires_grad=True), W.grad=tensor([-0.0309]), b=tensor([0.2911], requires_grad=True), b.grad=tensor([0.0699])\n",
      "after  step() : W=tensor([0.8722], requires_grad=True), W.grad=tensor([-0.0309]), b=tensor([0.2904], requires_grad=True), b.grad=tensor([0.0699])\n",
      "=======================99==================\n",
      "hypothesis=\n",
      "tensor([[1.1627],\n",
      "        [2.0349],\n",
      "        [2.9071]], grad_fn=<AddBackward0>)\n",
      "loss=0.012100748717784882\n",
      "before backward() : W=tensor([0.8722], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2904], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8722], requires_grad=True), W.grad=tensor([-0.0308]), b=tensor([0.2904], requires_grad=True), b.grad=tensor([0.0698])\n",
      "after  step() : W=tensor([0.8725], requires_grad=True), W.grad=tensor([-0.0308]), b=tensor([0.2897], requires_grad=True), b.grad=tensor([0.0698])\n",
      "=======================100==================\n",
      "hypothesis=\n",
      "tensor([[1.1623],\n",
      "        [2.0348],\n",
      "        [2.9073]], grad_fn=<AddBackward0>)\n",
      "loss=0.012042650021612644\n",
      "before backward() : W=tensor([0.8725], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2897], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8725], requires_grad=True), W.grad=tensor([-0.0307]), b=tensor([0.2897], requires_grad=True), b.grad=tensor([0.0696])\n",
      "after  step() : W=tensor([0.8728], requires_grad=True), W.grad=tensor([-0.0307]), b=tensor([0.2890], requires_grad=True), b.grad=tensor([0.0696])\n",
      "=======================101==================\n",
      "hypothesis=\n",
      "tensor([[1.1619],\n",
      "        [2.0347],\n",
      "        [2.9076]], grad_fn=<AddBackward0>)\n",
      "loss=0.01198482047766447\n",
      "before backward() : W=tensor([0.8728], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2890], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8728], requires_grad=True), W.grad=tensor([-0.0306]), b=tensor([0.2890], requires_grad=True), b.grad=tensor([0.0695])\n",
      "after  step() : W=tensor([0.8732], requires_grad=True), W.grad=tensor([-0.0306]), b=tensor([0.2883], requires_grad=True), b.grad=tensor([0.0695])\n",
      "=======================102==================\n",
      "hypothesis=\n",
      "tensor([[1.1615],\n",
      "        [2.0346],\n",
      "        [2.9078]], grad_fn=<AddBackward0>)\n",
      "loss=0.011927269399166107\n",
      "before backward() : W=tensor([0.8732], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2883], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8732], requires_grad=True), W.grad=tensor([-0.0305]), b=tensor([0.2883], requires_grad=True), b.grad=tensor([0.0693])\n",
      "after  step() : W=tensor([0.8735], requires_grad=True), W.grad=tensor([-0.0305]), b=tensor([0.2876], requires_grad=True), b.grad=tensor([0.0693])\n",
      "=======================103==================\n",
      "hypothesis=\n",
      "tensor([[1.1611],\n",
      "        [2.0346],\n",
      "        [2.9080]], grad_fn=<AddBackward0>)\n",
      "loss=0.011869986541569233\n",
      "before backward() : W=tensor([0.8735], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2876], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8735], requires_grad=True), W.grad=tensor([-0.0305]), b=tensor([0.2876], requires_grad=True), b.grad=tensor([0.0691])\n",
      "after  step() : W=tensor([0.8738], requires_grad=True), W.grad=tensor([-0.0305]), b=tensor([0.2870], requires_grad=True), b.grad=tensor([0.0691])\n",
      "=======================104==================\n",
      "hypothesis=\n",
      "tensor([[1.1607],\n",
      "        [2.0345],\n",
      "        [2.9082]], grad_fn=<AddBackward0>)\n",
      "loss=0.01181297842413187\n",
      "before backward() : W=tensor([0.8738], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2870], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8738], requires_grad=True), W.grad=tensor([-0.0304]), b=tensor([0.2870], requires_grad=True), b.grad=tensor([0.0690])\n",
      "after  step() : W=tensor([0.8741], requires_grad=True), W.grad=tensor([-0.0304]), b=tensor([0.2863], requires_grad=True), b.grad=tensor([0.0690])\n",
      "=======================105==================\n",
      "hypothesis=\n",
      "tensor([[1.1603],\n",
      "        [2.0344],\n",
      "        [2.9085]], grad_fn=<AddBackward0>)\n",
      "loss=0.011756268329918385\n",
      "before backward() : W=tensor([0.8741], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2863], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8741], requires_grad=True), W.grad=tensor([-0.0303]), b=tensor([0.2863], requires_grad=True), b.grad=tensor([0.0688])\n",
      "after  step() : W=tensor([0.8744], requires_grad=True), W.grad=tensor([-0.0303]), b=tensor([0.2856], requires_grad=True), b.grad=tensor([0.0688])\n",
      "=======================106==================\n",
      "hypothesis=\n",
      "tensor([[1.1599],\n",
      "        [2.0343],\n",
      "        [2.9087]], grad_fn=<AddBackward0>)\n",
      "loss=0.011699799448251724\n",
      "before backward() : W=tensor([0.8744], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2856], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8744], requires_grad=True), W.grad=tensor([-0.0302]), b=tensor([0.2856], requires_grad=True), b.grad=tensor([0.0686])\n",
      "after  step() : W=tensor([0.8747], requires_grad=True), W.grad=tensor([-0.0302]), b=tensor([0.2849], requires_grad=True), b.grad=tensor([0.0686])\n",
      "=======================107==================\n",
      "hypothesis=\n",
      "tensor([[1.1596],\n",
      "        [2.0342],\n",
      "        [2.9089]], grad_fn=<AddBackward0>)\n",
      "loss=0.01164360623806715\n",
      "before backward() : W=tensor([0.8747], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2849], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8747], requires_grad=True), W.grad=tensor([-0.0302]), b=tensor([0.2849], requires_grad=True), b.grad=tensor([0.0685])\n",
      "after  step() : W=tensor([0.8750], requires_grad=True), W.grad=tensor([-0.0302]), b=tensor([0.2842], requires_grad=True), b.grad=tensor([0.0685])\n",
      "=======================108==================\n",
      "hypothesis=\n",
      "tensor([[1.1592],\n",
      "        [2.0342],\n",
      "        [2.9091]], grad_fn=<AddBackward0>)\n",
      "loss=0.011587705463171005\n",
      "before backward() : W=tensor([0.8750], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2842], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8750], requires_grad=True), W.grad=tensor([-0.0301]), b=tensor([0.2842], requires_grad=True), b.grad=tensor([0.0683])\n",
      "after  step() : W=tensor([0.8753], requires_grad=True), W.grad=tensor([-0.0301]), b=tensor([0.2835], requires_grad=True), b.grad=tensor([0.0683])\n",
      "=======================109==================\n",
      "hypothesis=\n",
      "tensor([[1.1588],\n",
      "        [2.0341],\n",
      "        [2.9093]], grad_fn=<AddBackward0>)\n",
      "loss=0.011532068252563477\n",
      "before backward() : W=tensor([0.8753], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2835], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8753], requires_grad=True), W.grad=tensor([-0.0300]), b=tensor([0.2835], requires_grad=True), b.grad=tensor([0.0681])\n",
      "after  step() : W=tensor([0.8756], requires_grad=True), W.grad=tensor([-0.0300]), b=tensor([0.2828], requires_grad=True), b.grad=tensor([0.0681])\n",
      "=======================110==================\n",
      "hypothesis=\n",
      "tensor([[1.1584],\n",
      "        [2.0340],\n",
      "        [2.9096]], grad_fn=<AddBackward0>)\n",
      "loss=0.011476690880954266\n",
      "before backward() : W=tensor([0.8756], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2828], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8756], requires_grad=True), W.grad=tensor([-0.0299]), b=tensor([0.2828], requires_grad=True), b.grad=tensor([0.0680])\n",
      "after  step() : W=tensor([0.8759], requires_grad=True), W.grad=tensor([-0.0299]), b=tensor([0.2822], requires_grad=True), b.grad=tensor([0.0680])\n",
      "=======================111==================\n",
      "hypothesis=\n",
      "tensor([[1.1580],\n",
      "        [2.0339],\n",
      "        [2.9098]], grad_fn=<AddBackward0>)\n",
      "loss=0.011421561241149902\n",
      "before backward() : W=tensor([0.8759], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2822], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8759], requires_grad=True), W.grad=tensor([-0.0299]), b=tensor([0.2822], requires_grad=True), b.grad=tensor([0.0678])\n",
      "after  step() : W=tensor([0.8762], requires_grad=True), W.grad=tensor([-0.0299]), b=tensor([0.2815], requires_grad=True), b.grad=tensor([0.0678])\n",
      "=======================112==================\n",
      "hypothesis=\n",
      "tensor([[1.1577],\n",
      "        [2.0338],\n",
      "        [2.9100]], grad_fn=<AddBackward0>)\n",
      "loss=0.011366728693246841\n",
      "before backward() : W=tensor([0.8762], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2815], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8762], requires_grad=True), W.grad=tensor([-0.0298]), b=tensor([0.2815], requires_grad=True), b.grad=tensor([0.0677])\n",
      "after  step() : W=tensor([0.8765], requires_grad=True), W.grad=tensor([-0.0298]), b=tensor([0.2808], requires_grad=True), b.grad=tensor([0.0677])\n",
      "=======================113==================\n",
      "hypothesis=\n",
      "tensor([[1.1573],\n",
      "        [2.0337],\n",
      "        [2.9102]], grad_fn=<AddBackward0>)\n",
      "loss=0.011312148533761501\n",
      "before backward() : W=tensor([0.8765], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2808], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8765], requires_grad=True), W.grad=tensor([-0.0297]), b=tensor([0.2808], requires_grad=True), b.grad=tensor([0.0675])\n",
      "after  step() : W=tensor([0.8768], requires_grad=True), W.grad=tensor([-0.0297]), b=tensor([0.2801], requires_grad=True), b.grad=tensor([0.0675])\n",
      "=======================114==================\n",
      "hypothesis=\n",
      "tensor([[1.1569],\n",
      "        [2.0337],\n",
      "        [2.9104]], grad_fn=<AddBackward0>)\n",
      "loss=0.011257815174758434\n",
      "before backward() : W=tensor([0.8768], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2801], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8768], requires_grad=True), W.grad=tensor([-0.0296]), b=tensor([0.2801], requires_grad=True), b.grad=tensor([0.0673])\n",
      "after  step() : W=tensor([0.8771], requires_grad=True), W.grad=tensor([-0.0296]), b=tensor([0.2795], requires_grad=True), b.grad=tensor([0.0673])\n",
      "=======================115==================\n",
      "hypothesis=\n",
      "tensor([[1.1565],\n",
      "        [2.0336],\n",
      "        [2.9107]], grad_fn=<AddBackward0>)\n",
      "loss=0.011203758418560028\n",
      "before backward() : W=tensor([0.8771], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2795], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8771], requires_grad=True), W.grad=tensor([-0.0296]), b=tensor([0.2795], requires_grad=True), b.grad=tensor([0.0672])\n",
      "after  step() : W=tensor([0.8774], requires_grad=True), W.grad=tensor([-0.0296]), b=tensor([0.2788], requires_grad=True), b.grad=tensor([0.0672])\n",
      "=======================116==================\n",
      "hypothesis=\n",
      "tensor([[1.1561],\n",
      "        [2.0335],\n",
      "        [2.9109]], grad_fn=<AddBackward0>)\n",
      "loss=0.011149962432682514\n",
      "before backward() : W=tensor([0.8774], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2788], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8774], requires_grad=True), W.grad=tensor([-0.0295]), b=tensor([0.2788], requires_grad=True), b.grad=tensor([0.0670])\n",
      "after  step() : W=tensor([0.8777], requires_grad=True), W.grad=tensor([-0.0295]), b=tensor([0.2781], requires_grad=True), b.grad=tensor([0.0670])\n",
      "=======================117==================\n",
      "hypothesis=\n",
      "tensor([[1.1558],\n",
      "        [2.0334],\n",
      "        [2.9111]], grad_fn=<AddBackward0>)\n",
      "loss=0.011096407659351826\n",
      "before backward() : W=tensor([0.8777], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2781], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8777], requires_grad=True), W.grad=tensor([-0.0294]), b=tensor([0.2781], requires_grad=True), b.grad=tensor([0.0669])\n",
      "after  step() : W=tensor([0.8779], requires_grad=True), W.grad=tensor([-0.0294]), b=tensor([0.2775], requires_grad=True), b.grad=tensor([0.0669])\n",
      "=======================118==================\n",
      "hypothesis=\n",
      "tensor([[1.1554],\n",
      "        [2.0333],\n",
      "        [2.9113]], grad_fn=<AddBackward0>)\n",
      "loss=0.011043122969567776\n",
      "before backward() : W=tensor([0.8779], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2775], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8779], requires_grad=True), W.grad=tensor([-0.0293]), b=tensor([0.2775], requires_grad=True), b.grad=tensor([0.0667])\n",
      "after  step() : W=tensor([0.8782], requires_grad=True), W.grad=tensor([-0.0293]), b=tensor([0.2768], requires_grad=True), b.grad=tensor([0.0667])\n",
      "=======================119==================\n",
      "hypothesis=\n",
      "tensor([[1.1550],\n",
      "        [2.0333],\n",
      "        [2.9115]], grad_fn=<AddBackward0>)\n",
      "loss=0.010990094393491745\n",
      "before backward() : W=tensor([0.8782], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2768], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8782], requires_grad=True), W.grad=tensor([-0.0293]), b=tensor([0.2768], requires_grad=True), b.grad=tensor([0.0665])\n",
      "after  step() : W=tensor([0.8785], requires_grad=True), W.grad=tensor([-0.0293]), b=tensor([0.2761], requires_grad=True), b.grad=tensor([0.0665])\n",
      "=======================120==================\n",
      "hypothesis=\n",
      "tensor([[1.1547],\n",
      "        [2.0332],\n",
      "        [2.9117]], grad_fn=<AddBackward0>)\n",
      "loss=0.01093731727451086\n",
      "before backward() : W=tensor([0.8785], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2761], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8785], requires_grad=True), W.grad=tensor([-0.0292]), b=tensor([0.2761], requires_grad=True), b.grad=tensor([0.0664])\n",
      "after  step() : W=tensor([0.8788], requires_grad=True), W.grad=tensor([-0.0292]), b=tensor([0.2755], requires_grad=True), b.grad=tensor([0.0664])\n",
      "=======================121==================\n",
      "hypothesis=\n",
      "tensor([[1.1543],\n",
      "        [2.0331],\n",
      "        [2.9119]], grad_fn=<AddBackward0>)\n",
      "loss=0.010884805582463741\n",
      "before backward() : W=tensor([0.8788], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2755], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8788], requires_grad=True), W.grad=tensor([-0.0291]), b=tensor([0.2755], requires_grad=True), b.grad=tensor([0.0662])\n",
      "after  step() : W=tensor([0.8791], requires_grad=True), W.grad=tensor([-0.0291]), b=tensor([0.2748], requires_grad=True), b.grad=tensor([0.0662])\n",
      "=======================122==================\n",
      "hypothesis=\n",
      "tensor([[1.1539],\n",
      "        [2.0330],\n",
      "        [2.9121]], grad_fn=<AddBackward0>)\n",
      "loss=0.010832540690898895\n",
      "before backward() : W=tensor([0.8791], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2748], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8791], requires_grad=True), W.grad=tensor([-0.0291]), b=tensor([0.2748], requires_grad=True), b.grad=tensor([0.0661])\n",
      "after  step() : W=tensor([0.8794], requires_grad=True), W.grad=tensor([-0.0291]), b=tensor([0.2741], requires_grad=True), b.grad=tensor([0.0661])\n",
      "=======================123==================\n",
      "hypothesis=\n",
      "tensor([[1.1535],\n",
      "        [2.0329],\n",
      "        [2.9124]], grad_fn=<AddBackward0>)\n",
      "loss=0.01078051421791315\n",
      "before backward() : W=tensor([0.8794], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2741], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8794], requires_grad=True), W.grad=tensor([-0.0290]), b=tensor([0.2741], requires_grad=True), b.grad=tensor([0.0659])\n",
      "after  step() : W=tensor([0.8797], requires_grad=True), W.grad=tensor([-0.0290]), b=tensor([0.2735], requires_grad=True), b.grad=tensor([0.0659])\n",
      "=======================124==================\n",
      "hypothesis=\n",
      "tensor([[1.1532],\n",
      "        [2.0329],\n",
      "        [2.9126]], grad_fn=<AddBackward0>)\n",
      "loss=0.010728745721280575\n",
      "before backward() : W=tensor([0.8797], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2735], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8797], requires_grad=True), W.grad=tensor([-0.0289]), b=tensor([0.2735], requires_grad=True), b.grad=tensor([0.0657])\n",
      "after  step() : W=tensor([0.8800], requires_grad=True), W.grad=tensor([-0.0289]), b=tensor([0.2728], requires_grad=True), b.grad=tensor([0.0657])\n",
      "=======================125==================\n",
      "hypothesis=\n",
      "tensor([[1.1528],\n",
      "        [2.0328],\n",
      "        [2.9128]], grad_fn=<AddBackward0>)\n",
      "loss=0.01067721750587225\n",
      "before backward() : W=tensor([0.8800], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2728], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8800], requires_grad=True), W.grad=tensor([-0.0289]), b=tensor([0.2728], requires_grad=True), b.grad=tensor([0.0656])\n",
      "after  step() : W=tensor([0.8803], requires_grad=True), W.grad=tensor([-0.0289]), b=tensor([0.2722], requires_grad=True), b.grad=tensor([0.0656])\n",
      "=======================126==================\n",
      "hypothesis=\n",
      "tensor([[1.1524],\n",
      "        [2.0327],\n",
      "        [2.9130]], grad_fn=<AddBackward0>)\n",
      "loss=0.010625958442687988\n",
      "before backward() : W=tensor([0.8803], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2722], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8803], requires_grad=True), W.grad=tensor([-0.0288]), b=tensor([0.2722], requires_grad=True), b.grad=tensor([0.0654])\n",
      "after  step() : W=tensor([0.8806], requires_grad=True), W.grad=tensor([-0.0288]), b=tensor([0.2715], requires_grad=True), b.grad=tensor([0.0654])\n",
      "=======================127==================\n",
      "hypothesis=\n",
      "tensor([[1.1521],\n",
      "        [2.0326],\n",
      "        [2.9132]], grad_fn=<AddBackward0>)\n",
      "loss=0.010574921034276485\n",
      "before backward() : W=tensor([0.8806], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2715], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8806], requires_grad=True), W.grad=tensor([-0.0287]), b=tensor([0.2715], requires_grad=True), b.grad=tensor([0.0653])\n",
      "after  step() : W=tensor([0.8809], requires_grad=True), W.grad=tensor([-0.0287]), b=tensor([0.2709], requires_grad=True), b.grad=tensor([0.0653])\n",
      "=======================128==================\n",
      "hypothesis=\n",
      "tensor([[1.1517],\n",
      "        [2.0326],\n",
      "        [2.9134]], grad_fn=<AddBackward0>)\n",
      "loss=0.010524139739573002\n",
      "before backward() : W=tensor([0.8809], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2709], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8809], requires_grad=True), W.grad=tensor([-0.0286]), b=tensor([0.2709], requires_grad=True), b.grad=tensor([0.0651])\n",
      "after  step() : W=tensor([0.8811], requires_grad=True), W.grad=tensor([-0.0286]), b=tensor([0.2702], requires_grad=True), b.grad=tensor([0.0651])\n",
      "=======================129==================\n",
      "hypothesis=\n",
      "tensor([[1.1513],\n",
      "        [2.0325],\n",
      "        [2.9136]], grad_fn=<AddBackward0>)\n",
      "loss=0.010473608039319515\n",
      "before backward() : W=tensor([0.8811], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2702], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8811], requires_grad=True), W.grad=tensor([-0.0286]), b=tensor([0.2702], requires_grad=True), b.grad=tensor([0.0650])\n",
      "after  step() : W=tensor([0.8814], requires_grad=True), W.grad=tensor([-0.0286]), b=tensor([0.2696], requires_grad=True), b.grad=tensor([0.0650])\n",
      "=======================130==================\n",
      "hypothesis=\n",
      "tensor([[1.1510],\n",
      "        [2.0324],\n",
      "        [2.9138]], grad_fn=<AddBackward0>)\n",
      "loss=0.010423312894999981\n",
      "before backward() : W=tensor([0.8814], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2696], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8814], requires_grad=True), W.grad=tensor([-0.0285]), b=tensor([0.2696], requires_grad=True), b.grad=tensor([0.0648])\n",
      "after  step() : W=tensor([0.8817], requires_grad=True), W.grad=tensor([-0.0285]), b=tensor([0.2689], requires_grad=True), b.grad=tensor([0.0648])\n",
      "=======================131==================\n",
      "hypothesis=\n",
      "tensor([[1.1506],\n",
      "        [2.0323],\n",
      "        [2.9140]], grad_fn=<AddBackward0>)\n",
      "loss=0.010373249650001526\n",
      "before backward() : W=tensor([0.8817], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2689], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8817], requires_grad=True), W.grad=tensor([-0.0284]), b=tensor([0.2689], requires_grad=True), b.grad=tensor([0.0646])\n",
      "after  step() : W=tensor([0.8820], requires_grad=True), W.grad=tensor([-0.0284]), b=tensor([0.2683], requires_grad=True), b.grad=tensor([0.0646])\n",
      "=======================132==================\n",
      "hypothesis=\n",
      "tensor([[1.1503],\n",
      "        [2.0322],\n",
      "        [2.9142]], grad_fn=<AddBackward0>)\n",
      "loss=0.010323448106646538\n",
      "before backward() : W=tensor([0.8820], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2683], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8820], requires_grad=True), W.grad=tensor([-0.0284]), b=tensor([0.2683], requires_grad=True), b.grad=tensor([0.0645])\n",
      "after  step() : W=tensor([0.8823], requires_grad=True), W.grad=tensor([-0.0284]), b=tensor([0.2676], requires_grad=True), b.grad=tensor([0.0645])\n",
      "=======================133==================\n",
      "hypothesis=\n",
      "tensor([[1.1499],\n",
      "        [2.0322],\n",
      "        [2.9144]], grad_fn=<AddBackward0>)\n",
      "loss=0.01027387473732233\n",
      "before backward() : W=tensor([0.8823], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2676], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8823], requires_grad=True), W.grad=tensor([-0.0283]), b=tensor([0.2676], requires_grad=True), b.grad=tensor([0.0643])\n",
      "after  step() : W=tensor([0.8826], requires_grad=True), W.grad=tensor([-0.0283]), b=tensor([0.2670], requires_grad=True), b.grad=tensor([0.0643])\n",
      "=======================134==================\n",
      "hypothesis=\n",
      "tensor([[1.1495],\n",
      "        [2.0321],\n",
      "        [2.9146]], grad_fn=<AddBackward0>)\n",
      "loss=0.010224543511867523\n",
      "before backward() : W=tensor([0.8826], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2670], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8826], requires_grad=True), W.grad=tensor([-0.0282]), b=tensor([0.2670], requires_grad=True), b.grad=tensor([0.0642])\n",
      "after  step() : W=tensor([0.8828], requires_grad=True), W.grad=tensor([-0.0282]), b=tensor([0.2663], requires_grad=True), b.grad=tensor([0.0642])\n",
      "=======================135==================\n",
      "hypothesis=\n",
      "tensor([[1.1492],\n",
      "        [2.0320],\n",
      "        [2.9149]], grad_fn=<AddBackward0>)\n",
      "loss=0.010175428353250027\n",
      "before backward() : W=tensor([0.8828], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2663], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8828], requires_grad=True), W.grad=tensor([-0.0282]), b=tensor([0.2663], requires_grad=True), b.grad=tensor([0.0640])\n",
      "after  step() : W=tensor([0.8831], requires_grad=True), W.grad=tensor([-0.0282]), b=tensor([0.2657], requires_grad=True), b.grad=tensor([0.0640])\n",
      "=======================136==================\n",
      "hypothesis=\n",
      "tensor([[1.1488],\n",
      "        [2.0319],\n",
      "        [2.9151]], grad_fn=<AddBackward0>)\n",
      "loss=0.010126559063792229\n",
      "before backward() : W=tensor([0.8831], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2657], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8831], requires_grad=True), W.grad=tensor([-0.0281]), b=tensor([0.2657], requires_grad=True), b.grad=tensor([0.0639])\n",
      "after  step() : W=tensor([0.8834], requires_grad=True), W.grad=tensor([-0.0281]), b=tensor([0.2650], requires_grad=True), b.grad=tensor([0.0639])\n",
      "=======================137==================\n",
      "hypothesis=\n",
      "tensor([[1.1485],\n",
      "        [2.0319],\n",
      "        [2.9153]], grad_fn=<AddBackward0>)\n",
      "loss=0.010077951475977898\n",
      "before backward() : W=tensor([0.8834], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2650], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8834], requires_grad=True), W.grad=tensor([-0.0280]), b=tensor([0.2650], requires_grad=True), b.grad=tensor([0.0637])\n",
      "after  step() : W=tensor([0.8837], requires_grad=True), W.grad=tensor([-0.0280]), b=tensor([0.2644], requires_grad=True), b.grad=tensor([0.0637])\n",
      "=======================138==================\n",
      "hypothesis=\n",
      "tensor([[1.1481],\n",
      "        [2.0318],\n",
      "        [2.9155]], grad_fn=<AddBackward0>)\n",
      "loss=0.010029532015323639\n",
      "before backward() : W=tensor([0.8837], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2644], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8837], requires_grad=True), W.grad=tensor([-0.0280]), b=tensor([0.2644], requires_grad=True), b.grad=tensor([0.0636])\n",
      "after  step() : W=tensor([0.8840], requires_grad=True), W.grad=tensor([-0.0280]), b=tensor([0.2638], requires_grad=True), b.grad=tensor([0.0636])\n",
      "=======================139==================\n",
      "hypothesis=\n",
      "tensor([[1.1477],\n",
      "        [2.0317],\n",
      "        [2.9157]], grad_fn=<AddBackward0>)\n",
      "loss=0.009981376118957996\n",
      "before backward() : W=tensor([0.8840], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2638], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8840], requires_grad=True), W.grad=tensor([-0.0279]), b=tensor([0.2638], requires_grad=True), b.grad=tensor([0.0634])\n",
      "after  step() : W=tensor([0.8842], requires_grad=True), W.grad=tensor([-0.0279]), b=tensor([0.2631], requires_grad=True), b.grad=tensor([0.0634])\n",
      "=======================140==================\n",
      "hypothesis=\n",
      "tensor([[1.1474],\n",
      "        [2.0316],\n",
      "        [2.9159]], grad_fn=<AddBackward0>)\n",
      "loss=0.009933439083397388\n",
      "before backward() : W=tensor([0.8842], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2631], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8842], requires_grad=True), W.grad=tensor([-0.0278]), b=tensor([0.2631], requires_grad=True), b.grad=tensor([0.0633])\n",
      "after  step() : W=tensor([0.8845], requires_grad=True), W.grad=tensor([-0.0278]), b=tensor([0.2625], requires_grad=True), b.grad=tensor([0.0633])\n",
      "=======================141==================\n",
      "hypothesis=\n",
      "tensor([[1.1470],\n",
      "        [2.0316],\n",
      "        [2.9161]], grad_fn=<AddBackward0>)\n",
      "loss=0.009885733015835285\n",
      "before backward() : W=tensor([0.8845], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2625], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8845], requires_grad=True), W.grad=tensor([-0.0278]), b=tensor([0.2625], requires_grad=True), b.grad=tensor([0.0631])\n",
      "after  step() : W=tensor([0.8848], requires_grad=True), W.grad=tensor([-0.0278]), b=tensor([0.2619], requires_grad=True), b.grad=tensor([0.0631])\n",
      "=======================142==================\n",
      "hypothesis=\n",
      "tensor([[1.1467],\n",
      "        [2.0315],\n",
      "        [2.9163]], grad_fn=<AddBackward0>)\n",
      "loss=0.00983827281743288\n",
      "before backward() : W=tensor([0.8848], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2619], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8848], requires_grad=True), W.grad=tensor([-0.0277]), b=tensor([0.2619], requires_grad=True), b.grad=tensor([0.0630])\n",
      "after  step() : W=tensor([0.8851], requires_grad=True), W.grad=tensor([-0.0277]), b=tensor([0.2612], requires_grad=True), b.grad=tensor([0.0630])\n",
      "=======================143==================\n",
      "hypothesis=\n",
      "tensor([[1.1463],\n",
      "        [2.0314],\n",
      "        [2.9165]], grad_fn=<AddBackward0>)\n",
      "loss=0.00979103147983551\n",
      "before backward() : W=tensor([0.8851], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2612], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8851], requires_grad=True), W.grad=tensor([-0.0276]), b=tensor([0.2612], requires_grad=True), b.grad=tensor([0.0628])\n",
      "after  step() : W=tensor([0.8854], requires_grad=True), W.grad=tensor([-0.0276]), b=tensor([0.2606], requires_grad=True), b.grad=tensor([0.0628])\n",
      "=======================144==================\n",
      "hypothesis=\n",
      "tensor([[1.1460],\n",
      "        [2.0313],\n",
      "        [2.9167]], grad_fn=<AddBackward0>)\n",
      "loss=0.009744010865688324\n",
      "before backward() : W=tensor([0.8854], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2606], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8854], requires_grad=True), W.grad=tensor([-0.0276]), b=tensor([0.2606], requires_grad=True), b.grad=tensor([0.0627])\n",
      "after  step() : W=tensor([0.8856], requires_grad=True), W.grad=tensor([-0.0276]), b=tensor([0.2600], requires_grad=True), b.grad=tensor([0.0627])\n",
      "=======================145==================\n",
      "hypothesis=\n",
      "tensor([[1.1456],\n",
      "        [2.0313],\n",
      "        [2.9169]], grad_fn=<AddBackward0>)\n",
      "loss=0.009697232395410538\n",
      "before backward() : W=tensor([0.8856], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2600], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8856], requires_grad=True), W.grad=tensor([-0.0275]), b=tensor([0.2600], requires_grad=True), b.grad=tensor([0.0625])\n",
      "after  step() : W=tensor([0.8859], requires_grad=True), W.grad=tensor([-0.0275]), b=tensor([0.2594], requires_grad=True), b.grad=tensor([0.0625])\n",
      "=======================146==================\n",
      "hypothesis=\n",
      "tensor([[1.1453],\n",
      "        [2.0312],\n",
      "        [2.9171]], grad_fn=<AddBackward0>)\n",
      "loss=0.009650644846260548\n",
      "before backward() : W=tensor([0.8859], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2594], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8859], requires_grad=True), W.grad=tensor([-0.0274]), b=tensor([0.2594], requires_grad=True), b.grad=tensor([0.0624])\n",
      "after  step() : W=tensor([0.8862], requires_grad=True), W.grad=tensor([-0.0274]), b=tensor([0.2587], requires_grad=True), b.grad=tensor([0.0624])\n",
      "=======================147==================\n",
      "hypothesis=\n",
      "tensor([[1.1449],\n",
      "        [2.0311],\n",
      "        [2.9173]], grad_fn=<AddBackward0>)\n",
      "loss=0.009604309685528278\n",
      "before backward() : W=tensor([0.8862], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2587], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8862], requires_grad=True), W.grad=tensor([-0.0274]), b=tensor([0.2587], requires_grad=True), b.grad=tensor([0.0622])\n",
      "after  step() : W=tensor([0.8865], requires_grad=True), W.grad=tensor([-0.0274]), b=tensor([0.2581], requires_grad=True), b.grad=tensor([0.0622])\n",
      "=======================148==================\n",
      "hypothesis=\n",
      "tensor([[1.1446],\n",
      "        [2.0310],\n",
      "        [2.9175]], grad_fn=<AddBackward0>)\n",
      "loss=0.009558185003697872\n",
      "before backward() : W=tensor([0.8865], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2581], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8865], requires_grad=True), W.grad=tensor([-0.0273]), b=tensor([0.2581], requires_grad=True), b.grad=tensor([0.0621])\n",
      "after  step() : W=tensor([0.8867], requires_grad=True), W.grad=tensor([-0.0273]), b=tensor([0.2575], requires_grad=True), b.grad=tensor([0.0621])\n",
      "=======================149==================\n",
      "hypothesis=\n",
      "tensor([[1.1442],\n",
      "        [2.0310],\n",
      "        [2.9177]], grad_fn=<AddBackward0>)\n",
      "loss=0.009512304328382015\n",
      "before backward() : W=tensor([0.8867], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2575], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8867], requires_grad=True), W.grad=tensor([-0.0272]), b=tensor([0.2575], requires_grad=True), b.grad=tensor([0.0619])\n",
      "after  step() : W=tensor([0.8870], requires_grad=True), W.grad=tensor([-0.0272]), b=tensor([0.2569], requires_grad=True), b.grad=tensor([0.0619])\n",
      "=======================150==================\n",
      "hypothesis=\n",
      "tensor([[1.1439],\n",
      "        [2.0309],\n",
      "        [2.9179]], grad_fn=<AddBackward0>)\n",
      "loss=0.009466618299484253\n",
      "before backward() : W=tensor([0.8870], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2569], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8870], requires_grad=True), W.grad=tensor([-0.0272]), b=tensor([0.2569], requires_grad=True), b.grad=tensor([0.0618])\n",
      "after  step() : W=tensor([0.8873], requires_grad=True), W.grad=tensor([-0.0272]), b=tensor([0.2563], requires_grad=True), b.grad=tensor([0.0618])\n",
      "=======================151==================\n",
      "hypothesis=\n",
      "tensor([[1.1435],\n",
      "        [2.0308],\n",
      "        [2.9181]], grad_fn=<AddBackward0>)\n",
      "loss=0.009421142749488354\n",
      "before backward() : W=tensor([0.8873], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2563], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8873], requires_grad=True), W.grad=tensor([-0.0271]), b=tensor([0.2563], requires_grad=True), b.grad=tensor([0.0616])\n",
      "after  step() : W=tensor([0.8875], requires_grad=True), W.grad=tensor([-0.0271]), b=tensor([0.2557], requires_grad=True), b.grad=tensor([0.0616])\n",
      "=======================152==================\n",
      "hypothesis=\n",
      "tensor([[1.1432],\n",
      "        [2.0307],\n",
      "        [2.9183]], grad_fn=<AddBackward0>)\n",
      "loss=0.009375917725265026\n",
      "before backward() : W=tensor([0.8875], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2557], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8875], requires_grad=True), W.grad=tensor([-0.0270]), b=tensor([0.2557], requires_grad=True), b.grad=tensor([0.0615])\n",
      "after  step() : W=tensor([0.8878], requires_grad=True), W.grad=tensor([-0.0270]), b=tensor([0.2550], requires_grad=True), b.grad=tensor([0.0615])\n",
      "=======================153==================\n",
      "hypothesis=\n",
      "tensor([[1.1428],\n",
      "        [2.0307],\n",
      "        [2.9185]], grad_fn=<AddBackward0>)\n",
      "loss=0.009330893866717815\n",
      "before backward() : W=tensor([0.8878], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2550], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8878], requires_grad=True), W.grad=tensor([-0.0270]), b=tensor([0.2550], requires_grad=True), b.grad=tensor([0.0613])\n",
      "after  step() : W=tensor([0.8881], requires_grad=True), W.grad=tensor([-0.0270]), b=tensor([0.2544], requires_grad=True), b.grad=tensor([0.0613])\n",
      "=======================154==================\n",
      "hypothesis=\n",
      "tensor([[1.1425],\n",
      "        [2.0306],\n",
      "        [2.9187]], grad_fn=<AddBackward0>)\n",
      "loss=0.009286082349717617\n",
      "before backward() : W=tensor([0.8881], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2544], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8881], requires_grad=True), W.grad=tensor([-0.0269]), b=tensor([0.2544], requires_grad=True), b.grad=tensor([0.0612])\n",
      "after  step() : W=tensor([0.8883], requires_grad=True), W.grad=tensor([-0.0269]), b=tensor([0.2538], requires_grad=True), b.grad=tensor([0.0612])\n",
      "=======================155==================\n",
      "hypothesis=\n",
      "tensor([[1.1422],\n",
      "        [2.0305],\n",
      "        [2.9189]], grad_fn=<AddBackward0>)\n",
      "loss=0.009241501800715923\n",
      "before backward() : W=tensor([0.8883], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2538], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8883], requires_grad=True), W.grad=tensor([-0.0268]), b=tensor([0.2538], requires_grad=True), b.grad=tensor([0.0610])\n",
      "after  step() : W=tensor([0.8886], requires_grad=True), W.grad=tensor([-0.0268]), b=tensor([0.2532], requires_grad=True), b.grad=tensor([0.0610])\n",
      "=======================156==================\n",
      "hypothesis=\n",
      "tensor([[1.1418],\n",
      "        [2.0304],\n",
      "        [2.9191]], grad_fn=<AddBackward0>)\n",
      "loss=0.009197107516229153\n",
      "before backward() : W=tensor([0.8886], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2532], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8886], requires_grad=True), W.grad=tensor([-0.0268]), b=tensor([0.2532], requires_grad=True), b.grad=tensor([0.0609])\n",
      "after  step() : W=tensor([0.8889], requires_grad=True), W.grad=tensor([-0.0268]), b=tensor([0.2526], requires_grad=True), b.grad=tensor([0.0609])\n",
      "=======================157==================\n",
      "hypothesis=\n",
      "tensor([[1.1415],\n",
      "        [2.0304],\n",
      "        [2.9192]], grad_fn=<AddBackward0>)\n",
      "loss=0.009152941405773163\n",
      "before backward() : W=tensor([0.8889], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2526], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8889], requires_grad=True), W.grad=tensor([-0.0267]), b=tensor([0.2526], requires_grad=True), b.grad=tensor([0.0607])\n",
      "after  step() : W=tensor([0.8892], requires_grad=True), W.grad=tensor([-0.0267]), b=tensor([0.2520], requires_grad=True), b.grad=tensor([0.0607])\n",
      "=======================158==================\n",
      "hypothesis=\n",
      "tensor([[1.1411],\n",
      "        [2.0303],\n",
      "        [2.9194]], grad_fn=<AddBackward0>)\n",
      "loss=0.00910897459834814\n",
      "before backward() : W=tensor([0.8892], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2520], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8892], requires_grad=True), W.grad=tensor([-0.0266]), b=tensor([0.2520], requires_grad=True), b.grad=tensor([0.0606])\n",
      "after  step() : W=tensor([0.8894], requires_grad=True), W.grad=tensor([-0.0266]), b=tensor([0.2514], requires_grad=True), b.grad=tensor([0.0606])\n",
      "=======================159==================\n",
      "hypothesis=\n",
      "tensor([[1.1408],\n",
      "        [2.0302],\n",
      "        [2.9196]], grad_fn=<AddBackward0>)\n",
      "loss=0.009065239690244198\n",
      "before backward() : W=tensor([0.8894], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2514], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8894], requires_grad=True), W.grad=tensor([-0.0266]), b=tensor([0.2514], requires_grad=True), b.grad=tensor([0.0604])\n",
      "after  step() : W=tensor([0.8897], requires_grad=True), W.grad=tensor([-0.0266]), b=tensor([0.2508], requires_grad=True), b.grad=tensor([0.0604])\n",
      "=======================160==================\n",
      "hypothesis=\n",
      "tensor([[1.1405],\n",
      "        [2.0301],\n",
      "        [2.9198]], grad_fn=<AddBackward0>)\n",
      "loss=0.009021717123687267\n",
      "before backward() : W=tensor([0.8897], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2508], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8897], requires_grad=True), W.grad=tensor([-0.0265]), b=tensor([0.2508], requires_grad=True), b.grad=tensor([0.0603])\n",
      "after  step() : W=tensor([0.8899], requires_grad=True), W.grad=tensor([-0.0265]), b=tensor([0.2502], requires_grad=True), b.grad=tensor([0.0603])\n",
      "=======================161==================\n",
      "hypothesis=\n",
      "tensor([[1.1401],\n",
      "        [2.0301],\n",
      "        [2.9200]], grad_fn=<AddBackward0>)\n",
      "loss=0.008978391997516155\n",
      "before backward() : W=tensor([0.8899], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2502], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8899], requires_grad=True), W.grad=tensor([-0.0265]), b=tensor([0.2502], requires_grad=True), b.grad=tensor([0.0601])\n",
      "after  step() : W=tensor([0.8902], requires_grad=True), W.grad=tensor([-0.0265]), b=tensor([0.2496], requires_grad=True), b.grad=tensor([0.0601])\n",
      "=======================162==================\n",
      "hypothesis=\n",
      "tensor([[1.1398],\n",
      "        [2.0300],\n",
      "        [2.9202]], grad_fn=<AddBackward0>)\n",
      "loss=0.008935275487601757\n",
      "before backward() : W=tensor([0.8902], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2496], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8902], requires_grad=True), W.grad=tensor([-0.0264]), b=tensor([0.2496], requires_grad=True), b.grad=tensor([0.0600])\n",
      "after  step() : W=tensor([0.8905], requires_grad=True), W.grad=tensor([-0.0264]), b=tensor([0.2490], requires_grad=True), b.grad=tensor([0.0600])\n",
      "=======================163==================\n",
      "hypothesis=\n",
      "tensor([[1.1394],\n",
      "        [2.0299],\n",
      "        [2.9204]], grad_fn=<AddBackward0>)\n",
      "loss=0.00889237318187952\n",
      "before backward() : W=tensor([0.8905], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2490], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8905], requires_grad=True), W.grad=tensor([-0.0263]), b=tensor([0.2490], requires_grad=True), b.grad=tensor([0.0599])\n",
      "after  step() : W=tensor([0.8907], requires_grad=True), W.grad=tensor([-0.0263]), b=tensor([0.2484], requires_grad=True), b.grad=tensor([0.0599])\n",
      "=======================164==================\n",
      "hypothesis=\n",
      "tensor([[1.1391],\n",
      "        [2.0299],\n",
      "        [2.9206]], grad_fn=<AddBackward0>)\n",
      "loss=0.008849664591252804\n",
      "before backward() : W=tensor([0.8907], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2484], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8907], requires_grad=True), W.grad=tensor([-0.0263]), b=tensor([0.2484], requires_grad=True), b.grad=tensor([0.0597])\n",
      "after  step() : W=tensor([0.8910], requires_grad=True), W.grad=tensor([-0.0263]), b=tensor([0.2478], requires_grad=True), b.grad=tensor([0.0597])\n",
      "=======================165==================\n",
      "hypothesis=\n",
      "tensor([[1.1388],\n",
      "        [2.0298],\n",
      "        [2.9208]], grad_fn=<AddBackward0>)\n",
      "loss=0.00880718044936657\n",
      "before backward() : W=tensor([0.8910], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2478], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8910], requires_grad=True), W.grad=tensor([-0.0262]), b=tensor([0.2478], requires_grad=True), b.grad=tensor([0.0596])\n",
      "after  step() : W=tensor([0.8913], requires_grad=True), W.grad=tensor([-0.0262]), b=tensor([0.2472], requires_grad=True), b.grad=tensor([0.0596])\n",
      "=======================166==================\n",
      "hypothesis=\n",
      "tensor([[1.1384],\n",
      "        [2.0297],\n",
      "        [2.9210]], grad_fn=<AddBackward0>)\n",
      "loss=0.008764876052737236\n",
      "before backward() : W=tensor([0.8913], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2472], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8913], requires_grad=True), W.grad=tensor([-0.0261]), b=tensor([0.2472], requires_grad=True), b.grad=tensor([0.0594])\n",
      "after  step() : W=tensor([0.8915], requires_grad=True), W.grad=tensor([-0.0261]), b=tensor([0.2466], requires_grad=True), b.grad=tensor([0.0594])\n",
      "=======================167==================\n",
      "hypothesis=\n",
      "tensor([[1.1381],\n",
      "        [2.0296],\n",
      "        [2.9212]], grad_fn=<AddBackward0>)\n",
      "loss=0.00872278492897749\n",
      "before backward() : W=tensor([0.8915], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2466], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8915], requires_grad=True), W.grad=tensor([-0.0261]), b=tensor([0.2466], requires_grad=True), b.grad=tensor([0.0593])\n",
      "after  step() : W=tensor([0.8918], requires_grad=True), W.grad=tensor([-0.0261]), b=tensor([0.2460], requires_grad=True), b.grad=tensor([0.0593])\n",
      "=======================168==================\n",
      "hypothesis=\n",
      "tensor([[1.1378],\n",
      "        [2.0296],\n",
      "        [2.9214]], grad_fn=<AddBackward0>)\n",
      "loss=0.008680908940732479\n",
      "before backward() : W=tensor([0.8918], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2460], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8918], requires_grad=True), W.grad=tensor([-0.0260]), b=tensor([0.2460], requires_grad=True), b.grad=tensor([0.0591])\n",
      "after  step() : W=tensor([0.8920], requires_grad=True), W.grad=tensor([-0.0260]), b=tensor([0.2454], requires_grad=True), b.grad=tensor([0.0591])\n",
      "=======================169==================\n",
      "hypothesis=\n",
      "tensor([[1.1374],\n",
      "        [2.0295],\n",
      "        [2.9215]], grad_fn=<AddBackward0>)\n",
      "loss=0.008639226667582989\n",
      "before backward() : W=tensor([0.8920], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2454], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8920], requires_grad=True), W.grad=tensor([-0.0260]), b=tensor([0.2454], requires_grad=True), b.grad=tensor([0.0590])\n",
      "after  step() : W=tensor([0.8923], requires_grad=True), W.grad=tensor([-0.0260]), b=tensor([0.2448], requires_grad=True), b.grad=tensor([0.0590])\n",
      "=======================170==================\n",
      "hypothesis=\n",
      "tensor([[1.1371],\n",
      "        [2.0294],\n",
      "        [2.9217]], grad_fn=<AddBackward0>)\n",
      "loss=0.008597723208367825\n",
      "before backward() : W=tensor([0.8923], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2448], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8923], requires_grad=True), W.grad=tensor([-0.0259]), b=tensor([0.2448], requires_grad=True), b.grad=tensor([0.0589])\n",
      "after  step() : W=tensor([0.8926], requires_grad=True), W.grad=tensor([-0.0259]), b=tensor([0.2442], requires_grad=True), b.grad=tensor([0.0589])\n",
      "=======================171==================\n",
      "hypothesis=\n",
      "tensor([[1.1368],\n",
      "        [2.0294],\n",
      "        [2.9219]], grad_fn=<AddBackward0>)\n",
      "loss=0.008556450717151165\n",
      "before backward() : W=tensor([0.8926], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2442], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8926], requires_grad=True), W.grad=tensor([-0.0258]), b=tensor([0.2442], requires_grad=True), b.grad=tensor([0.0587])\n",
      "after  step() : W=tensor([0.8928], requires_grad=True), W.grad=tensor([-0.0258]), b=tensor([0.2436], requires_grad=True), b.grad=tensor([0.0587])\n",
      "=======================172==================\n",
      "hypothesis=\n",
      "tensor([[1.1365],\n",
      "        [2.0293],\n",
      "        [2.9221]], grad_fn=<AddBackward0>)\n",
      "loss=0.008515345863997936\n",
      "before backward() : W=tensor([0.8928], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2436], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8928], requires_grad=True), W.grad=tensor([-0.0258]), b=tensor([0.2436], requires_grad=True), b.grad=tensor([0.0586])\n",
      "after  step() : W=tensor([0.8931], requires_grad=True), W.grad=tensor([-0.0258]), b=tensor([0.2431], requires_grad=True), b.grad=tensor([0.0586])\n",
      "=======================173==================\n",
      "hypothesis=\n",
      "tensor([[1.1361],\n",
      "        [2.0292],\n",
      "        [2.9223]], grad_fn=<AddBackward0>)\n",
      "loss=0.008474461734294891\n",
      "before backward() : W=tensor([0.8931], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2431], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8931], requires_grad=True), W.grad=tensor([-0.0257]), b=tensor([0.2431], requires_grad=True), b.grad=tensor([0.0584])\n",
      "after  step() : W=tensor([0.8933], requires_grad=True), W.grad=tensor([-0.0257]), b=tensor([0.2425], requires_grad=True), b.grad=tensor([0.0584])\n",
      "=======================174==================\n",
      "hypothesis=\n",
      "tensor([[1.1358],\n",
      "        [2.0291],\n",
      "        [2.9225]], grad_fn=<AddBackward0>)\n",
      "loss=0.008433772251009941\n",
      "before backward() : W=tensor([0.8933], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2425], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8933], requires_grad=True), W.grad=tensor([-0.0256]), b=tensor([0.2425], requires_grad=True), b.grad=tensor([0.0583])\n",
      "after  step() : W=tensor([0.8936], requires_grad=True), W.grad=tensor([-0.0256]), b=tensor([0.2419], requires_grad=True), b.grad=tensor([0.0583])\n",
      "=======================175==================\n",
      "hypothesis=\n",
      "tensor([[1.1355],\n",
      "        [2.0291],\n",
      "        [2.9227]], grad_fn=<AddBackward0>)\n",
      "loss=0.008393274620175362\n",
      "before backward() : W=tensor([0.8936], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2419], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8936], requires_grad=True), W.grad=tensor([-0.0256]), b=tensor([0.2419], requires_grad=True), b.grad=tensor([0.0581])\n",
      "after  step() : W=tensor([0.8939], requires_grad=True), W.grad=tensor([-0.0256]), b=tensor([0.2413], requires_grad=True), b.grad=tensor([0.0581])\n",
      "=======================176==================\n",
      "hypothesis=\n",
      "tensor([[1.1352],\n",
      "        [2.0290],\n",
      "        [2.9229]], grad_fn=<AddBackward0>)\n",
      "loss=0.008352965116500854\n",
      "before backward() : W=tensor([0.8939], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2413], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8939], requires_grad=True), W.grad=tensor([-0.0255]), b=tensor([0.2413], requires_grad=True), b.grad=tensor([0.0580])\n",
      "after  step() : W=tensor([0.8941], requires_grad=True), W.grad=tensor([-0.0255]), b=tensor([0.2407], requires_grad=True), b.grad=tensor([0.0580])\n",
      "=======================177==================\n",
      "hypothesis=\n",
      "tensor([[1.1348],\n",
      "        [2.0289],\n",
      "        [2.9230]], grad_fn=<AddBackward0>)\n",
      "loss=0.008312861435115337\n",
      "before backward() : W=tensor([0.8941], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2407], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8941], requires_grad=True), W.grad=tensor([-0.0255]), b=tensor([0.2407], requires_grad=True), b.grad=tensor([0.0579])\n",
      "after  step() : W=tensor([0.8944], requires_grad=True), W.grad=tensor([-0.0255]), b=tensor([0.2401], requires_grad=True), b.grad=tensor([0.0579])\n",
      "=======================178==================\n",
      "hypothesis=\n",
      "tensor([[1.1345],\n",
      "        [2.0289],\n",
      "        [2.9232]], grad_fn=<AddBackward0>)\n",
      "loss=0.008272926323115826\n",
      "before backward() : W=tensor([0.8944], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2401], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8944], requires_grad=True), W.grad=tensor([-0.0254]), b=tensor([0.2401], requires_grad=True), b.grad=tensor([0.0577])\n",
      "after  step() : W=tensor([0.8946], requires_grad=True), W.grad=tensor([-0.0254]), b=tensor([0.2396], requires_grad=True), b.grad=tensor([0.0577])\n",
      "=======================179==================\n",
      "hypothesis=\n",
      "tensor([[1.1342],\n",
      "        [2.0288],\n",
      "        [2.9234]], grad_fn=<AddBackward0>)\n",
      "loss=0.008233204483985901\n",
      "before backward() : W=tensor([0.8946], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2396], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8946], requires_grad=True), W.grad=tensor([-0.0253]), b=tensor([0.2396], requires_grad=True), b.grad=tensor([0.0576])\n",
      "after  step() : W=tensor([0.8949], requires_grad=True), W.grad=tensor([-0.0253]), b=tensor([0.2390], requires_grad=True), b.grad=tensor([0.0576])\n",
      "=======================180==================\n",
      "hypothesis=\n",
      "tensor([[1.1339],\n",
      "        [2.0287],\n",
      "        [2.9236]], grad_fn=<AddBackward0>)\n",
      "loss=0.008193670772016048\n",
      "before backward() : W=tensor([0.8949], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2390], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8949], requires_grad=True), W.grad=tensor([-0.0253]), b=tensor([0.2390], requires_grad=True), b.grad=tensor([0.0575])\n",
      "after  step() : W=tensor([0.8951], requires_grad=True), W.grad=tensor([-0.0253]), b=tensor([0.2384], requires_grad=True), b.grad=tensor([0.0575])\n",
      "=======================181==================\n",
      "hypothesis=\n",
      "tensor([[1.1335],\n",
      "        [2.0287],\n",
      "        [2.9238]], grad_fn=<AddBackward0>)\n",
      "loss=0.008154318667948246\n",
      "before backward() : W=tensor([0.8951], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2384], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8951], requires_grad=True), W.grad=tensor([-0.0252]), b=tensor([0.2384], requires_grad=True), b.grad=tensor([0.0573])\n",
      "after  step() : W=tensor([0.8954], requires_grad=True), W.grad=tensor([-0.0252]), b=tensor([0.2378], requires_grad=True), b.grad=tensor([0.0573])\n",
      "=======================182==================\n",
      "hypothesis=\n",
      "tensor([[1.1332],\n",
      "        [2.0286],\n",
      "        [2.9240]], grad_fn=<AddBackward0>)\n",
      "loss=0.008115164935588837\n",
      "before backward() : W=tensor([0.8954], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2378], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8954], requires_grad=True), W.grad=tensor([-0.0252]), b=tensor([0.2378], requires_grad=True), b.grad=tensor([0.0572])\n",
      "after  step() : W=tensor([0.8956], requires_grad=True), W.grad=tensor([-0.0252]), b=tensor([0.2373], requires_grad=True), b.grad=tensor([0.0572])\n",
      "=======================183==================\n",
      "hypothesis=\n",
      "tensor([[1.1329],\n",
      "        [2.0285],\n",
      "        [2.9241]], grad_fn=<AddBackward0>)\n",
      "loss=0.00807619746774435\n",
      "before backward() : W=tensor([0.8956], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2373], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8956], requires_grad=True), W.grad=tensor([-0.0251]), b=tensor([0.2373], requires_grad=True), b.grad=tensor([0.0570])\n",
      "after  step() : W=tensor([0.8959], requires_grad=True), W.grad=tensor([-0.0251]), b=tensor([0.2367], requires_grad=True), b.grad=tensor([0.0570])\n",
      "=======================184==================\n",
      "hypothesis=\n",
      "tensor([[1.1326],\n",
      "        [2.0285],\n",
      "        [2.9243]], grad_fn=<AddBackward0>)\n",
      "loss=0.008037413470447063\n",
      "before backward() : W=tensor([0.8959], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2367], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8959], requires_grad=True), W.grad=tensor([-0.0250]), b=tensor([0.2367], requires_grad=True), b.grad=tensor([0.0569])\n",
      "after  step() : W=tensor([0.8961], requires_grad=True), W.grad=tensor([-0.0250]), b=tensor([0.2361], requires_grad=True), b.grad=tensor([0.0569])\n",
      "=======================185==================\n",
      "hypothesis=\n",
      "tensor([[1.1323],\n",
      "        [2.0284],\n",
      "        [2.9245]], grad_fn=<AddBackward0>)\n",
      "loss=0.007998812012374401\n",
      "before backward() : W=tensor([0.8961], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2361], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8961], requires_grad=True), W.grad=tensor([-0.0250]), b=tensor([0.2361], requires_grad=True), b.grad=tensor([0.0568])\n",
      "after  step() : W=tensor([0.8964], requires_grad=True), W.grad=tensor([-0.0250]), b=tensor([0.2356], requires_grad=True), b.grad=tensor([0.0568])\n",
      "=======================186==================\n",
      "hypothesis=\n",
      "tensor([[1.1319],\n",
      "        [2.0283],\n",
      "        [2.9247]], grad_fn=<AddBackward0>)\n",
      "loss=0.007960398681461811\n",
      "before backward() : W=tensor([0.8964], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2356], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8964], requires_grad=True), W.grad=tensor([-0.0249]), b=tensor([0.2356], requires_grad=True), b.grad=tensor([0.0566])\n",
      "after  step() : W=tensor([0.8966], requires_grad=True), W.grad=tensor([-0.0249]), b=tensor([0.2350], requires_grad=True), b.grad=tensor([0.0566])\n",
      "=======================187==================\n",
      "hypothesis=\n",
      "tensor([[1.1316],\n",
      "        [2.0282],\n",
      "        [2.9249]], grad_fn=<AddBackward0>)\n",
      "loss=0.007922175340354443\n",
      "before backward() : W=tensor([0.8966], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2350], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8966], requires_grad=True), W.grad=tensor([-0.0249]), b=tensor([0.2350], requires_grad=True), b.grad=tensor([0.0565])\n",
      "after  step() : W=tensor([0.8969], requires_grad=True), W.grad=tensor([-0.0249]), b=tensor([0.2344], requires_grad=True), b.grad=tensor([0.0565])\n",
      "=======================188==================\n",
      "hypothesis=\n",
      "tensor([[1.1313],\n",
      "        [2.0282],\n",
      "        [2.9251]], grad_fn=<AddBackward0>)\n",
      "loss=0.007884140126407146\n",
      "before backward() : W=tensor([0.8969], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2344], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8969], requires_grad=True), W.grad=tensor([-0.0248]), b=tensor([0.2344], requires_grad=True), b.grad=tensor([0.0564])\n",
      "after  step() : W=tensor([0.8971], requires_grad=True), W.grad=tensor([-0.0248]), b=tensor([0.2339], requires_grad=True), b.grad=tensor([0.0564])\n",
      "=======================189==================\n",
      "hypothesis=\n",
      "tensor([[1.1310],\n",
      "        [2.0281],\n",
      "        [2.9252]], grad_fn=<AddBackward0>)\n",
      "loss=0.007846281863749027\n",
      "before backward() : W=tensor([0.8971], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2339], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8971], requires_grad=True), W.grad=tensor([-0.0247]), b=tensor([0.2339], requires_grad=True), b.grad=tensor([0.0562])\n",
      "after  step() : W=tensor([0.8974], requires_grad=True), W.grad=tensor([-0.0247]), b=tensor([0.2333], requires_grad=True), b.grad=tensor([0.0562])\n",
      "=======================190==================\n",
      "hypothesis=\n",
      "tensor([[1.1307],\n",
      "        [2.0280],\n",
      "        [2.9254]], grad_fn=<AddBackward0>)\n",
      "loss=0.007808599155396223\n",
      "before backward() : W=tensor([0.8974], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2333], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8974], requires_grad=True), W.grad=tensor([-0.0247]), b=tensor([0.2333], requires_grad=True), b.grad=tensor([0.0561])\n",
      "after  step() : W=tensor([0.8976], requires_grad=True), W.grad=tensor([-0.0247]), b=tensor([0.2327], requires_grad=True), b.grad=tensor([0.0561])\n",
      "=======================191==================\n",
      "hypothesis=\n",
      "tensor([[1.1304],\n",
      "        [2.0280],\n",
      "        [2.9256]], grad_fn=<AddBackward0>)\n",
      "loss=0.007771102711558342\n",
      "before backward() : W=tensor([0.8976], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2327], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8976], requires_grad=True), W.grad=tensor([-0.0246]), b=tensor([0.2327], requires_grad=True), b.grad=tensor([0.0560])\n",
      "after  step() : W=tensor([0.8979], requires_grad=True), W.grad=tensor([-0.0246]), b=tensor([0.2322], requires_grad=True), b.grad=tensor([0.0560])\n",
      "=======================192==================\n",
      "hypothesis=\n",
      "tensor([[1.1300],\n",
      "        [2.0279],\n",
      "        [2.9258]], grad_fn=<AddBackward0>)\n",
      "loss=0.007733786478638649\n",
      "before backward() : W=tensor([0.8979], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2322], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8979], requires_grad=True), W.grad=tensor([-0.0246]), b=tensor([0.2322], requires_grad=True), b.grad=tensor([0.0558])\n",
      "after  step() : W=tensor([0.8981], requires_grad=True), W.grad=tensor([-0.0246]), b=tensor([0.2316], requires_grad=True), b.grad=tensor([0.0558])\n",
      "=======================193==================\n",
      "hypothesis=\n",
      "tensor([[1.1297],\n",
      "        [2.0278],\n",
      "        [2.9259]], grad_fn=<AddBackward0>)\n",
      "loss=0.007696649059653282\n",
      "before backward() : W=tensor([0.8981], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2316], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8981], requires_grad=True), W.grad=tensor([-0.0245]), b=tensor([0.2316], requires_grad=True), b.grad=tensor([0.0557])\n",
      "after  step() : W=tensor([0.8984], requires_grad=True), W.grad=tensor([-0.0245]), b=tensor([0.2311], requires_grad=True), b.grad=tensor([0.0557])\n",
      "=======================194==================\n",
      "hypothesis=\n",
      "tensor([[1.1294],\n",
      "        [2.0278],\n",
      "        [2.9261]], grad_fn=<AddBackward0>)\n",
      "loss=0.007659684866666794\n",
      "before backward() : W=tensor([0.8984], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2311], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8984], requires_grad=True), W.grad=tensor([-0.0244]), b=tensor([0.2311], requires_grad=True), b.grad=tensor([0.0555])\n",
      "after  step() : W=tensor([0.8986], requires_grad=True), W.grad=tensor([-0.0244]), b=tensor([0.2305], requires_grad=True), b.grad=tensor([0.0555])\n",
      "=======================195==================\n",
      "hypothesis=\n",
      "tensor([[1.1291],\n",
      "        [2.0277],\n",
      "        [2.9263]], grad_fn=<AddBackward0>)\n",
      "loss=0.007622899953275919\n",
      "before backward() : W=tensor([0.8986], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2305], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8986], requires_grad=True), W.grad=tensor([-0.0244]), b=tensor([0.2305], requires_grad=True), b.grad=tensor([0.0554])\n",
      "after  step() : W=tensor([0.8988], requires_grad=True), W.grad=tensor([-0.0244]), b=tensor([0.2300], requires_grad=True), b.grad=tensor([0.0554])\n",
      "=======================196==================\n",
      "hypothesis=\n",
      "tensor([[1.1288],\n",
      "        [2.0276],\n",
      "        [2.9265]], grad_fn=<AddBackward0>)\n",
      "loss=0.007586291059851646\n",
      "before backward() : W=tensor([0.8988], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2300], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8988], requires_grad=True), W.grad=tensor([-0.0243]), b=tensor([0.2300], requires_grad=True), b.grad=tensor([0.0553])\n",
      "after  step() : W=tensor([0.8991], requires_grad=True), W.grad=tensor([-0.0243]), b=tensor([0.2294], requires_grad=True), b.grad=tensor([0.0553])\n",
      "=======================197==================\n",
      "hypothesis=\n",
      "tensor([[1.1285],\n",
      "        [2.0276],\n",
      "        [2.9267]], grad_fn=<AddBackward0>)\n",
      "loss=0.007549856323748827\n",
      "before backward() : W=tensor([0.8991], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2294], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8991], requires_grad=True), W.grad=tensor([-0.0243]), b=tensor([0.2294], requires_grad=True), b.grad=tensor([0.0551])\n",
      "after  step() : W=tensor([0.8993], requires_grad=True), W.grad=tensor([-0.0243]), b=tensor([0.2289], requires_grad=True), b.grad=tensor([0.0551])\n",
      "=======================198==================\n",
      "hypothesis=\n",
      "tensor([[1.1282],\n",
      "        [2.0275],\n",
      "        [2.9268]], grad_fn=<AddBackward0>)\n",
      "loss=0.007513608783483505\n",
      "before backward() : W=tensor([0.8993], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2289], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8993], requires_grad=True), W.grad=tensor([-0.0242]), b=tensor([0.2289], requires_grad=True), b.grad=tensor([0.0550])\n",
      "after  step() : W=tensor([0.8996], requires_grad=True), W.grad=tensor([-0.0242]), b=tensor([0.2283], requires_grad=True), b.grad=tensor([0.0550])\n",
      "=======================199==================\n",
      "hypothesis=\n",
      "tensor([[1.1279],\n",
      "        [2.0274],\n",
      "        [2.9270]], grad_fn=<AddBackward0>)\n",
      "loss=0.0074775367975234985\n",
      "before backward() : W=tensor([0.8996], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2283], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8996], requires_grad=True), W.grad=tensor([-0.0241]), b=tensor([0.2283], requires_grad=True), b.grad=tensor([0.0549])\n",
      "after  step() : W=tensor([0.8998], requires_grad=True), W.grad=tensor([-0.0241]), b=tensor([0.2278], requires_grad=True), b.grad=tensor([0.0549])\n",
      "=======================200==================\n",
      "hypothesis=\n",
      "tensor([[1.1276],\n",
      "        [2.0274],\n",
      "        [2.9272]], grad_fn=<AddBackward0>)\n",
      "loss=0.007441634312272072\n",
      "before backward() : W=tensor([0.8998], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2278], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.8998], requires_grad=True), W.grad=tensor([-0.0241]), b=tensor([0.2278], requires_grad=True), b.grad=tensor([0.0548])\n",
      "after  step() : W=tensor([0.9000], requires_grad=True), W.grad=tensor([-0.0241]), b=tensor([0.2272], requires_grad=True), b.grad=tensor([0.0548])\n",
      "=======================201==================\n",
      "hypothesis=\n",
      "tensor([[1.1273],\n",
      "        [2.0273],\n",
      "        [2.9274]], grad_fn=<AddBackward0>)\n",
      "loss=0.007405884563922882\n",
      "before backward() : W=tensor([0.9000], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2272], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9000], requires_grad=True), W.grad=tensor([-0.0240]), b=tensor([0.2272], requires_grad=True), b.grad=tensor([0.0546])\n",
      "after  step() : W=tensor([0.9003], requires_grad=True), W.grad=tensor([-0.0240]), b=tensor([0.2267], requires_grad=True), b.grad=tensor([0.0546])\n",
      "=======================202==================\n",
      "hypothesis=\n",
      "tensor([[1.1270],\n",
      "        [2.0272],\n",
      "        [2.9275]], grad_fn=<AddBackward0>)\n",
      "loss=0.0073703289963305\n",
      "before backward() : W=tensor([0.9003], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2267], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9003], requires_grad=True), W.grad=tensor([-0.0240]), b=tensor([0.2267], requires_grad=True), b.grad=tensor([0.0545])\n",
      "after  step() : W=tensor([0.9005], requires_grad=True), W.grad=tensor([-0.0240]), b=tensor([0.2261], requires_grad=True), b.grad=tensor([0.0545])\n",
      "=======================203==================\n",
      "hypothesis=\n",
      "tensor([[1.1266],\n",
      "        [2.0272],\n",
      "        [2.9277]], grad_fn=<AddBackward0>)\n",
      "loss=0.0073349326848983765\n",
      "before backward() : W=tensor([0.9005], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2261], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9005], requires_grad=True), W.grad=tensor([-0.0239]), b=tensor([0.2261], requires_grad=True), b.grad=tensor([0.0544])\n",
      "after  step() : W=tensor([0.9008], requires_grad=True), W.grad=tensor([-0.0239]), b=tensor([0.2256], requires_grad=True), b.grad=tensor([0.0544])\n",
      "=======================204==================\n",
      "hypothesis=\n",
      "tensor([[1.1263],\n",
      "        [2.0271],\n",
      "        [2.9279]], grad_fn=<AddBackward0>)\n",
      "loss=0.00729970820248127\n",
      "before backward() : W=tensor([0.9008], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2256], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9008], requires_grad=True), W.grad=tensor([-0.0239]), b=tensor([0.2256], requires_grad=True), b.grad=tensor([0.0542])\n",
      "after  step() : W=tensor([0.9010], requires_grad=True), W.grad=tensor([-0.0239]), b=tensor([0.2250], requires_grad=True), b.grad=tensor([0.0542])\n",
      "=======================205==================\n",
      "hypothesis=\n",
      "tensor([[1.1260],\n",
      "        [2.0270],\n",
      "        [2.9281]], grad_fn=<AddBackward0>)\n",
      "loss=0.007264656480401754\n",
      "before backward() : W=tensor([0.9010], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2250], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9010], requires_grad=True), W.grad=tensor([-0.0238]), b=tensor([0.2250], requires_grad=True), b.grad=tensor([0.0541])\n",
      "after  step() : W=tensor([0.9012], requires_grad=True), W.grad=tensor([-0.0238]), b=tensor([0.2245], requires_grad=True), b.grad=tensor([0.0541])\n",
      "=======================206==================\n",
      "hypothesis=\n",
      "tensor([[1.1257],\n",
      "        [2.0270],\n",
      "        [2.9282]], grad_fn=<AddBackward0>)\n",
      "loss=0.007229770999401808\n",
      "before backward() : W=tensor([0.9012], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2245], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9012], requires_grad=True), W.grad=tensor([-0.0237]), b=tensor([0.2245], requires_grad=True), b.grad=tensor([0.0540])\n",
      "after  step() : W=tensor([0.9015], requires_grad=True), W.grad=tensor([-0.0237]), b=tensor([0.2240], requires_grad=True), b.grad=tensor([0.0540])\n",
      "=======================207==================\n",
      "hypothesis=\n",
      "tensor([[1.1254],\n",
      "        [2.0269],\n",
      "        [2.9284]], grad_fn=<AddBackward0>)\n",
      "loss=0.007195045705884695\n",
      "before backward() : W=tensor([0.9015], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2240], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9015], requires_grad=True), W.grad=tensor([-0.0237]), b=tensor([0.2240], requires_grad=True), b.grad=tensor([0.0538])\n",
      "after  step() : W=tensor([0.9017], requires_grad=True), W.grad=tensor([-0.0237]), b=tensor([0.2234], requires_grad=True), b.grad=tensor([0.0538])\n",
      "=======================208==================\n",
      "hypothesis=\n",
      "tensor([[1.1251],\n",
      "        [2.0269],\n",
      "        [2.9286]], grad_fn=<AddBackward0>)\n",
      "loss=0.007160495966672897\n",
      "before backward() : W=tensor([0.9017], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2234], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9017], requires_grad=True), W.grad=tensor([-0.0236]), b=tensor([0.2234], requires_grad=True), b.grad=tensor([0.0537])\n",
      "after  step() : W=tensor([0.9020], requires_grad=True), W.grad=tensor([-0.0236]), b=tensor([0.2229], requires_grad=True), b.grad=tensor([0.0537])\n",
      "=======================209==================\n",
      "hypothesis=\n",
      "tensor([[1.1248],\n",
      "        [2.0268],\n",
      "        [2.9287]], grad_fn=<AddBackward0>)\n",
      "loss=0.007126109674572945\n",
      "before backward() : W=tensor([0.9020], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2229], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9020], requires_grad=True), W.grad=tensor([-0.0236]), b=tensor([0.2229], requires_grad=True), b.grad=tensor([0.0536])\n",
      "after  step() : W=tensor([0.9022], requires_grad=True), W.grad=tensor([-0.0236]), b=tensor([0.2223], requires_grad=True), b.grad=tensor([0.0536])\n",
      "=======================210==================\n",
      "hypothesis=\n",
      "tensor([[1.1245],\n",
      "        [2.0267],\n",
      "        [2.9289]], grad_fn=<AddBackward0>)\n",
      "loss=0.007091894745826721\n",
      "before backward() : W=tensor([0.9022], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2223], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9022], requires_grad=True), W.grad=tensor([-0.0235]), b=tensor([0.2223], requires_grad=True), b.grad=tensor([0.0534])\n",
      "after  step() : W=tensor([0.9024], requires_grad=True), W.grad=tensor([-0.0235]), b=tensor([0.2218], requires_grad=True), b.grad=tensor([0.0534])\n",
      "=======================211==================\n",
      "hypothesis=\n",
      "tensor([[1.1242],\n",
      "        [2.0267],\n",
      "        [2.9291]], grad_fn=<AddBackward0>)\n",
      "loss=0.007057840470224619\n",
      "before backward() : W=tensor([0.9024], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2218], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9024], requires_grad=True), W.grad=tensor([-0.0235]), b=tensor([0.2218], requires_grad=True), b.grad=tensor([0.0533])\n",
      "after  step() : W=tensor([0.9027], requires_grad=True), W.grad=tensor([-0.0235]), b=tensor([0.2213], requires_grad=True), b.grad=tensor([0.0533])\n",
      "=======================212==================\n",
      "hypothesis=\n",
      "tensor([[1.1239],\n",
      "        [2.0266],\n",
      "        [2.9293]], grad_fn=<AddBackward0>)\n",
      "loss=0.007023939397186041\n",
      "before backward() : W=tensor([0.9027], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2213], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9027], requires_grad=True), W.grad=tensor([-0.0234]), b=tensor([0.2213], requires_grad=True), b.grad=tensor([0.0532])\n",
      "after  step() : W=tensor([0.9029], requires_grad=True), W.grad=tensor([-0.0234]), b=tensor([0.2207], requires_grad=True), b.grad=tensor([0.0532])\n",
      "=======================213==================\n",
      "hypothesis=\n",
      "tensor([[1.1236],\n",
      "        [2.0265],\n",
      "        [2.9294]], grad_fn=<AddBackward0>)\n",
      "loss=0.006990216206759214\n",
      "before backward() : W=tensor([0.9029], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2207], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9029], requires_grad=True), W.grad=tensor([-0.0233]), b=tensor([0.2207], requires_grad=True), b.grad=tensor([0.0531])\n",
      "after  step() : W=tensor([0.9031], requires_grad=True), W.grad=tensor([-0.0233]), b=tensor([0.2202], requires_grad=True), b.grad=tensor([0.0531])\n",
      "=======================214==================\n",
      "hypothesis=\n",
      "tensor([[1.1233],\n",
      "        [2.0265],\n",
      "        [2.9296]], grad_fn=<AddBackward0>)\n",
      "loss=0.006956644356250763\n",
      "before backward() : W=tensor([0.9031], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2202], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9031], requires_grad=True), W.grad=tensor([-0.0233]), b=tensor([0.2202], requires_grad=True), b.grad=tensor([0.0529])\n",
      "after  step() : W=tensor([0.9034], requires_grad=True), W.grad=tensor([-0.0233]), b=tensor([0.2197], requires_grad=True), b.grad=tensor([0.0529])\n",
      "=======================215==================\n",
      "hypothesis=\n",
      "tensor([[1.1230],\n",
      "        [2.0264],\n",
      "        [2.9298]], grad_fn=<AddBackward0>)\n",
      "loss=0.006923248991370201\n",
      "before backward() : W=tensor([0.9034], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2197], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9034], requires_grad=True), W.grad=tensor([-0.0232]), b=tensor([0.2197], requires_grad=True), b.grad=tensor([0.0528])\n",
      "after  step() : W=tensor([0.9036], requires_grad=True), W.grad=tensor([-0.0232]), b=tensor([0.2192], requires_grad=True), b.grad=tensor([0.0528])\n",
      "=======================216==================\n",
      "hypothesis=\n",
      "tensor([[1.1227],\n",
      "        [2.0263],\n",
      "        [2.9299]], grad_fn=<AddBackward0>)\n",
      "loss=0.00688999705016613\n",
      "before backward() : W=tensor([0.9036], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2192], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9036], requires_grad=True), W.grad=tensor([-0.0232]), b=tensor([0.2192], requires_grad=True), b.grad=tensor([0.0527])\n",
      "after  step() : W=tensor([0.9038], requires_grad=True), W.grad=tensor([-0.0232]), b=tensor([0.2186], requires_grad=True), b.grad=tensor([0.0527])\n",
      "=======================217==================\n",
      "hypothesis=\n",
      "tensor([[1.1225],\n",
      "        [2.0263],\n",
      "        [2.9301]], grad_fn=<AddBackward0>)\n",
      "loss=0.0068569001741707325\n",
      "before backward() : W=tensor([0.9038], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2186], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9038], requires_grad=True), W.grad=tensor([-0.0231]), b=tensor([0.2186], requires_grad=True), b.grad=tensor([0.0526])\n",
      "after  step() : W=tensor([0.9041], requires_grad=True), W.grad=tensor([-0.0231]), b=tensor([0.2181], requires_grad=True), b.grad=tensor([0.0526])\n",
      "=======================218==================\n",
      "hypothesis=\n",
      "tensor([[1.1222],\n",
      "        [2.0262],\n",
      "        [2.9303]], grad_fn=<AddBackward0>)\n",
      "loss=0.006823984440416098\n",
      "before backward() : W=tensor([0.9041], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2181], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9041], requires_grad=True), W.grad=tensor([-0.0231]), b=tensor([0.2181], requires_grad=True), b.grad=tensor([0.0524])\n",
      "after  step() : W=tensor([0.9043], requires_grad=True), W.grad=tensor([-0.0231]), b=tensor([0.2176], requires_grad=True), b.grad=tensor([0.0524])\n",
      "=======================219==================\n",
      "hypothesis=\n",
      "tensor([[1.1219],\n",
      "        [2.0262],\n",
      "        [2.9304]], grad_fn=<AddBackward0>)\n",
      "loss=0.006791220977902412\n",
      "before backward() : W=tensor([0.9043], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2176], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9043], requires_grad=True), W.grad=tensor([-0.0230]), b=tensor([0.2176], requires_grad=True), b.grad=tensor([0.0523])\n",
      "after  step() : W=tensor([0.9045], requires_grad=True), W.grad=tensor([-0.0230]), b=tensor([0.2171], requires_grad=True), b.grad=tensor([0.0523])\n",
      "=======================220==================\n",
      "hypothesis=\n",
      "tensor([[1.1216],\n",
      "        [2.0261],\n",
      "        [2.9306]], grad_fn=<AddBackward0>)\n",
      "loss=0.006758606527000666\n",
      "before backward() : W=tensor([0.9045], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2171], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9045], requires_grad=True), W.grad=tensor([-0.0230]), b=tensor([0.2171], requires_grad=True), b.grad=tensor([0.0522])\n",
      "after  step() : W=tensor([0.9047], requires_grad=True), W.grad=tensor([-0.0230]), b=tensor([0.2165], requires_grad=True), b.grad=tensor([0.0522])\n",
      "=======================221==================\n",
      "hypothesis=\n",
      "tensor([[1.1213],\n",
      "        [2.0260],\n",
      "        [2.9308]], grad_fn=<AddBackward0>)\n",
      "loss=0.006726153194904327\n",
      "before backward() : W=tensor([0.9047], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2165], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9047], requires_grad=True), W.grad=tensor([-0.0229]), b=tensor([0.2165], requires_grad=True), b.grad=tensor([0.0521])\n",
      "after  step() : W=tensor([0.9050], requires_grad=True), W.grad=tensor([-0.0229]), b=tensor([0.2160], requires_grad=True), b.grad=tensor([0.0521])\n",
      "=======================222==================\n",
      "hypothesis=\n",
      "tensor([[1.1210],\n",
      "        [2.0260],\n",
      "        [2.9309]], grad_fn=<AddBackward0>)\n",
      "loss=0.006693841423839331\n",
      "before backward() : W=tensor([0.9050], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2160], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9050], requires_grad=True), W.grad=tensor([-0.0228]), b=tensor([0.2160], requires_grad=True), b.grad=tensor([0.0519])\n",
      "after  step() : W=tensor([0.9052], requires_grad=True), W.grad=tensor([-0.0228]), b=tensor([0.2155], requires_grad=True), b.grad=tensor([0.0519])\n",
      "=======================223==================\n",
      "hypothesis=\n",
      "tensor([[1.1207],\n",
      "        [2.0259],\n",
      "        [2.9311]], grad_fn=<AddBackward0>)\n",
      "loss=0.006661698222160339\n",
      "before backward() : W=tensor([0.9052], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2155], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9052], requires_grad=True), W.grad=tensor([-0.0228]), b=tensor([0.2155], requires_grad=True), b.grad=tensor([0.0518])\n",
      "after  step() : W=tensor([0.9054], requires_grad=True), W.grad=tensor([-0.0228]), b=tensor([0.2150], requires_grad=True), b.grad=tensor([0.0518])\n",
      "=======================224==================\n",
      "hypothesis=\n",
      "tensor([[1.1204],\n",
      "        [2.0258],\n",
      "        [2.9313]], grad_fn=<AddBackward0>)\n",
      "loss=0.006629711017012596\n",
      "before backward() : W=tensor([0.9054], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2150], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9054], requires_grad=True), W.grad=tensor([-0.0227]), b=tensor([0.2150], requires_grad=True), b.grad=tensor([0.0517])\n",
      "after  step() : W=tensor([0.9057], requires_grad=True), W.grad=tensor([-0.0227]), b=tensor([0.2145], requires_grad=True), b.grad=tensor([0.0517])\n",
      "=======================225==================\n",
      "hypothesis=\n",
      "tensor([[1.1201],\n",
      "        [2.0258],\n",
      "        [2.9314]], grad_fn=<AddBackward0>)\n",
      "loss=0.006597877014428377\n",
      "before backward() : W=tensor([0.9057], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2145], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9057], requires_grad=True), W.grad=tensor([-0.0227]), b=tensor([0.2145], requires_grad=True), b.grad=tensor([0.0516])\n",
      "after  step() : W=tensor([0.9059], requires_grad=True), W.grad=tensor([-0.0227]), b=tensor([0.2139], requires_grad=True), b.grad=tensor([0.0516])\n",
      "=======================226==================\n",
      "hypothesis=\n",
      "tensor([[1.1198],\n",
      "        [2.0257],\n",
      "        [2.9316]], grad_fn=<AddBackward0>)\n",
      "loss=0.006566187832504511\n",
      "before backward() : W=tensor([0.9059], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2139], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9059], requires_grad=True), W.grad=tensor([-0.0226]), b=tensor([0.2139], requires_grad=True), b.grad=tensor([0.0514])\n",
      "after  step() : W=tensor([0.9061], requires_grad=True), W.grad=tensor([-0.0226]), b=tensor([0.2134], requires_grad=True), b.grad=tensor([0.0514])\n",
      "=======================227==================\n",
      "hypothesis=\n",
      "tensor([[1.1195],\n",
      "        [2.0257],\n",
      "        [2.9318]], grad_fn=<AddBackward0>)\n",
      "loss=0.006534669082611799\n",
      "before backward() : W=tensor([0.9061], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2134], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9061], requires_grad=True), W.grad=tensor([-0.0226]), b=tensor([0.2134], requires_grad=True), b.grad=tensor([0.0513])\n",
      "after  step() : W=tensor([0.9063], requires_grad=True), W.grad=tensor([-0.0226]), b=tensor([0.2129], requires_grad=True), b.grad=tensor([0.0513])\n",
      "=======================228==================\n",
      "hypothesis=\n",
      "tensor([[1.1193],\n",
      "        [2.0256],\n",
      "        [2.9319]], grad_fn=<AddBackward0>)\n",
      "loss=0.006503276992589235\n",
      "before backward() : W=tensor([0.9063], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2129], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9063], requires_grad=True), W.grad=tensor([-0.0225]), b=tensor([0.2129], requires_grad=True), b.grad=tensor([0.0512])\n",
      "after  step() : W=tensor([0.9066], requires_grad=True), W.grad=tensor([-0.0225]), b=tensor([0.2124], requires_grad=True), b.grad=tensor([0.0512])\n",
      "=======================229==================\n",
      "hypothesis=\n",
      "tensor([[1.1190],\n",
      "        [2.0255],\n",
      "        [2.9321]], grad_fn=<AddBackward0>)\n",
      "loss=0.00647204602137208\n",
      "before backward() : W=tensor([0.9066], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2124], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9066], requires_grad=True), W.grad=tensor([-0.0225]), b=tensor([0.2124], requires_grad=True), b.grad=tensor([0.0511])\n",
      "after  step() : W=tensor([0.9068], requires_grad=True), W.grad=tensor([-0.0225]), b=tensor([0.2119], requires_grad=True), b.grad=tensor([0.0511])\n",
      "=======================230==================\n",
      "hypothesis=\n",
      "tensor([[1.1187],\n",
      "        [2.0255],\n",
      "        [2.9323]], grad_fn=<AddBackward0>)\n",
      "loss=0.006440965458750725\n",
      "before backward() : W=tensor([0.9068], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2119], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9068], requires_grad=True), W.grad=tensor([-0.0224]), b=tensor([0.2119], requires_grad=True), b.grad=tensor([0.0509])\n",
      "after  step() : W=tensor([0.9070], requires_grad=True), W.grad=tensor([-0.0224]), b=tensor([0.2114], requires_grad=True), b.grad=tensor([0.0509])\n",
      "=======================231==================\n",
      "hypothesis=\n",
      "tensor([[1.1184],\n",
      "        [2.0254],\n",
      "        [2.9324]], grad_fn=<AddBackward0>)\n",
      "loss=0.0064100478775799274\n",
      "before backward() : W=tensor([0.9070], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2114], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9070], requires_grad=True), W.grad=tensor([-0.0224]), b=tensor([0.2114], requires_grad=True), b.grad=tensor([0.0508])\n",
      "after  step() : W=tensor([0.9072], requires_grad=True), W.grad=tensor([-0.0224]), b=tensor([0.2109], requires_grad=True), b.grad=tensor([0.0508])\n",
      "=======================232==================\n",
      "hypothesis=\n",
      "tensor([[1.1181],\n",
      "        [2.0253],\n",
      "        [2.9326]], grad_fn=<AddBackward0>)\n",
      "loss=0.006379263009876013\n",
      "before backward() : W=tensor([0.9072], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2109], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9072], requires_grad=True), W.grad=tensor([-0.0223]), b=tensor([0.2109], requires_grad=True), b.grad=tensor([0.0507])\n",
      "after  step() : W=tensor([0.9075], requires_grad=True), W.grad=tensor([-0.0223]), b=tensor([0.2104], requires_grad=True), b.grad=tensor([0.0507])\n",
      "=======================233==================\n",
      "hypothesis=\n",
      "tensor([[1.1178],\n",
      "        [2.0253],\n",
      "        [2.9327]], grad_fn=<AddBackward0>)\n",
      "loss=0.006348627153784037\n",
      "before backward() : W=tensor([0.9075], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2104], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9075], requires_grad=True), W.grad=tensor([-0.0222]), b=tensor([0.2104], requires_grad=True), b.grad=tensor([0.0506])\n",
      "after  step() : W=tensor([0.9077], requires_grad=True), W.grad=tensor([-0.0222]), b=tensor([0.2099], requires_grad=True), b.grad=tensor([0.0506])\n",
      "=======================234==================\n",
      "hypothesis=\n",
      "tensor([[1.1175],\n",
      "        [2.0252],\n",
      "        [2.9329]], grad_fn=<AddBackward0>)\n",
      "loss=0.0063181486912071705\n",
      "before backward() : W=tensor([0.9077], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2099], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9077], requires_grad=True), W.grad=tensor([-0.0222]), b=tensor([0.2099], requires_grad=True), b.grad=tensor([0.0504])\n",
      "after  step() : W=tensor([0.9079], requires_grad=True), W.grad=tensor([-0.0222]), b=tensor([0.2094], requires_grad=True), b.grad=tensor([0.0504])\n",
      "=======================235==================\n",
      "hypothesis=\n",
      "tensor([[1.1173],\n",
      "        [2.0252],\n",
      "        [2.9331]], grad_fn=<AddBackward0>)\n",
      "loss=0.00628780759871006\n",
      "before backward() : W=tensor([0.9079], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2094], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9079], requires_grad=True), W.grad=tensor([-0.0221]), b=tensor([0.2094], requires_grad=True), b.grad=tensor([0.0503])\n",
      "after  step() : W=tensor([0.9081], requires_grad=True), W.grad=tensor([-0.0221]), b=tensor([0.2089], requires_grad=True), b.grad=tensor([0.0503])\n",
      "=======================236==================\n",
      "hypothesis=\n",
      "tensor([[1.1170],\n",
      "        [2.0251],\n",
      "        [2.9332]], grad_fn=<AddBackward0>)\n",
      "loss=0.006257610395550728\n",
      "before backward() : W=tensor([0.9081], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2089], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9081], requires_grad=True), W.grad=tensor([-0.0221]), b=tensor([0.2089], requires_grad=True), b.grad=tensor([0.0502])\n",
      "after  step() : W=tensor([0.9083], requires_grad=True), W.grad=tensor([-0.0221]), b=tensor([0.2084], requires_grad=True), b.grad=tensor([0.0502])\n",
      "=======================237==================\n",
      "hypothesis=\n",
      "tensor([[1.1167],\n",
      "        [2.0250],\n",
      "        [2.9334]], grad_fn=<AddBackward0>)\n",
      "loss=0.006227562669664621\n",
      "before backward() : W=tensor([0.9083], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2084], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9083], requires_grad=True), W.grad=tensor([-0.0220]), b=tensor([0.2084], requires_grad=True), b.grad=tensor([0.0501])\n",
      "after  step() : W=tensor([0.9086], requires_grad=True), W.grad=tensor([-0.0220]), b=tensor([0.2079], requires_grad=True), b.grad=tensor([0.0501])\n",
      "=======================238==================\n",
      "hypothesis=\n",
      "tensor([[1.1164],\n",
      "        [2.0250],\n",
      "        [2.9335]], grad_fn=<AddBackward0>)\n",
      "loss=0.006197645794600248\n",
      "before backward() : W=tensor([0.9086], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2079], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9086], requires_grad=True), W.grad=tensor([-0.0220]), b=tensor([0.2079], requires_grad=True), b.grad=tensor([0.0500])\n",
      "after  step() : W=tensor([0.9088], requires_grad=True), W.grad=tensor([-0.0220]), b=tensor([0.2074], requires_grad=True), b.grad=tensor([0.0500])\n",
      "=======================239==================\n",
      "hypothesis=\n",
      "tensor([[1.1161],\n",
      "        [2.0249],\n",
      "        [2.9337]], grad_fn=<AddBackward0>)\n",
      "loss=0.006167893763631582\n",
      "before backward() : W=tensor([0.9088], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2074], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9088], requires_grad=True), W.grad=tensor([-0.0219]), b=tensor([0.2074], requires_grad=True), b.grad=tensor([0.0498])\n",
      "after  step() : W=tensor([0.9090], requires_grad=True), W.grad=tensor([-0.0219]), b=tensor([0.2069], requires_grad=True), b.grad=tensor([0.0498])\n",
      "=======================240==================\n",
      "hypothesis=\n",
      "tensor([[1.1159],\n",
      "        [2.0249],\n",
      "        [2.9339]], grad_fn=<AddBackward0>)\n",
      "loss=0.0061382693238556385\n",
      "before backward() : W=tensor([0.9090], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2069], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9090], requires_grad=True), W.grad=tensor([-0.0219]), b=tensor([0.2069], requires_grad=True), b.grad=tensor([0.0497])\n",
      "after  step() : W=tensor([0.9092], requires_grad=True), W.grad=tensor([-0.0219]), b=tensor([0.2064], requires_grad=True), b.grad=tensor([0.0497])\n",
      "=======================241==================\n",
      "hypothesis=\n",
      "tensor([[1.1156],\n",
      "        [2.0248],\n",
      "        [2.9340]], grad_fn=<AddBackward0>)\n",
      "loss=0.006108803674578667\n",
      "before backward() : W=tensor([0.9092], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2064], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9092], requires_grad=True), W.grad=tensor([-0.0218]), b=tensor([0.2064], requires_grad=True), b.grad=tensor([0.0496])\n",
      "after  step() : W=tensor([0.9094], requires_grad=True), W.grad=tensor([-0.0218]), b=tensor([0.2059], requires_grad=True), b.grad=tensor([0.0496])\n",
      "=======================242==================\n",
      "hypothesis=\n",
      "tensor([[1.1153],\n",
      "        [2.0247],\n",
      "        [2.9342]], grad_fn=<AddBackward0>)\n",
      "loss=0.006079468410462141\n",
      "before backward() : W=tensor([0.9094], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2059], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9094], requires_grad=True), W.grad=tensor([-0.0218]), b=tensor([0.2059], requires_grad=True), b.grad=tensor([0.0495])\n",
      "after  step() : W=tensor([0.9097], requires_grad=True), W.grad=tensor([-0.0218]), b=tensor([0.2054], requires_grad=True), b.grad=tensor([0.0495])\n",
      "=======================243==================\n",
      "hypothesis=\n",
      "tensor([[1.1150],\n",
      "        [2.0247],\n",
      "        [2.9343]], grad_fn=<AddBackward0>)\n",
      "loss=0.0060502695851027966\n",
      "before backward() : W=tensor([0.9097], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2054], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9097], requires_grad=True), W.grad=tensor([-0.0217]), b=tensor([0.2054], requires_grad=True), b.grad=tensor([0.0494])\n",
      "after  step() : W=tensor([0.9099], requires_grad=True), W.grad=tensor([-0.0217]), b=tensor([0.2049], requires_grad=True), b.grad=tensor([0.0494])\n",
      "=======================244==================\n",
      "hypothesis=\n",
      "tensor([[1.1147],\n",
      "        [2.0246],\n",
      "        [2.9345]], grad_fn=<AddBackward0>)\n",
      "loss=0.006021210923790932\n",
      "before backward() : W=tensor([0.9099], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2049], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9099], requires_grad=True), W.grad=tensor([-0.0217]), b=tensor([0.2049], requires_grad=True), b.grad=tensor([0.0492])\n",
      "after  step() : W=tensor([0.9101], requires_grad=True), W.grad=tensor([-0.0217]), b=tensor([0.2044], requires_grad=True), b.grad=tensor([0.0492])\n",
      "=======================245==================\n",
      "hypothesis=\n",
      "tensor([[1.1145],\n",
      "        [2.0246],\n",
      "        [2.9347]], grad_fn=<AddBackward0>)\n",
      "loss=0.00599230220541358\n",
      "before backward() : W=tensor([0.9101], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2044], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9101], requires_grad=True), W.grad=tensor([-0.0216]), b=tensor([0.2044], requires_grad=True), b.grad=tensor([0.0491])\n",
      "after  step() : W=tensor([0.9103], requires_grad=True), W.grad=tensor([-0.0216]), b=tensor([0.2039], requires_grad=True), b.grad=tensor([0.0491])\n",
      "=======================246==================\n",
      "hypothesis=\n",
      "tensor([[1.1142],\n",
      "        [2.0245],\n",
      "        [2.9348]], grad_fn=<AddBackward0>)\n",
      "loss=0.005963526666164398\n",
      "before backward() : W=tensor([0.9103], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2039], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9103], requires_grad=True), W.grad=tensor([-0.0216]), b=tensor([0.2039], requires_grad=True), b.grad=tensor([0.0490])\n",
      "after  step() : W=tensor([0.9105], requires_grad=True), W.grad=tensor([-0.0216]), b=tensor([0.2034], requires_grad=True), b.grad=tensor([0.0490])\n",
      "=======================247==================\n",
      "hypothesis=\n",
      "tensor([[1.1139],\n",
      "        [2.0244],\n",
      "        [2.9350]], grad_fn=<AddBackward0>)\n",
      "loss=0.005934881046414375\n",
      "before backward() : W=tensor([0.9105], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2034], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9105], requires_grad=True), W.grad=tensor([-0.0215]), b=tensor([0.2034], requires_grad=True), b.grad=tensor([0.0489])\n",
      "after  step() : W=tensor([0.9107], requires_grad=True), W.grad=tensor([-0.0215]), b=tensor([0.2029], requires_grad=True), b.grad=tensor([0.0489])\n",
      "=======================248==================\n",
      "hypothesis=\n",
      "tensor([[1.1136],\n",
      "        [2.0244],\n",
      "        [2.9351]], grad_fn=<AddBackward0>)\n",
      "loss=0.005906382575631142\n",
      "before backward() : W=tensor([0.9107], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2029], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9107], requires_grad=True), W.grad=tensor([-0.0215]), b=tensor([0.2029], requires_grad=True), b.grad=tensor([0.0488])\n",
      "after  step() : W=tensor([0.9110], requires_grad=True), W.grad=tensor([-0.0215]), b=tensor([0.2024], requires_grad=True), b.grad=tensor([0.0488])\n",
      "=======================249==================\n",
      "hypothesis=\n",
      "tensor([[1.1134],\n",
      "        [2.0243],\n",
      "        [2.9353]], grad_fn=<AddBackward0>)\n",
      "loss=0.005878018215298653\n",
      "before backward() : W=tensor([0.9110], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2024], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9110], requires_grad=True), W.grad=tensor([-0.0214]), b=tensor([0.2024], requires_grad=True), b.grad=tensor([0.0487])\n",
      "after  step() : W=tensor([0.9112], requires_grad=True), W.grad=tensor([-0.0214]), b=tensor([0.2019], requires_grad=True), b.grad=tensor([0.0487])\n",
      "=======================250==================\n",
      "hypothesis=\n",
      "tensor([[1.1131],\n",
      "        [2.0243],\n",
      "        [2.9354]], grad_fn=<AddBackward0>)\n",
      "loss=0.005849801003932953\n",
      "before backward() : W=tensor([0.9112], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2019], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9112], requires_grad=True), W.grad=tensor([-0.0214]), b=tensor([0.2019], requires_grad=True), b.grad=tensor([0.0485])\n",
      "after  step() : W=tensor([0.9114], requires_grad=True), W.grad=tensor([-0.0214]), b=tensor([0.2014], requires_grad=True), b.grad=tensor([0.0485])\n",
      "=======================251==================\n",
      "hypothesis=\n",
      "tensor([[1.1128],\n",
      "        [2.0242],\n",
      "        [2.9356]], grad_fn=<AddBackward0>)\n",
      "loss=0.005821706261485815\n",
      "before backward() : W=tensor([0.9114], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2014], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9114], requires_grad=True), W.grad=tensor([-0.0213]), b=tensor([0.2014], requires_grad=True), b.grad=tensor([0.0484])\n",
      "after  step() : W=tensor([0.9116], requires_grad=True), W.grad=tensor([-0.0213]), b=tensor([0.2010], requires_grad=True), b.grad=tensor([0.0484])\n",
      "=======================252==================\n",
      "hypothesis=\n",
      "tensor([[1.1126],\n",
      "        [2.0242],\n",
      "        [2.9358]], grad_fn=<AddBackward0>)\n",
      "loss=0.00579375633969903\n",
      "before backward() : W=tensor([0.9116], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2010], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9116], requires_grad=True), W.grad=tensor([-0.0213]), b=tensor([0.2010], requires_grad=True), b.grad=tensor([0.0483])\n",
      "after  step() : W=tensor([0.9118], requires_grad=True), W.grad=tensor([-0.0213]), b=tensor([0.2005], requires_grad=True), b.grad=tensor([0.0483])\n",
      "=======================253==================\n",
      "hypothesis=\n",
      "tensor([[1.1123],\n",
      "        [2.0241],\n",
      "        [2.9359]], grad_fn=<AddBackward0>)\n",
      "loss=0.005765920504927635\n",
      "before backward() : W=tensor([0.9118], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2005], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9118], requires_grad=True), W.grad=tensor([-0.0212]), b=tensor([0.2005], requires_grad=True), b.grad=tensor([0.0482])\n",
      "after  step() : W=tensor([0.9120], requires_grad=True), W.grad=tensor([-0.0212]), b=tensor([0.2000], requires_grad=True), b.grad=tensor([0.0482])\n",
      "=======================254==================\n",
      "hypothesis=\n",
      "tensor([[1.1120],\n",
      "        [2.0240],\n",
      "        [2.9361]], grad_fn=<AddBackward0>)\n",
      "loss=0.005738238338381052\n",
      "before backward() : W=tensor([0.9120], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.2000], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9120], requires_grad=True), W.grad=tensor([-0.0211]), b=tensor([0.2000], requires_grad=True), b.grad=tensor([0.0481])\n",
      "after  step() : W=tensor([0.9122], requires_grad=True), W.grad=tensor([-0.0211]), b=tensor([0.1995], requires_grad=True), b.grad=tensor([0.0481])\n",
      "=======================255==================\n",
      "hypothesis=\n",
      "tensor([[1.1118],\n",
      "        [2.0240],\n",
      "        [2.9362]], grad_fn=<AddBackward0>)\n",
      "loss=0.005710691213607788\n",
      "before backward() : W=tensor([0.9122], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1995], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9122], requires_grad=True), W.grad=tensor([-0.0211]), b=tensor([0.1995], requires_grad=True), b.grad=tensor([0.0480])\n",
      "after  step() : W=tensor([0.9124], requires_grad=True), W.grad=tensor([-0.0211]), b=tensor([0.1990], requires_grad=True), b.grad=tensor([0.0480])\n",
      "=======================256==================\n",
      "hypothesis=\n",
      "tensor([[1.1115],\n",
      "        [2.0239],\n",
      "        [2.9364]], grad_fn=<AddBackward0>)\n",
      "loss=0.005683263298124075\n",
      "before backward() : W=tensor([0.9124], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1990], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9124], requires_grad=True), W.grad=tensor([-0.0210]), b=tensor([0.1990], requires_grad=True), b.grad=tensor([0.0478])\n",
      "after  step() : W=tensor([0.9127], requires_grad=True), W.grad=tensor([-0.0210]), b=tensor([0.1986], requires_grad=True), b.grad=tensor([0.0478])\n",
      "=======================257==================\n",
      "hypothesis=\n",
      "tensor([[1.1112],\n",
      "        [2.0239],\n",
      "        [2.9365]], grad_fn=<AddBackward0>)\n",
      "loss=0.005655966699123383\n",
      "before backward() : W=tensor([0.9127], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1986], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9127], requires_grad=True), W.grad=tensor([-0.0210]), b=tensor([0.1986], requires_grad=True), b.grad=tensor([0.0477])\n",
      "after  step() : W=tensor([0.9129], requires_grad=True), W.grad=tensor([-0.0210]), b=tensor([0.1981], requires_grad=True), b.grad=tensor([0.0477])\n",
      "=======================258==================\n",
      "hypothesis=\n",
      "tensor([[1.1109],\n",
      "        [2.0238],\n",
      "        [2.9367]], grad_fn=<AddBackward0>)\n",
      "loss=0.00562881538644433\n",
      "before backward() : W=tensor([0.9129], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1981], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9129], requires_grad=True), W.grad=tensor([-0.0209]), b=tensor([0.1981], requires_grad=True), b.grad=tensor([0.0476])\n",
      "after  step() : W=tensor([0.9131], requires_grad=True), W.grad=tensor([-0.0209]), b=tensor([0.1976], requires_grad=True), b.grad=tensor([0.0476])\n",
      "=======================259==================\n",
      "hypothesis=\n",
      "tensor([[1.1107],\n",
      "        [2.0238],\n",
      "        [2.9368]], grad_fn=<AddBackward0>)\n",
      "loss=0.005601774901151657\n",
      "before backward() : W=tensor([0.9131], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1976], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9131], requires_grad=True), W.grad=tensor([-0.0209]), b=tensor([0.1976], requires_grad=True), b.grad=tensor([0.0475])\n",
      "after  step() : W=tensor([0.9133], requires_grad=True), W.grad=tensor([-0.0209]), b=tensor([0.1971], requires_grad=True), b.grad=tensor([0.0475])\n",
      "=======================260==================\n",
      "hypothesis=\n",
      "tensor([[1.1104],\n",
      "        [2.0237],\n",
      "        [2.9370]], grad_fn=<AddBackward0>)\n",
      "loss=0.005574872251600027\n",
      "before backward() : W=tensor([0.9133], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1971], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9133], requires_grad=True), W.grad=tensor([-0.0208]), b=tensor([0.1971], requires_grad=True), b.grad=tensor([0.0474])\n",
      "after  step() : W=tensor([0.9135], requires_grad=True), W.grad=tensor([-0.0208]), b=tensor([0.1967], requires_grad=True), b.grad=tensor([0.0474])\n",
      "=======================261==================\n",
      "hypothesis=\n",
      "tensor([[1.1101],\n",
      "        [2.0236],\n",
      "        [2.9371]], grad_fn=<AddBackward0>)\n",
      "loss=0.005548104178160429\n",
      "before backward() : W=tensor([0.9135], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1967], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9135], requires_grad=True), W.grad=tensor([-0.0208]), b=tensor([0.1967], requires_grad=True), b.grad=tensor([0.0473])\n",
      "after  step() : W=tensor([0.9137], requires_grad=True), W.grad=tensor([-0.0208]), b=tensor([0.1962], requires_grad=True), b.grad=tensor([0.0473])\n",
      "=======================262==================\n",
      "hypothesis=\n",
      "tensor([[1.1099],\n",
      "        [2.0236],\n",
      "        [2.9373]], grad_fn=<AddBackward0>)\n",
      "loss=0.005521459970623255\n",
      "before backward() : W=tensor([0.9137], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1962], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9137], requires_grad=True), W.grad=tensor([-0.0207]), b=tensor([0.1962], requires_grad=True), b.grad=tensor([0.0472])\n",
      "after  step() : W=tensor([0.9139], requires_grad=True), W.grad=tensor([-0.0207]), b=tensor([0.1957], requires_grad=True), b.grad=tensor([0.0472])\n",
      "=======================263==================\n",
      "hypothesis=\n",
      "tensor([[1.1096],\n",
      "        [2.0235],\n",
      "        [2.9374]], grad_fn=<AddBackward0>)\n",
      "loss=0.005494951736181974\n",
      "before backward() : W=tensor([0.9139], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1957], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9139], requires_grad=True), W.grad=tensor([-0.0207]), b=tensor([0.1957], requires_grad=True), b.grad=tensor([0.0470])\n",
      "after  step() : W=tensor([0.9141], requires_grad=True), W.grad=tensor([-0.0207]), b=tensor([0.1952], requires_grad=True), b.grad=tensor([0.0470])\n",
      "=======================264==================\n",
      "hypothesis=\n",
      "tensor([[1.1094],\n",
      "        [2.0235],\n",
      "        [2.9376]], grad_fn=<AddBackward0>)\n",
      "loss=0.005468565970659256\n",
      "before backward() : W=tensor([0.9141], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1952], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9141], requires_grad=True), W.grad=tensor([-0.0206]), b=tensor([0.1952], requires_grad=True), b.grad=tensor([0.0469])\n",
      "after  step() : W=tensor([0.9143], requires_grad=True), W.grad=tensor([-0.0206]), b=tensor([0.1948], requires_grad=True), b.grad=tensor([0.0469])\n",
      "=======================265==================\n",
      "hypothesis=\n",
      "tensor([[1.1091],\n",
      "        [2.0234],\n",
      "        [2.9377]], grad_fn=<AddBackward0>)\n",
      "loss=0.005442304071038961\n",
      "before backward() : W=tensor([0.9143], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1948], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9143], requires_grad=True), W.grad=tensor([-0.0206]), b=tensor([0.1948], requires_grad=True), b.grad=tensor([0.0468])\n",
      "after  step() : W=tensor([0.9145], requires_grad=True), W.grad=tensor([-0.0206]), b=tensor([0.1943], requires_grad=True), b.grad=tensor([0.0468])\n",
      "=======================266==================\n",
      "hypothesis=\n",
      "tensor([[1.1088],\n",
      "        [2.0234],\n",
      "        [2.9379]], grad_fn=<AddBackward0>)\n",
      "loss=0.005416167434304953\n",
      "before backward() : W=tensor([0.9145], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1943], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9145], requires_grad=True), W.grad=tensor([-0.0205]), b=tensor([0.1943], requires_grad=True), b.grad=tensor([0.0467])\n",
      "after  step() : W=tensor([0.9147], requires_grad=True), W.grad=tensor([-0.0205]), b=tensor([0.1938], requires_grad=True), b.grad=tensor([0.0467])\n",
      "=======================267==================\n",
      "hypothesis=\n",
      "tensor([[1.1086],\n",
      "        [2.0233],\n",
      "        [2.9380]], grad_fn=<AddBackward0>)\n",
      "loss=0.00539015606045723\n",
      "before backward() : W=tensor([0.9147], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1938], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9147], requires_grad=True), W.grad=tensor([-0.0205]), b=tensor([0.1938], requires_grad=True), b.grad=tensor([0.0466])\n",
      "after  step() : W=tensor([0.9149], requires_grad=True), W.grad=tensor([-0.0205]), b=tensor([0.1934], requires_grad=True), b.grad=tensor([0.0466])\n",
      "=======================268==================\n",
      "hypothesis=\n",
      "tensor([[1.1083],\n",
      "        [2.0232],\n",
      "        [2.9382]], grad_fn=<AddBackward0>)\n",
      "loss=0.005364274140447378\n",
      "before backward() : W=tensor([0.9149], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1934], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9149], requires_grad=True), W.grad=tensor([-0.0204]), b=tensor([0.1934], requires_grad=True), b.grad=tensor([0.0465])\n",
      "after  step() : W=tensor([0.9151], requires_grad=True), W.grad=tensor([-0.0204]), b=tensor([0.1929], requires_grad=True), b.grad=tensor([0.0465])\n",
      "=======================269==================\n",
      "hypothesis=\n",
      "tensor([[1.1080],\n",
      "        [2.0232],\n",
      "        [2.9383]], grad_fn=<AddBackward0>)\n",
      "loss=0.005338519811630249\n",
      "before backward() : W=tensor([0.9151], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1929], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9151], requires_grad=True), W.grad=tensor([-0.0204]), b=tensor([0.1929], requires_grad=True), b.grad=tensor([0.0464])\n",
      "after  step() : W=tensor([0.9153], requires_grad=True), W.grad=tensor([-0.0204]), b=tensor([0.1924], requires_grad=True), b.grad=tensor([0.0464])\n",
      "=======================270==================\n",
      "hypothesis=\n",
      "tensor([[1.1078],\n",
      "        [2.0231],\n",
      "        [2.9385]], grad_fn=<AddBackward0>)\n",
      "loss=0.005312879104167223\n",
      "before backward() : W=tensor([0.9153], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1924], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9153], requires_grad=True), W.grad=tensor([-0.0204]), b=tensor([0.1924], requires_grad=True), b.grad=tensor([0.0463])\n",
      "after  step() : W=tensor([0.9155], requires_grad=True), W.grad=tensor([-0.0204]), b=tensor([0.1920], requires_grad=True), b.grad=tensor([0.0463])\n",
      "=======================271==================\n",
      "hypothesis=\n",
      "tensor([[1.1075],\n",
      "        [2.0231],\n",
      "        [2.9386]], grad_fn=<AddBackward0>)\n",
      "loss=0.005287367384880781\n",
      "before backward() : W=tensor([0.9155], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1920], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9155], requires_grad=True), W.grad=tensor([-0.0203]), b=tensor([0.1920], requires_grad=True), b.grad=tensor([0.0462])\n",
      "after  step() : W=tensor([0.9157], requires_grad=True), W.grad=tensor([-0.0203]), b=tensor([0.1915], requires_grad=True), b.grad=tensor([0.0462])\n",
      "=======================272==================\n",
      "hypothesis=\n",
      "tensor([[1.1073],\n",
      "        [2.0230],\n",
      "        [2.9388]], grad_fn=<AddBackward0>)\n",
      "loss=0.005261978600174189\n",
      "before backward() : W=tensor([0.9157], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1915], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9157], requires_grad=True), W.grad=tensor([-0.0203]), b=tensor([0.1915], requires_grad=True), b.grad=tensor([0.0460])\n",
      "after  step() : W=tensor([0.9160], requires_grad=True), W.grad=tensor([-0.0203]), b=tensor([0.1911], requires_grad=True), b.grad=tensor([0.0460])\n",
      "=======================273==================\n",
      "hypothesis=\n",
      "tensor([[1.1070],\n",
      "        [2.0230],\n",
      "        [2.9389]], grad_fn=<AddBackward0>)\n",
      "loss=0.005236705765128136\n",
      "before backward() : W=tensor([0.9160], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1911], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9160], requires_grad=True), W.grad=tensor([-0.0202]), b=tensor([0.1911], requires_grad=True), b.grad=tensor([0.0459])\n",
      "after  step() : W=tensor([0.9162], requires_grad=True), W.grad=tensor([-0.0202]), b=tensor([0.1906], requires_grad=True), b.grad=tensor([0.0459])\n",
      "=======================274==================\n",
      "hypothesis=\n",
      "tensor([[1.1068],\n",
      "        [2.0229],\n",
      "        [2.9391]], grad_fn=<AddBackward0>)\n",
      "loss=0.00521154934540391\n",
      "before backward() : W=tensor([0.9162], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1906], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9162], requires_grad=True), W.grad=tensor([-0.0202]), b=tensor([0.1906], requires_grad=True), b.grad=tensor([0.0458])\n",
      "after  step() : W=tensor([0.9164], requires_grad=True), W.grad=tensor([-0.0202]), b=tensor([0.1901], requires_grad=True), b.grad=tensor([0.0458])\n",
      "=======================275==================\n",
      "hypothesis=\n",
      "tensor([[1.1065],\n",
      "        [2.0229],\n",
      "        [2.9392]], grad_fn=<AddBackward0>)\n",
      "loss=0.005186531227082014\n",
      "before backward() : W=tensor([0.9164], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1901], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9164], requires_grad=True), W.grad=tensor([-0.0201]), b=tensor([0.1901], requires_grad=True), b.grad=tensor([0.0457])\n",
      "after  step() : W=tensor([0.9166], requires_grad=True), W.grad=tensor([-0.0201]), b=tensor([0.1897], requires_grad=True), b.grad=tensor([0.0457])\n",
      "=======================276==================\n",
      "hypothesis=\n",
      "tensor([[1.1062],\n",
      "        [2.0228],\n",
      "        [2.9394]], grad_fn=<AddBackward0>)\n",
      "loss=0.0051616220735013485\n",
      "before backward() : W=tensor([0.9166], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1897], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9166], requires_grad=True), W.grad=tensor([-0.0201]), b=tensor([0.1897], requires_grad=True), b.grad=tensor([0.0456])\n",
      "after  step() : W=tensor([0.9168], requires_grad=True), W.grad=tensor([-0.0201]), b=tensor([0.1892], requires_grad=True), b.grad=tensor([0.0456])\n",
      "=======================277==================\n",
      "hypothesis=\n",
      "tensor([[1.1060],\n",
      "        [2.0227],\n",
      "        [2.9395]], grad_fn=<AddBackward0>)\n",
      "loss=0.00513683445751667\n",
      "before backward() : W=tensor([0.9168], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1892], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9168], requires_grad=True), W.grad=tensor([-0.0200]), b=tensor([0.1892], requires_grad=True), b.grad=tensor([0.0455])\n",
      "after  step() : W=tensor([0.9170], requires_grad=True), W.grad=tensor([-0.0200]), b=tensor([0.1888], requires_grad=True), b.grad=tensor([0.0455])\n",
      "=======================278==================\n",
      "hypothesis=\n",
      "tensor([[1.1057],\n",
      "        [2.0227],\n",
      "        [2.9396]], grad_fn=<AddBackward0>)\n",
      "loss=0.005112180486321449\n",
      "before backward() : W=tensor([0.9170], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1888], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9170], requires_grad=True), W.grad=tensor([-0.0200]), b=tensor([0.1888], requires_grad=True), b.grad=tensor([0.0454])\n",
      "after  step() : W=tensor([0.9172], requires_grad=True), W.grad=tensor([-0.0200]), b=tensor([0.1883], requires_grad=True), b.grad=tensor([0.0454])\n",
      "=======================279==================\n",
      "hypothesis=\n",
      "tensor([[1.1055],\n",
      "        [2.0226],\n",
      "        [2.9398]], grad_fn=<AddBackward0>)\n",
      "loss=0.005087627097964287\n",
      "before backward() : W=tensor([0.9172], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1883], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9172], requires_grad=True), W.grad=tensor([-0.0199]), b=tensor([0.1883], requires_grad=True), b.grad=tensor([0.0453])\n",
      "after  step() : W=tensor([0.9174], requires_grad=True), W.grad=tensor([-0.0199]), b=tensor([0.1879], requires_grad=True), b.grad=tensor([0.0453])\n",
      "=======================280==================\n",
      "hypothesis=\n",
      "tensor([[1.1052],\n",
      "        [2.0226],\n",
      "        [2.9399]], grad_fn=<AddBackward0>)\n",
      "loss=0.005063189193606377\n",
      "before backward() : W=tensor([0.9174], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1879], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9174], requires_grad=True), W.grad=tensor([-0.0199]), b=tensor([0.1879], requires_grad=True), b.grad=tensor([0.0452])\n",
      "after  step() : W=tensor([0.9176], requires_grad=True), W.grad=tensor([-0.0199]), b=tensor([0.1874], requires_grad=True), b.grad=tensor([0.0452])\n",
      "=======================281==================\n",
      "hypothesis=\n",
      "tensor([[1.1050],\n",
      "        [2.0225],\n",
      "        [2.9401]], grad_fn=<AddBackward0>)\n",
      "loss=0.005038879346102476\n",
      "before backward() : W=tensor([0.9176], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1874], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9176], requires_grad=True), W.grad=tensor([-0.0198]), b=tensor([0.1874], requires_grad=True), b.grad=tensor([0.0451])\n",
      "after  step() : W=tensor([0.9178], requires_grad=True), W.grad=tensor([-0.0198]), b=tensor([0.1870], requires_grad=True), b.grad=tensor([0.0451])\n",
      "=======================282==================\n",
      "hypothesis=\n",
      "tensor([[1.1047],\n",
      "        [2.0225],\n",
      "        [2.9402]], grad_fn=<AddBackward0>)\n",
      "loss=0.005014690570533276\n",
      "before backward() : W=tensor([0.9178], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1870], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9178], requires_grad=True), W.grad=tensor([-0.0198]), b=tensor([0.1870], requires_grad=True), b.grad=tensor([0.0449])\n",
      "after  step() : W=tensor([0.9180], requires_grad=True), W.grad=tensor([-0.0198]), b=tensor([0.1865], requires_grad=True), b.grad=tensor([0.0449])\n",
      "=======================283==================\n",
      "hypothesis=\n",
      "tensor([[1.1045],\n",
      "        [2.0224],\n",
      "        [2.9404]], grad_fn=<AddBackward0>)\n",
      "loss=0.004990603309124708\n",
      "before backward() : W=tensor([0.9180], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1865], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9180], requires_grad=True), W.grad=tensor([-0.0197]), b=tensor([0.1865], requires_grad=True), b.grad=tensor([0.0448])\n",
      "after  step() : W=tensor([0.9181], requires_grad=True), W.grad=tensor([-0.0197]), b=tensor([0.1861], requires_grad=True), b.grad=tensor([0.0448])\n",
      "=======================284==================\n",
      "hypothesis=\n",
      "tensor([[1.1042],\n",
      "        [2.0224],\n",
      "        [2.9405]], grad_fn=<AddBackward0>)\n",
      "loss=0.004966637585312128\n",
      "before backward() : W=tensor([0.9181], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1861], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9181], requires_grad=True), W.grad=tensor([-0.0197]), b=tensor([0.1861], requires_grad=True), b.grad=tensor([0.0447])\n",
      "after  step() : W=tensor([0.9183], requires_grad=True), W.grad=tensor([-0.0197]), b=tensor([0.1856], requires_grad=True), b.grad=tensor([0.0447])\n",
      "=======================285==================\n",
      "hypothesis=\n",
      "tensor([[1.1040],\n",
      "        [2.0223],\n",
      "        [2.9407]], grad_fn=<AddBackward0>)\n",
      "loss=0.004942788276821375\n",
      "before backward() : W=tensor([0.9183], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1856], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9183], requires_grad=True), W.grad=tensor([-0.0196]), b=tensor([0.1856], requires_grad=True), b.grad=tensor([0.0446])\n",
      "after  step() : W=tensor([0.9185], requires_grad=True), W.grad=tensor([-0.0196]), b=tensor([0.1852], requires_grad=True), b.grad=tensor([0.0446])\n",
      "=======================286==================\n",
      "hypothesis=\n",
      "tensor([[1.1037],\n",
      "        [2.0223],\n",
      "        [2.9408]], grad_fn=<AddBackward0>)\n",
      "loss=0.0049190521240234375\n",
      "before backward() : W=tensor([0.9185], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1852], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9185], requires_grad=True), W.grad=tensor([-0.0196]), b=tensor([0.1852], requires_grad=True), b.grad=tensor([0.0445])\n",
      "after  step() : W=tensor([0.9187], requires_grad=True), W.grad=tensor([-0.0196]), b=tensor([0.1847], requires_grad=True), b.grad=tensor([0.0445])\n",
      "=======================287==================\n",
      "hypothesis=\n",
      "tensor([[1.1035],\n",
      "        [2.0222],\n",
      "        [2.9409]], grad_fn=<AddBackward0>)\n",
      "loss=0.004895426798611879\n",
      "before backward() : W=tensor([0.9187], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1847], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9187], requires_grad=True), W.grad=tensor([-0.0195]), b=tensor([0.1847], requires_grad=True), b.grad=tensor([0.0444])\n",
      "after  step() : W=tensor([0.9189], requires_grad=True), W.grad=tensor([-0.0195]), b=tensor([0.1843], requires_grad=True), b.grad=tensor([0.0444])\n",
      "=======================288==================\n",
      "hypothesis=\n",
      "tensor([[1.1032],\n",
      "        [2.0222],\n",
      "        [2.9411]], grad_fn=<AddBackward0>)\n",
      "loss=0.0048719169571995735\n",
      "before backward() : W=tensor([0.9189], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1843], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9189], requires_grad=True), W.grad=tensor([-0.0195]), b=tensor([0.1843], requires_grad=True), b.grad=tensor([0.0443])\n",
      "after  step() : W=tensor([0.9191], requires_grad=True), W.grad=tensor([-0.0195]), b=tensor([0.1838], requires_grad=True), b.grad=tensor([0.0443])\n",
      "=======================289==================\n",
      "hypothesis=\n",
      "tensor([[1.1030],\n",
      "        [2.0221],\n",
      "        [2.9412]], grad_fn=<AddBackward0>)\n",
      "loss=0.004848526790738106\n",
      "before backward() : W=tensor([0.9191], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1838], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9191], requires_grad=True), W.grad=tensor([-0.0194]), b=tensor([0.1838], requires_grad=True), b.grad=tensor([0.0442])\n",
      "after  step() : W=tensor([0.9193], requires_grad=True), W.grad=tensor([-0.0194]), b=tensor([0.1834], requires_grad=True), b.grad=tensor([0.0442])\n",
      "=======================290==================\n",
      "hypothesis=\n",
      "tensor([[1.1027],\n",
      "        [2.0220],\n",
      "        [2.9414]], grad_fn=<AddBackward0>)\n",
      "loss=0.004825252573937178\n",
      "before backward() : W=tensor([0.9193], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1834], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9193], requires_grad=True), W.grad=tensor([-0.0194]), b=tensor([0.1834], requires_grad=True), b.grad=tensor([0.0441])\n",
      "after  step() : W=tensor([0.9195], requires_grad=True), W.grad=tensor([-0.0194]), b=tensor([0.1830], requires_grad=True), b.grad=tensor([0.0441])\n",
      "=======================291==================\n",
      "hypothesis=\n",
      "tensor([[1.1025],\n",
      "        [2.0220],\n",
      "        [2.9415]], grad_fn=<AddBackward0>)\n",
      "loss=0.0048020705580711365\n",
      "before backward() : W=tensor([0.9195], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1830], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9195], requires_grad=True), W.grad=tensor([-0.0193]), b=tensor([0.1830], requires_grad=True), b.grad=tensor([0.0440])\n",
      "after  step() : W=tensor([0.9197], requires_grad=True), W.grad=tensor([-0.0193]), b=tensor([0.1825], requires_grad=True), b.grad=tensor([0.0440])\n",
      "=======================292==================\n",
      "hypothesis=\n",
      "tensor([[1.1022],\n",
      "        [2.0219],\n",
      "        [2.9416]], grad_fn=<AddBackward0>)\n",
      "loss=0.0047790114767849445\n",
      "before backward() : W=tensor([0.9197], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1825], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9197], requires_grad=True), W.grad=tensor([-0.0193]), b=tensor([0.1825], requires_grad=True), b.grad=tensor([0.0439])\n",
      "after  step() : W=tensor([0.9199], requires_grad=True), W.grad=tensor([-0.0193]), b=tensor([0.1821], requires_grad=True), b.grad=tensor([0.0439])\n",
      "=======================293==================\n",
      "hypothesis=\n",
      "tensor([[1.1020],\n",
      "        [2.0219],\n",
      "        [2.9418]], grad_fn=<AddBackward0>)\n",
      "loss=0.004756062291562557\n",
      "before backward() : W=tensor([0.9199], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1821], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9199], requires_grad=True), W.grad=tensor([-0.0193]), b=tensor([0.1821], requires_grad=True), b.grad=tensor([0.0438])\n",
      "after  step() : W=tensor([0.9201], requires_grad=True), W.grad=tensor([-0.0193]), b=tensor([0.1816], requires_grad=True), b.grad=tensor([0.0438])\n",
      "=======================294==================\n",
      "hypothesis=\n",
      "tensor([[1.1017],\n",
      "        [2.0218],\n",
      "        [2.9419]], grad_fn=<AddBackward0>)\n",
      "loss=0.004733229521661997\n",
      "before backward() : W=tensor([0.9201], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1816], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9201], requires_grad=True), W.grad=tensor([-0.0192]), b=tensor([0.1816], requires_grad=True), b.grad=tensor([0.0437])\n",
      "after  step() : W=tensor([0.9203], requires_grad=True), W.grad=tensor([-0.0192]), b=tensor([0.1812], requires_grad=True), b.grad=tensor([0.0437])\n",
      "=======================295==================\n",
      "hypothesis=\n",
      "tensor([[1.1015],\n",
      "        [2.0218],\n",
      "        [2.9421]], grad_fn=<AddBackward0>)\n",
      "loss=0.004710493143647909\n",
      "before backward() : W=tensor([0.9203], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1812], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9203], requires_grad=True), W.grad=tensor([-0.0192]), b=tensor([0.1812], requires_grad=True), b.grad=tensor([0.0436])\n",
      "after  step() : W=tensor([0.9205], requires_grad=True), W.grad=tensor([-0.0192]), b=tensor([0.1808], requires_grad=True), b.grad=tensor([0.0436])\n",
      "=======================296==================\n",
      "hypothesis=\n",
      "tensor([[1.1012],\n",
      "        [2.0217],\n",
      "        [2.9422]], grad_fn=<AddBackward0>)\n",
      "loss=0.004687868990004063\n",
      "before backward() : W=tensor([0.9205], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1808], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9205], requires_grad=True), W.grad=tensor([-0.0191]), b=tensor([0.1808], requires_grad=True), b.grad=tensor([0.0435])\n",
      "after  step() : W=tensor([0.9207], requires_grad=True), W.grad=tensor([-0.0191]), b=tensor([0.1803], requires_grad=True), b.grad=tensor([0.0435])\n",
      "=======================297==================\n",
      "hypothesis=\n",
      "tensor([[1.1010],\n",
      "        [2.0217],\n",
      "        [2.9423]], grad_fn=<AddBackward0>)\n",
      "loss=0.004665362183004618\n",
      "before backward() : W=tensor([0.9207], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1803], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9207], requires_grad=True), W.grad=tensor([-0.0191]), b=tensor([0.1803], requires_grad=True), b.grad=tensor([0.0434])\n",
      "after  step() : W=tensor([0.9209], requires_grad=True), W.grad=tensor([-0.0191]), b=tensor([0.1799], requires_grad=True), b.grad=tensor([0.0434])\n",
      "=======================298==================\n",
      "hypothesis=\n",
      "tensor([[1.1008],\n",
      "        [2.0216],\n",
      "        [2.9425]], grad_fn=<AddBackward0>)\n",
      "loss=0.004642959218472242\n",
      "before backward() : W=tensor([0.9209], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1799], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9209], requires_grad=True), W.grad=tensor([-0.0190]), b=tensor([0.1799], requires_grad=True), b.grad=tensor([0.0432])\n",
      "after  step() : W=tensor([0.9211], requires_grad=True), W.grad=tensor([-0.0190]), b=tensor([0.1795], requires_grad=True), b.grad=tensor([0.0432])\n",
      "=======================299==================\n",
      "hypothesis=\n",
      "tensor([[1.1005],\n",
      "        [2.0216],\n",
      "        [2.9426]], grad_fn=<AddBackward0>)\n",
      "loss=0.004620662424713373\n",
      "before backward() : W=tensor([0.9211], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1795], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9211], requires_grad=True), W.grad=tensor([-0.0190]), b=tensor([0.1795], requires_grad=True), b.grad=tensor([0.0431])\n",
      "after  step() : W=tensor([0.9212], requires_grad=True), W.grad=tensor([-0.0190]), b=tensor([0.1790], requires_grad=True), b.grad=tensor([0.0431])\n",
      "=======================300==================\n",
      "hypothesis=\n",
      "tensor([[1.1003],\n",
      "        [2.0215],\n",
      "        [2.9428]], grad_fn=<AddBackward0>)\n",
      "loss=0.004598480183631182\n",
      "before backward() : W=tensor([0.9212], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1790], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9212], requires_grad=True), W.grad=tensor([-0.0189]), b=tensor([0.1790], requires_grad=True), b.grad=tensor([0.0430])\n",
      "after  step() : W=tensor([0.9214], requires_grad=True), W.grad=tensor([-0.0189]), b=tensor([0.1786], requires_grad=True), b.grad=tensor([0.0430])\n",
      "=======================301==================\n",
      "hypothesis=\n",
      "tensor([[1.1000],\n",
      "        [2.0215],\n",
      "        [2.9429]], grad_fn=<AddBackward0>)\n",
      "loss=0.004576392006129026\n",
      "before backward() : W=tensor([0.9214], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1786], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9214], requires_grad=True), W.grad=tensor([-0.0189]), b=tensor([0.1786], requires_grad=True), b.grad=tensor([0.0429])\n",
      "after  step() : W=tensor([0.9216], requires_grad=True), W.grad=tensor([-0.0189]), b=tensor([0.1782], requires_grad=True), b.grad=tensor([0.0429])\n",
      "=======================302==================\n",
      "hypothesis=\n",
      "tensor([[1.0998],\n",
      "        [2.0214],\n",
      "        [2.9430]], grad_fn=<AddBackward0>)\n",
      "loss=0.0045544179156422615\n",
      "before backward() : W=tensor([0.9216], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1782], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9216], requires_grad=True), W.grad=tensor([-0.0188]), b=tensor([0.1782], requires_grad=True), b.grad=tensor([0.0428])\n",
      "after  step() : W=tensor([0.9218], requires_grad=True), W.grad=tensor([-0.0188]), b=tensor([0.1778], requires_grad=True), b.grad=tensor([0.0428])\n",
      "=======================303==================\n",
      "hypothesis=\n",
      "tensor([[1.0996],\n",
      "        [2.0214],\n",
      "        [2.9432]], grad_fn=<AddBackward0>)\n",
      "loss=0.004532555118203163\n",
      "before backward() : W=tensor([0.9218], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1778], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9218], requires_grad=True), W.grad=tensor([-0.0188]), b=tensor([0.1778], requires_grad=True), b.grad=tensor([0.0427])\n",
      "after  step() : W=tensor([0.9220], requires_grad=True), W.grad=tensor([-0.0188]), b=tensor([0.1773], requires_grad=True), b.grad=tensor([0.0427])\n",
      "=======================304==================\n",
      "hypothesis=\n",
      "tensor([[1.0993],\n",
      "        [2.0213],\n",
      "        [2.9433]], grad_fn=<AddBackward0>)\n",
      "loss=0.004510775674134493\n",
      "before backward() : W=tensor([0.9220], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1773], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9220], requires_grad=True), W.grad=tensor([-0.0188]), b=tensor([0.1773], requires_grad=True), b.grad=tensor([0.0426])\n",
      "after  step() : W=tensor([0.9222], requires_grad=True), W.grad=tensor([-0.0188]), b=tensor([0.1769], requires_grad=True), b.grad=tensor([0.0426])\n",
      "=======================305==================\n",
      "hypothesis=\n",
      "tensor([[1.0991],\n",
      "        [2.0213],\n",
      "        [2.9434]], grad_fn=<AddBackward0>)\n",
      "loss=0.004489116836339235\n",
      "before backward() : W=tensor([0.9222], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1769], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9222], requires_grad=True), W.grad=tensor([-0.0187]), b=tensor([0.1769], requires_grad=True), b.grad=tensor([0.0425])\n",
      "after  step() : W=tensor([0.9224], requires_grad=True), W.grad=tensor([-0.0187]), b=tensor([0.1765], requires_grad=True), b.grad=tensor([0.0425])\n",
      "=======================306==================\n",
      "hypothesis=\n",
      "tensor([[1.0988],\n",
      "        [2.0212],\n",
      "        [2.9436]], grad_fn=<AddBackward0>)\n",
      "loss=0.004467555787414312\n",
      "before backward() : W=tensor([0.9224], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1765], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9224], requires_grad=True), W.grad=tensor([-0.0187]), b=tensor([0.1765], requires_grad=True), b.grad=tensor([0.0424])\n",
      "after  step() : W=tensor([0.9226], requires_grad=True), W.grad=tensor([-0.0187]), b=tensor([0.1760], requires_grad=True), b.grad=tensor([0.0424])\n",
      "=======================307==================\n",
      "hypothesis=\n",
      "tensor([[1.0986],\n",
      "        [2.0212],\n",
      "        [2.9437]], grad_fn=<AddBackward0>)\n",
      "loss=0.004446110688149929\n",
      "before backward() : W=tensor([0.9226], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1760], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9226], requires_grad=True), W.grad=tensor([-0.0186]), b=tensor([0.1760], requires_grad=True), b.grad=tensor([0.0423])\n",
      "after  step() : W=tensor([0.9227], requires_grad=True), W.grad=tensor([-0.0186]), b=tensor([0.1756], requires_grad=True), b.grad=tensor([0.0423])\n",
      "=======================308==================\n",
      "hypothesis=\n",
      "tensor([[1.0984],\n",
      "        [2.0211],\n",
      "        [2.9439]], grad_fn=<AddBackward0>)\n",
      "loss=0.004424756392836571\n",
      "before backward() : W=tensor([0.9227], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1756], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9227], requires_grad=True), W.grad=tensor([-0.0186]), b=tensor([0.1756], requires_grad=True), b.grad=tensor([0.0422])\n",
      "after  step() : W=tensor([0.9229], requires_grad=True), W.grad=tensor([-0.0186]), b=tensor([0.1752], requires_grad=True), b.grad=tensor([0.0422])\n",
      "=======================309==================\n",
      "hypothesis=\n",
      "tensor([[1.0981],\n",
      "        [2.0211],\n",
      "        [2.9440]], grad_fn=<AddBackward0>)\n",
      "loss=0.004403506405651569\n",
      "before backward() : W=tensor([0.9229], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1752], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9229], requires_grad=True), W.grad=tensor([-0.0185]), b=tensor([0.1752], requires_grad=True), b.grad=tensor([0.0421])\n",
      "after  step() : W=tensor([0.9231], requires_grad=True), W.grad=tensor([-0.0185]), b=tensor([0.1748], requires_grad=True), b.grad=tensor([0.0421])\n",
      "=======================310==================\n",
      "hypothesis=\n",
      "tensor([[1.0979],\n",
      "        [2.0210],\n",
      "        [2.9441]], grad_fn=<AddBackward0>)\n",
      "loss=0.004382358398288488\n",
      "before backward() : W=tensor([0.9231], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1748], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9231], requires_grad=True), W.grad=tensor([-0.0185]), b=tensor([0.1748], requires_grad=True), b.grad=tensor([0.0420])\n",
      "after  step() : W=tensor([0.9233], requires_grad=True), W.grad=tensor([-0.0185]), b=tensor([0.1744], requires_grad=True), b.grad=tensor([0.0420])\n",
      "=======================311==================\n",
      "hypothesis=\n",
      "tensor([[1.0977],\n",
      "        [2.0210],\n",
      "        [2.9443]], grad_fn=<AddBackward0>)\n",
      "loss=0.0043613179586827755\n",
      "before backward() : W=tensor([0.9233], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1744], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9233], requires_grad=True), W.grad=tensor([-0.0184]), b=tensor([0.1744], requires_grad=True), b.grad=tensor([0.0419])\n",
      "after  step() : W=tensor([0.9235], requires_grad=True), W.grad=tensor([-0.0184]), b=tensor([0.1739], requires_grad=True), b.grad=tensor([0.0419])\n",
      "=======================312==================\n",
      "hypothesis=\n",
      "tensor([[1.0974],\n",
      "        [2.0209],\n",
      "        [2.9444]], grad_fn=<AddBackward0>)\n",
      "loss=0.004340370185673237\n",
      "before backward() : W=tensor([0.9235], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1739], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9235], requires_grad=True), W.grad=tensor([-0.0184]), b=tensor([0.1739], requires_grad=True), b.grad=tensor([0.0418])\n",
      "after  step() : W=tensor([0.9237], requires_grad=True), W.grad=tensor([-0.0184]), b=tensor([0.1735], requires_grad=True), b.grad=tensor([0.0418])\n",
      "=======================313==================\n",
      "hypothesis=\n",
      "tensor([[1.0972],\n",
      "        [2.0209],\n",
      "        [2.9445]], grad_fn=<AddBackward0>)\n",
      "loss=0.004319529514759779\n",
      "before backward() : W=tensor([0.9237], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1735], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9237], requires_grad=True), W.grad=tensor([-0.0184]), b=tensor([0.1735], requires_grad=True), b.grad=tensor([0.0417])\n",
      "after  step() : W=tensor([0.9239], requires_grad=True), W.grad=tensor([-0.0184]), b=tensor([0.1731], requires_grad=True), b.grad=tensor([0.0417])\n",
      "=======================314==================\n",
      "hypothesis=\n",
      "tensor([[1.0970],\n",
      "        [2.0208],\n",
      "        [2.9447]], grad_fn=<AddBackward0>)\n",
      "loss=0.004298781510442495\n",
      "before backward() : W=tensor([0.9239], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1731], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9239], requires_grad=True), W.grad=tensor([-0.0183]), b=tensor([0.1731], requires_grad=True), b.grad=tensor([0.0416])\n",
      "after  step() : W=tensor([0.9240], requires_grad=True), W.grad=tensor([-0.0183]), b=tensor([0.1727], requires_grad=True), b.grad=tensor([0.0416])\n",
      "=======================315==================\n",
      "hypothesis=\n",
      "tensor([[1.0967],\n",
      "        [2.0208],\n",
      "        [2.9448]], grad_fn=<AddBackward0>)\n",
      "loss=0.004278139676898718\n",
      "before backward() : W=tensor([0.9240], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1727], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9240], requires_grad=True), W.grad=tensor([-0.0183]), b=tensor([0.1727], requires_grad=True), b.grad=tensor([0.0415])\n",
      "after  step() : W=tensor([0.9242], requires_grad=True), W.grad=tensor([-0.0183]), b=tensor([0.1723], requires_grad=True), b.grad=tensor([0.0415])\n",
      "=======================316==================\n",
      "hypothesis=\n",
      "tensor([[1.0965],\n",
      "        [2.0207],\n",
      "        [2.9449]], grad_fn=<AddBackward0>)\n",
      "loss=0.004257597494870424\n",
      "before backward() : W=tensor([0.9242], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1723], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9242], requires_grad=True), W.grad=tensor([-0.0182]), b=tensor([0.1723], requires_grad=True), b.grad=tensor([0.0414])\n",
      "after  step() : W=tensor([0.9244], requires_grad=True), W.grad=tensor([-0.0182]), b=tensor([0.1719], requires_grad=True), b.grad=tensor([0.0414])\n",
      "=======================317==================\n",
      "hypothesis=\n",
      "tensor([[1.0963],\n",
      "        [2.0207],\n",
      "        [2.9451]], grad_fn=<AddBackward0>)\n",
      "loss=0.004237152636051178\n",
      "before backward() : W=tensor([0.9244], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1719], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9244], requires_grad=True), W.grad=tensor([-0.0182]), b=tensor([0.1719], requires_grad=True), b.grad=tensor([0.0413])\n",
      "after  step() : W=tensor([0.9246], requires_grad=True), W.grad=tensor([-0.0182]), b=tensor([0.1714], requires_grad=True), b.grad=tensor([0.0413])\n",
      "=======================318==================\n",
      "hypothesis=\n",
      "tensor([[1.0960],\n",
      "        [2.0206],\n",
      "        [2.9452]], grad_fn=<AddBackward0>)\n",
      "loss=0.004216799978166819\n",
      "before backward() : W=tensor([0.9246], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1714], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9246], requires_grad=True), W.grad=tensor([-0.0181]), b=tensor([0.1714], requires_grad=True), b.grad=tensor([0.0412])\n",
      "after  step() : W=tensor([0.9248], requires_grad=True), W.grad=tensor([-0.0181]), b=tensor([0.1710], requires_grad=True), b.grad=tensor([0.0412])\n",
      "=======================319==================\n",
      "hypothesis=\n",
      "tensor([[1.0958],\n",
      "        [2.0206],\n",
      "        [2.9453]], grad_fn=<AddBackward0>)\n",
      "loss=0.004196558613330126\n",
      "before backward() : W=tensor([0.9248], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1710], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9248], requires_grad=True), W.grad=tensor([-0.0181]), b=tensor([0.1710], requires_grad=True), b.grad=tensor([0.0411])\n",
      "after  step() : W=tensor([0.9249], requires_grad=True), W.grad=tensor([-0.0181]), b=tensor([0.1706], requires_grad=True), b.grad=tensor([0.0411])\n",
      "=======================320==================\n",
      "hypothesis=\n",
      "tensor([[1.0956],\n",
      "        [2.0205],\n",
      "        [2.9455]], grad_fn=<AddBackward0>)\n",
      "loss=0.004176408052444458\n",
      "before backward() : W=tensor([0.9249], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1706], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9249], requires_grad=True), W.grad=tensor([-0.0180]), b=tensor([0.1706], requires_grad=True), b.grad=tensor([0.0410])\n",
      "after  step() : W=tensor([0.9251], requires_grad=True), W.grad=tensor([-0.0180]), b=tensor([0.1702], requires_grad=True), b.grad=tensor([0.0410])\n",
      "=======================321==================\n",
      "hypothesis=\n",
      "tensor([[1.0953],\n",
      "        [2.0205],\n",
      "        [2.9456]], grad_fn=<AddBackward0>)\n",
      "loss=0.004156345967203379\n",
      "before backward() : W=tensor([0.9251], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1702], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9251], requires_grad=True), W.grad=tensor([-0.0180]), b=tensor([0.1702], requires_grad=True), b.grad=tensor([0.0409])\n",
      "after  step() : W=tensor([0.9253], requires_grad=True), W.grad=tensor([-0.0180]), b=tensor([0.1698], requires_grad=True), b.grad=tensor([0.0409])\n",
      "=======================322==================\n",
      "hypothesis=\n",
      "tensor([[1.0951],\n",
      "        [2.0204],\n",
      "        [2.9457]], grad_fn=<AddBackward0>)\n",
      "loss=0.004136397037655115\n",
      "before backward() : W=tensor([0.9253], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1698], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9253], requires_grad=True), W.grad=tensor([-0.0180]), b=tensor([0.1698], requires_grad=True), b.grad=tensor([0.0408])\n",
      "after  step() : W=tensor([0.9255], requires_grad=True), W.grad=tensor([-0.0180]), b=tensor([0.1694], requires_grad=True), b.grad=tensor([0.0408])\n",
      "=======================323==================\n",
      "hypothesis=\n",
      "tensor([[1.0949],\n",
      "        [2.0204],\n",
      "        [2.9458]], grad_fn=<AddBackward0>)\n",
      "loss=0.004116527270525694\n",
      "before backward() : W=tensor([0.9255], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1694], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9255], requires_grad=True), W.grad=tensor([-0.0179]), b=tensor([0.1694], requires_grad=True), b.grad=tensor([0.0407])\n",
      "after  step() : W=tensor([0.9257], requires_grad=True), W.grad=tensor([-0.0179]), b=tensor([0.1690], requires_grad=True), b.grad=tensor([0.0407])\n",
      "=======================324==================\n",
      "hypothesis=\n",
      "tensor([[1.0947],\n",
      "        [2.0203],\n",
      "        [2.9460]], grad_fn=<AddBackward0>)\n",
      "loss=0.0040967632085084915\n",
      "before backward() : W=tensor([0.9257], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1690], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9257], requires_grad=True), W.grad=tensor([-0.0179]), b=tensor([0.1690], requires_grad=True), b.grad=tensor([0.0406])\n",
      "after  step() : W=tensor([0.9258], requires_grad=True), W.grad=tensor([-0.0179]), b=tensor([0.1686], requires_grad=True), b.grad=tensor([0.0406])\n",
      "=======================325==================\n",
      "hypothesis=\n",
      "tensor([[1.0944],\n",
      "        [2.0203],\n",
      "        [2.9461]], grad_fn=<AddBackward0>)\n",
      "loss=0.004077093210071325\n",
      "before backward() : W=tensor([0.9258], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1686], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9258], requires_grad=True), W.grad=tensor([-0.0178]), b=tensor([0.1686], requires_grad=True), b.grad=tensor([0.0405])\n",
      "after  step() : W=tensor([0.9260], requires_grad=True), W.grad=tensor([-0.0178]), b=tensor([0.1682], requires_grad=True), b.grad=tensor([0.0405])\n",
      "=======================326==================\n",
      "hypothesis=\n",
      "tensor([[1.0942],\n",
      "        [2.0202],\n",
      "        [2.9462]], grad_fn=<AddBackward0>)\n",
      "loss=0.0040575116872787476\n",
      "before backward() : W=tensor([0.9260], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1682], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9260], requires_grad=True), W.grad=tensor([-0.0178]), b=tensor([0.1682], requires_grad=True), b.grad=tensor([0.0404])\n",
      "after  step() : W=tensor([0.9262], requires_grad=True), W.grad=tensor([-0.0178]), b=tensor([0.1678], requires_grad=True), b.grad=tensor([0.0404])\n",
      "=======================327==================\n",
      "hypothesis=\n",
      "tensor([[1.0940],\n",
      "        [2.0202],\n",
      "        [2.9464]], grad_fn=<AddBackward0>)\n",
      "loss=0.004038024228066206\n",
      "before backward() : W=tensor([0.9262], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1678], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9262], requires_grad=True), W.grad=tensor([-0.0177]), b=tensor([0.1678], requires_grad=True), b.grad=tensor([0.0403])\n",
      "after  step() : W=tensor([0.9264], requires_grad=True), W.grad=tensor([-0.0177]), b=tensor([0.1674], requires_grad=True), b.grad=tensor([0.0403])\n",
      "=======================328==================\n",
      "hypothesis=\n",
      "tensor([[1.0937],\n",
      "        [2.0201],\n",
      "        [2.9465]], grad_fn=<AddBackward0>)\n",
      "loss=0.004018640145659447\n",
      "before backward() : W=tensor([0.9264], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1674], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9264], requires_grad=True), W.grad=tensor([-0.0177]), b=tensor([0.1674], requires_grad=True), b.grad=tensor([0.0402])\n",
      "after  step() : W=tensor([0.9266], requires_grad=True), W.grad=tensor([-0.0177]), b=tensor([0.1670], requires_grad=True), b.grad=tensor([0.0402])\n",
      "=======================329==================\n",
      "hypothesis=\n",
      "tensor([[1.0935],\n",
      "        [2.0201],\n",
      "        [2.9466]], grad_fn=<AddBackward0>)\n",
      "loss=0.003999341744929552\n",
      "before backward() : W=tensor([0.9266], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1670], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9266], requires_grad=True), W.grad=tensor([-0.0177]), b=tensor([0.1670], requires_grad=True), b.grad=tensor([0.0401])\n",
      "after  step() : W=tensor([0.9267], requires_grad=True), W.grad=tensor([-0.0177]), b=tensor([0.1666], requires_grad=True), b.grad=tensor([0.0401])\n",
      "=======================330==================\n",
      "hypothesis=\n",
      "tensor([[1.0933],\n",
      "        [2.0200],\n",
      "        [2.9467]], grad_fn=<AddBackward0>)\n",
      "loss=0.0039801266975700855\n",
      "before backward() : W=tensor([0.9267], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1666], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9267], requires_grad=True), W.grad=tensor([-0.0176]), b=tensor([0.1666], requires_grad=True), b.grad=tensor([0.0400])\n",
      "after  step() : W=tensor([0.9269], requires_grad=True), W.grad=tensor([-0.0176]), b=tensor([0.1662], requires_grad=True), b.grad=tensor([0.0400])\n",
      "=======================331==================\n",
      "hypothesis=\n",
      "tensor([[1.0931],\n",
      "        [2.0200],\n",
      "        [2.9469]], grad_fn=<AddBackward0>)\n",
      "loss=0.00396102387458086\n",
      "before backward() : W=tensor([0.9269], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1662], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9269], requires_grad=True), W.grad=tensor([-0.0176]), b=tensor([0.1662], requires_grad=True), b.grad=tensor([0.0399])\n",
      "after  step() : W=tensor([0.9271], requires_grad=True), W.grad=tensor([-0.0176]), b=tensor([0.1658], requires_grad=True), b.grad=tensor([0.0399])\n",
      "=======================332==================\n",
      "hypothesis=\n",
      "tensor([[1.0928],\n",
      "        [2.0199],\n",
      "        [2.9470]], grad_fn=<AddBackward0>)\n",
      "loss=0.00394199974834919\n",
      "before backward() : W=tensor([0.9271], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1658], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9271], requires_grad=True), W.grad=tensor([-0.0175]), b=tensor([0.1658], requires_grad=True), b.grad=tensor([0.0398])\n",
      "after  step() : W=tensor([0.9273], requires_grad=True), W.grad=tensor([-0.0175]), b=tensor([0.1654], requires_grad=True), b.grad=tensor([0.0398])\n",
      "=======================333==================\n",
      "hypothesis=\n",
      "tensor([[1.0926],\n",
      "        [2.0199],\n",
      "        [2.9471]], grad_fn=<AddBackward0>)\n",
      "loss=0.003923074807971716\n",
      "before backward() : W=tensor([0.9273], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1654], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9273], requires_grad=True), W.grad=tensor([-0.0175]), b=tensor([0.1654], requires_grad=True), b.grad=tensor([0.0398])\n",
      "after  step() : W=tensor([0.9274], requires_grad=True), W.grad=tensor([-0.0175]), b=tensor([0.1650], requires_grad=True), b.grad=tensor([0.0398])\n",
      "=======================334==================\n",
      "hypothesis=\n",
      "tensor([[1.0924],\n",
      "        [2.0198],\n",
      "        [2.9473]], grad_fn=<AddBackward0>)\n",
      "loss=0.0039042362477630377\n",
      "before backward() : W=tensor([0.9274], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1650], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9274], requires_grad=True), W.grad=tensor([-0.0174]), b=tensor([0.1650], requires_grad=True), b.grad=tensor([0.0397])\n",
      "after  step() : W=tensor([0.9276], requires_grad=True), W.grad=tensor([-0.0174]), b=tensor([0.1646], requires_grad=True), b.grad=tensor([0.0397])\n",
      "=======================335==================\n",
      "hypothesis=\n",
      "tensor([[1.0922],\n",
      "        [2.0198],\n",
      "        [2.9474]], grad_fn=<AddBackward0>)\n",
      "loss=0.0038854721933603287\n",
      "before backward() : W=tensor([0.9276], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1646], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9276], requires_grad=True), W.grad=tensor([-0.0174]), b=tensor([0.1646], requires_grad=True), b.grad=tensor([0.0396])\n",
      "after  step() : W=tensor([0.9278], requires_grad=True), W.grad=tensor([-0.0174]), b=tensor([0.1642], requires_grad=True), b.grad=tensor([0.0396])\n",
      "=======================336==================\n",
      "hypothesis=\n",
      "tensor([[1.0920],\n",
      "        [2.0197],\n",
      "        [2.9475]], grad_fn=<AddBackward0>)\n",
      "loss=0.0038668224588036537\n",
      "before backward() : W=tensor([0.9278], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1642], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9278], requires_grad=True), W.grad=tensor([-0.0174]), b=tensor([0.1642], requires_grad=True), b.grad=tensor([0.0395])\n",
      "after  step() : W=tensor([0.9280], requires_grad=True), W.grad=tensor([-0.0174]), b=tensor([0.1638], requires_grad=True), b.grad=tensor([0.0395])\n",
      "=======================337==================\n",
      "hypothesis=\n",
      "tensor([[1.0917],\n",
      "        [2.0197],\n",
      "        [2.9476]], grad_fn=<AddBackward0>)\n",
      "loss=0.0038482581730931997\n",
      "before backward() : W=tensor([0.9280], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1638], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9280], requires_grad=True), W.grad=tensor([-0.0173]), b=tensor([0.1638], requires_grad=True), b.grad=tensor([0.0394])\n",
      "after  step() : W=tensor([0.9281], requires_grad=True), W.grad=tensor([-0.0173]), b=tensor([0.1634], requires_grad=True), b.grad=tensor([0.0394])\n",
      "=======================338==================\n",
      "hypothesis=\n",
      "tensor([[1.0915],\n",
      "        [2.0196],\n",
      "        [2.9478]], grad_fn=<AddBackward0>)\n",
      "loss=0.003829770488664508\n",
      "before backward() : W=tensor([0.9281], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1634], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9281], requires_grad=True), W.grad=tensor([-0.0173]), b=tensor([0.1634], requires_grad=True), b.grad=tensor([0.0393])\n",
      "after  step() : W=tensor([0.9283], requires_grad=True), W.grad=tensor([-0.0173]), b=tensor([0.1630], requires_grad=True), b.grad=tensor([0.0393])\n",
      "=======================339==================\n",
      "hypothesis=\n",
      "tensor([[1.0913],\n",
      "        [2.0196],\n",
      "        [2.9479]], grad_fn=<AddBackward0>)\n",
      "loss=0.003811381757259369\n",
      "before backward() : W=tensor([0.9283], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1630], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9283], requires_grad=True), W.grad=tensor([-0.0172]), b=tensor([0.1630], requires_grad=True), b.grad=tensor([0.0392])\n",
      "after  step() : W=tensor([0.9285], requires_grad=True), W.grad=tensor([-0.0172]), b=tensor([0.1626], requires_grad=True), b.grad=tensor([0.0392])\n",
      "=======================340==================\n",
      "hypothesis=\n",
      "tensor([[1.0911],\n",
      "        [2.0195],\n",
      "        [2.9480]], grad_fn=<AddBackward0>)\n",
      "loss=0.003793076379224658\n",
      "before backward() : W=tensor([0.9285], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1626], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9285], requires_grad=True), W.grad=tensor([-0.0172]), b=tensor([0.1626], requires_grad=True), b.grad=tensor([0.0391])\n",
      "after  step() : W=tensor([0.9286], requires_grad=True), W.grad=tensor([-0.0172]), b=tensor([0.1622], requires_grad=True), b.grad=tensor([0.0391])\n",
      "=======================341==================\n",
      "hypothesis=\n",
      "tensor([[1.0909],\n",
      "        [2.0195],\n",
      "        [2.9481]], grad_fn=<AddBackward0>)\n",
      "loss=0.003774867160245776\n",
      "before backward() : W=tensor([0.9286], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1622], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9286], requires_grad=True), W.grad=tensor([-0.0172]), b=tensor([0.1622], requires_grad=True), b.grad=tensor([0.0390])\n",
      "after  step() : W=tensor([0.9288], requires_grad=True), W.grad=tensor([-0.0172]), b=tensor([0.1618], requires_grad=True), b.grad=tensor([0.0390])\n",
      "=======================342==================\n",
      "hypothesis=\n",
      "tensor([[1.0906],\n",
      "        [2.0195],\n",
      "        [2.9483]], grad_fn=<AddBackward0>)\n",
      "loss=0.003756740363314748\n",
      "before backward() : W=tensor([0.9288], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1618], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9288], requires_grad=True), W.grad=tensor([-0.0171]), b=tensor([0.1618], requires_grad=True), b.grad=tensor([0.0389])\n",
      "after  step() : W=tensor([0.9290], requires_grad=True), W.grad=tensor([-0.0171]), b=tensor([0.1614], requires_grad=True), b.grad=tensor([0.0389])\n",
      "=======================343==================\n",
      "hypothesis=\n",
      "tensor([[1.0904],\n",
      "        [2.0194],\n",
      "        [2.9484]], grad_fn=<AddBackward0>)\n",
      "loss=0.003738703206181526\n",
      "before backward() : W=tensor([0.9290], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1614], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9290], requires_grad=True), W.grad=tensor([-0.0171]), b=tensor([0.1614], requires_grad=True), b.grad=tensor([0.0388])\n",
      "after  step() : W=tensor([0.9292], requires_grad=True), W.grad=tensor([-0.0171]), b=tensor([0.1610], requires_grad=True), b.grad=tensor([0.0388])\n",
      "=======================344==================\n",
      "hypothesis=\n",
      "tensor([[1.0902],\n",
      "        [2.0194],\n",
      "        [2.9485]], grad_fn=<AddBackward0>)\n",
      "loss=0.003720747074112296\n",
      "before backward() : W=tensor([0.9292], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1610], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9292], requires_grad=True), W.grad=tensor([-0.0170]), b=tensor([0.1610], requires_grad=True), b.grad=tensor([0.0387])\n",
      "after  step() : W=tensor([0.9293], requires_grad=True), W.grad=tensor([-0.0170]), b=tensor([0.1607], requires_grad=True), b.grad=tensor([0.0387])\n",
      "=======================345==================\n",
      "hypothesis=\n",
      "tensor([[1.0900],\n",
      "        [2.0193],\n",
      "        [2.9486]], grad_fn=<AddBackward0>)\n",
      "loss=0.003702881745994091\n",
      "before backward() : W=tensor([0.9293], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1607], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9293], requires_grad=True), W.grad=tensor([-0.0170]), b=tensor([0.1607], requires_grad=True), b.grad=tensor([0.0386])\n",
      "after  step() : W=tensor([0.9295], requires_grad=True), W.grad=tensor([-0.0170]), b=tensor([0.1603], requires_grad=True), b.grad=tensor([0.0386])\n",
      "=======================346==================\n",
      "hypothesis=\n",
      "tensor([[1.0898],\n",
      "        [2.0193],\n",
      "        [2.9488]], grad_fn=<AddBackward0>)\n",
      "loss=0.0036850955802947283\n",
      "before backward() : W=tensor([0.9295], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1603], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9295], requires_grad=True), W.grad=tensor([-0.0169]), b=tensor([0.1603], requires_grad=True), b.grad=tensor([0.0385])\n",
      "after  step() : W=tensor([0.9297], requires_grad=True), W.grad=tensor([-0.0169]), b=tensor([0.1599], requires_grad=True), b.grad=tensor([0.0385])\n",
      "=======================347==================\n",
      "hypothesis=\n",
      "tensor([[1.0896],\n",
      "        [2.0192],\n",
      "        [2.9489]], grad_fn=<AddBackward0>)\n",
      "loss=0.0036673990543931723\n",
      "before backward() : W=tensor([0.9297], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1599], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9297], requires_grad=True), W.grad=tensor([-0.0169]), b=tensor([0.1599], requires_grad=True), b.grad=tensor([0.0384])\n",
      "after  step() : W=tensor([0.9298], requires_grad=True), W.grad=tensor([-0.0169]), b=tensor([0.1595], requires_grad=True), b.grad=tensor([0.0384])\n",
      "=======================348==================\n",
      "hypothesis=\n",
      "tensor([[1.0893],\n",
      "        [2.0192],\n",
      "        [2.9490]], grad_fn=<AddBackward0>)\n",
      "loss=0.003649782156571746\n",
      "before backward() : W=tensor([0.9298], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1595], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9298], requires_grad=True), W.grad=tensor([-0.0169]), b=tensor([0.1595], requires_grad=True), b.grad=tensor([0.0383])\n",
      "after  step() : W=tensor([0.9300], requires_grad=True), W.grad=tensor([-0.0169]), b=tensor([0.1591], requires_grad=True), b.grad=tensor([0.0383])\n",
      "=======================349==================\n",
      "hypothesis=\n",
      "tensor([[1.0891],\n",
      "        [2.0191],\n",
      "        [2.9491]], grad_fn=<AddBackward0>)\n",
      "loss=0.003632261650636792\n",
      "before backward() : W=tensor([0.9300], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1591], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9300], requires_grad=True), W.grad=tensor([-0.0168]), b=tensor([0.1591], requires_grad=True), b.grad=tensor([0.0383])\n",
      "after  step() : W=tensor([0.9302], requires_grad=True), W.grad=tensor([-0.0168]), b=tensor([0.1587], requires_grad=True), b.grad=tensor([0.0383])\n",
      "=======================350==================\n",
      "hypothesis=\n",
      "tensor([[1.0889],\n",
      "        [2.0191],\n",
      "        [2.9493]], grad_fn=<AddBackward0>)\n",
      "loss=0.0036148198414593935\n",
      "before backward() : W=tensor([0.9302], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1587], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9302], requires_grad=True), W.grad=tensor([-0.0168]), b=tensor([0.1587], requires_grad=True), b.grad=tensor([0.0382])\n",
      "after  step() : W=tensor([0.9303], requires_grad=True), W.grad=tensor([-0.0168]), b=tensor([0.1584], requires_grad=True), b.grad=tensor([0.0382])\n",
      "=======================351==================\n",
      "hypothesis=\n",
      "tensor([[1.0887],\n",
      "        [2.0190],\n",
      "        [2.9494]], grad_fn=<AddBackward0>)\n",
      "loss=0.0035974644124507904\n",
      "before backward() : W=tensor([0.9303], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1584], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9303], requires_grad=True), W.grad=tensor([-0.0167]), b=tensor([0.1584], requires_grad=True), b.grad=tensor([0.0381])\n",
      "after  step() : W=tensor([0.9305], requires_grad=True), W.grad=tensor([-0.0167]), b=tensor([0.1580], requires_grad=True), b.grad=tensor([0.0381])\n",
      "=======================352==================\n",
      "hypothesis=\n",
      "tensor([[1.0885],\n",
      "        [2.0190],\n",
      "        [2.9495]], grad_fn=<AddBackward0>)\n",
      "loss=0.003580184653401375\n",
      "before backward() : W=tensor([0.9305], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1580], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9305], requires_grad=True), W.grad=tensor([-0.0167]), b=tensor([0.1580], requires_grad=True), b.grad=tensor([0.0380])\n",
      "after  step() : W=tensor([0.9307], requires_grad=True), W.grad=tensor([-0.0167]), b=tensor([0.1576], requires_grad=True), b.grad=tensor([0.0380])\n",
      "=======================353==================\n",
      "hypothesis=\n",
      "tensor([[1.0883],\n",
      "        [2.0189],\n",
      "        [2.9496]], grad_fn=<AddBackward0>)\n",
      "loss=0.0035629949998110533\n",
      "before backward() : W=tensor([0.9307], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1576], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9307], requires_grad=True), W.grad=tensor([-0.0167]), b=tensor([0.1576], requires_grad=True), b.grad=tensor([0.0379])\n",
      "after  step() : W=tensor([0.9308], requires_grad=True), W.grad=tensor([-0.0167]), b=tensor([0.1572], requires_grad=True), b.grad=tensor([0.0379])\n",
      "=======================354==================\n",
      "hypothesis=\n",
      "tensor([[1.0881],\n",
      "        [2.0189],\n",
      "        [2.9497]], grad_fn=<AddBackward0>)\n",
      "loss=0.003545885207131505\n",
      "before backward() : W=tensor([0.9308], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1572], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9308], requires_grad=True), W.grad=tensor([-0.0166]), b=tensor([0.1572], requires_grad=True), b.grad=tensor([0.0378])\n",
      "after  step() : W=tensor([0.9310], requires_grad=True), W.grad=tensor([-0.0166]), b=tensor([0.1568], requires_grad=True), b.grad=tensor([0.0378])\n",
      "=======================355==================\n",
      "hypothesis=\n",
      "tensor([[1.0878],\n",
      "        [2.0189],\n",
      "        [2.9499]], grad_fn=<AddBackward0>)\n",
      "loss=0.0035288557410240173\n",
      "before backward() : W=tensor([0.9310], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1568], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9310], requires_grad=True), W.grad=tensor([-0.0166]), b=tensor([0.1568], requires_grad=True), b.grad=tensor([0.0377])\n",
      "after  step() : W=tensor([0.9312], requires_grad=True), W.grad=tensor([-0.0166]), b=tensor([0.1565], requires_grad=True), b.grad=tensor([0.0377])\n",
      "=======================356==================\n",
      "hypothesis=\n",
      "tensor([[1.0876],\n",
      "        [2.0188],\n",
      "        [2.9500]], grad_fn=<AddBackward0>)\n",
      "loss=0.0035119103267788887\n",
      "before backward() : W=tensor([0.9312], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1565], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9312], requires_grad=True), W.grad=tensor([-0.0165]), b=tensor([0.1565], requires_grad=True), b.grad=tensor([0.0376])\n",
      "after  step() : W=tensor([0.9313], requires_grad=True), W.grad=tensor([-0.0165]), b=tensor([0.1561], requires_grad=True), b.grad=tensor([0.0376])\n",
      "=======================357==================\n",
      "hypothesis=\n",
      "tensor([[1.0874],\n",
      "        [2.0188],\n",
      "        [2.9501]], grad_fn=<AddBackward0>)\n",
      "loss=0.0034950419794768095\n",
      "before backward() : W=tensor([0.9313], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1561], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9313], requires_grad=True), W.grad=tensor([-0.0165]), b=tensor([0.1561], requires_grad=True), b.grad=tensor([0.0375])\n",
      "after  step() : W=tensor([0.9315], requires_grad=True), W.grad=tensor([-0.0165]), b=tensor([0.1557], requires_grad=True), b.grad=tensor([0.0375])\n",
      "=======================358==================\n",
      "hypothesis=\n",
      "tensor([[1.0872],\n",
      "        [2.0187],\n",
      "        [2.9502]], grad_fn=<AddBackward0>)\n",
      "loss=0.003478260012343526\n",
      "before backward() : W=tensor([0.9315], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1557], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9315], requires_grad=True), W.grad=tensor([-0.0165]), b=tensor([0.1557], requires_grad=True), b.grad=tensor([0.0374])\n",
      "after  step() : W=tensor([0.9317], requires_grad=True), W.grad=tensor([-0.0165]), b=tensor([0.1553], requires_grad=True), b.grad=tensor([0.0374])\n",
      "=======================359==================\n",
      "hypothesis=\n",
      "tensor([[1.0870],\n",
      "        [2.0187],\n",
      "        [2.9503]], grad_fn=<AddBackward0>)\n",
      "loss=0.003461560932919383\n",
      "before backward() : W=tensor([0.9317], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1553], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9317], requires_grad=True), W.grad=tensor([-0.0164]), b=tensor([0.1553], requires_grad=True), b.grad=tensor([0.0373])\n",
      "after  step() : W=tensor([0.9318], requires_grad=True), W.grad=tensor([-0.0164]), b=tensor([0.1550], requires_grad=True), b.grad=tensor([0.0373])\n",
      "=======================360==================\n",
      "hypothesis=\n",
      "tensor([[1.0868],\n",
      "        [2.0186],\n",
      "        [2.9505]], grad_fn=<AddBackward0>)\n",
      "loss=0.0034449330996721983\n",
      "before backward() : W=tensor([0.9318], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1550], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9318], requires_grad=True), W.grad=tensor([-0.0164]), b=tensor([0.1550], requires_grad=True), b.grad=tensor([0.0373])\n",
      "after  step() : W=tensor([0.9320], requires_grad=True), W.grad=tensor([-0.0164]), b=tensor([0.1546], requires_grad=True), b.grad=tensor([0.0373])\n",
      "=======================361==================\n",
      "hypothesis=\n",
      "tensor([[1.0866],\n",
      "        [2.0186],\n",
      "        [2.9506]], grad_fn=<AddBackward0>)\n",
      "loss=0.003428387688472867\n",
      "before backward() : W=tensor([0.9320], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1546], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9320], requires_grad=True), W.grad=tensor([-0.0163]), b=tensor([0.1546], requires_grad=True), b.grad=tensor([0.0372])\n",
      "after  step() : W=tensor([0.9322], requires_grad=True), W.grad=tensor([-0.0163]), b=tensor([0.1542], requires_grad=True), b.grad=tensor([0.0372])\n",
      "=======================362==================\n",
      "hypothesis=\n",
      "tensor([[1.0864],\n",
      "        [2.0185],\n",
      "        [2.9507]], grad_fn=<AddBackward0>)\n",
      "loss=0.003411928890272975\n",
      "before backward() : W=tensor([0.9322], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1542], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9322], requires_grad=True), W.grad=tensor([-0.0163]), b=tensor([0.1542], requires_grad=True), b.grad=tensor([0.0371])\n",
      "after  step() : W=tensor([0.9323], requires_grad=True), W.grad=tensor([-0.0163]), b=tensor([0.1538], requires_grad=True), b.grad=tensor([0.0371])\n",
      "=======================363==================\n",
      "hypothesis=\n",
      "tensor([[1.0862],\n",
      "        [2.0185],\n",
      "        [2.9508]], grad_fn=<AddBackward0>)\n",
      "loss=0.003395544132217765\n",
      "before backward() : W=tensor([0.9323], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1538], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9323], requires_grad=True), W.grad=tensor([-0.0163]), b=tensor([0.1538], requires_grad=True), b.grad=tensor([0.0370])\n",
      "after  step() : W=tensor([0.9325], requires_grad=True), W.grad=tensor([-0.0163]), b=tensor([0.1535], requires_grad=True), b.grad=tensor([0.0370])\n",
      "=======================364==================\n",
      "hypothesis=\n",
      "tensor([[1.0860],\n",
      "        [2.0184],\n",
      "        [2.9509]], grad_fn=<AddBackward0>)\n",
      "loss=0.0033792341127991676\n",
      "before backward() : W=tensor([0.9325], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1535], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9325], requires_grad=True), W.grad=tensor([-0.0162]), b=tensor([0.1535], requires_grad=True), b.grad=tensor([0.0369])\n",
      "after  step() : W=tensor([0.9326], requires_grad=True), W.grad=tensor([-0.0162]), b=tensor([0.1531], requires_grad=True), b.grad=tensor([0.0369])\n",
      "=======================365==================\n",
      "hypothesis=\n",
      "tensor([[1.0858],\n",
      "        [2.0184],\n",
      "        [2.9510]], grad_fn=<AddBackward0>)\n",
      "loss=0.0033630093093961477\n",
      "before backward() : W=tensor([0.9326], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1531], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9326], requires_grad=True), W.grad=tensor([-0.0162]), b=tensor([0.1531], requires_grad=True), b.grad=tensor([0.0368])\n",
      "after  step() : W=tensor([0.9328], requires_grad=True), W.grad=tensor([-0.0162]), b=tensor([0.1527], requires_grad=True), b.grad=tensor([0.0368])\n",
      "=======================366==================\n",
      "hypothesis=\n",
      "tensor([[1.0856],\n",
      "        [2.0184],\n",
      "        [2.9512]], grad_fn=<AddBackward0>)\n",
      "loss=0.003346865065395832\n",
      "before backward() : W=tensor([0.9328], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1527], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9328], requires_grad=True), W.grad=tensor([-0.0162]), b=tensor([0.1527], requires_grad=True), b.grad=tensor([0.0367])\n",
      "after  step() : W=tensor([0.9330], requires_grad=True), W.grad=tensor([-0.0162]), b=tensor([0.1524], requires_grad=True), b.grad=tensor([0.0367])\n",
      "=======================367==================\n",
      "hypothesis=\n",
      "tensor([[1.0853],\n",
      "        [2.0183],\n",
      "        [2.9513]], grad_fn=<AddBackward0>)\n",
      "loss=0.003330797888338566\n",
      "before backward() : W=tensor([0.9330], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1524], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9330], requires_grad=True), W.grad=tensor([-0.0161]), b=tensor([0.1524], requires_grad=True), b.grad=tensor([0.0366])\n",
      "after  step() : W=tensor([0.9331], requires_grad=True), W.grad=tensor([-0.0161]), b=tensor([0.1520], requires_grad=True), b.grad=tensor([0.0366])\n",
      "=======================368==================\n",
      "hypothesis=\n",
      "tensor([[1.0851],\n",
      "        [2.0183],\n",
      "        [2.9514]], grad_fn=<AddBackward0>)\n",
      "loss=0.0033147986978292465\n",
      "before backward() : W=tensor([0.9331], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1520], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9331], requires_grad=True), W.grad=tensor([-0.0161]), b=tensor([0.1520], requires_grad=True), b.grad=tensor([0.0365])\n",
      "after  step() : W=tensor([0.9333], requires_grad=True), W.grad=tensor([-0.0161]), b=tensor([0.1516], requires_grad=True), b.grad=tensor([0.0365])\n",
      "=======================369==================\n",
      "hypothesis=\n",
      "tensor([[1.0849],\n",
      "        [2.0182],\n",
      "        [2.9515]], grad_fn=<AddBackward0>)\n",
      "loss=0.0032988807652145624\n",
      "before backward() : W=tensor([0.9333], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1516], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9333], requires_grad=True), W.grad=tensor([-0.0160]), b=tensor([0.1516], requires_grad=True), b.grad=tensor([0.0365])\n",
      "after  step() : W=tensor([0.9335], requires_grad=True), W.grad=tensor([-0.0160]), b=tensor([0.1513], requires_grad=True), b.grad=tensor([0.0365])\n",
      "=======================370==================\n",
      "hypothesis=\n",
      "tensor([[1.0847],\n",
      "        [2.0182],\n",
      "        [2.9516]], grad_fn=<AddBackward0>)\n",
      "loss=0.0032830359414219856\n",
      "before backward() : W=tensor([0.9335], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1513], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9335], requires_grad=True), W.grad=tensor([-0.0160]), b=tensor([0.1513], requires_grad=True), b.grad=tensor([0.0364])\n",
      "after  step() : W=tensor([0.9336], requires_grad=True), W.grad=tensor([-0.0160]), b=tensor([0.1509], requires_grad=True), b.grad=tensor([0.0364])\n",
      "=======================371==================\n",
      "hypothesis=\n",
      "tensor([[1.0845],\n",
      "        [2.0181],\n",
      "        [2.9518]], grad_fn=<AddBackward0>)\n",
      "loss=0.0032672632951289415\n",
      "before backward() : W=tensor([0.9336], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1509], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9336], requires_grad=True), W.grad=tensor([-0.0160]), b=tensor([0.1509], requires_grad=True), b.grad=tensor([0.0363])\n",
      "after  step() : W=tensor([0.9338], requires_grad=True), W.grad=tensor([-0.0160]), b=tensor([0.1506], requires_grad=True), b.grad=tensor([0.0363])\n",
      "=======================372==================\n",
      "hypothesis=\n",
      "tensor([[1.0843],\n",
      "        [2.0181],\n",
      "        [2.9519]], grad_fn=<AddBackward0>)\n",
      "loss=0.0032515840139240026\n",
      "before backward() : W=tensor([0.9338], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1506], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9338], requires_grad=True), W.grad=tensor([-0.0159]), b=tensor([0.1506], requires_grad=True), b.grad=tensor([0.0362])\n",
      "after  step() : W=tensor([0.9339], requires_grad=True), W.grad=tensor([-0.0159]), b=tensor([0.1502], requires_grad=True), b.grad=tensor([0.0362])\n",
      "=======================373==================\n",
      "hypothesis=\n",
      "tensor([[1.0841],\n",
      "        [2.0181],\n",
      "        [2.9520]], grad_fn=<AddBackward0>)\n",
      "loss=0.003235965734347701\n",
      "before backward() : W=tensor([0.9339], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1502], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9339], requires_grad=True), W.grad=tensor([-0.0159]), b=tensor([0.1502], requires_grad=True), b.grad=tensor([0.0361])\n",
      "after  step() : W=tensor([0.9341], requires_grad=True), W.grad=tensor([-0.0159]), b=tensor([0.1498], requires_grad=True), b.grad=tensor([0.0361])\n",
      "=======================374==================\n",
      "hypothesis=\n",
      "tensor([[1.0839],\n",
      "        [2.0180],\n",
      "        [2.9521]], grad_fn=<AddBackward0>)\n",
      "loss=0.003220432670786977\n",
      "before backward() : W=tensor([0.9341], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1498], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9341], requires_grad=True), W.grad=tensor([-0.0158]), b=tensor([0.1498], requires_grad=True), b.grad=tensor([0.0360])\n",
      "after  step() : W=tensor([0.9342], requires_grad=True), W.grad=tensor([-0.0158]), b=tensor([0.1495], requires_grad=True), b.grad=tensor([0.0360])\n",
      "=======================375==================\n",
      "hypothesis=\n",
      "tensor([[1.0837],\n",
      "        [2.0180],\n",
      "        [2.9522]], grad_fn=<AddBackward0>)\n",
      "loss=0.0032049615401774645\n",
      "before backward() : W=tensor([0.9342], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1495], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9342], requires_grad=True), W.grad=tensor([-0.0158]), b=tensor([0.1495], requires_grad=True), b.grad=tensor([0.0359])\n",
      "after  step() : W=tensor([0.9344], requires_grad=True), W.grad=tensor([-0.0158]), b=tensor([0.1491], requires_grad=True), b.grad=tensor([0.0359])\n",
      "=======================376==================\n",
      "hypothesis=\n",
      "tensor([[1.0835],\n",
      "        [2.0179],\n",
      "        [2.9523]], grad_fn=<AddBackward0>)\n",
      "loss=0.003189571900293231\n",
      "before backward() : W=tensor([0.9344], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1491], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9344], requires_grad=True), W.grad=tensor([-0.0158]), b=tensor([0.1491], requires_grad=True), b.grad=tensor([0.0358])\n",
      "after  step() : W=tensor([0.9346], requires_grad=True), W.grad=tensor([-0.0158]), b=tensor([0.1488], requires_grad=True), b.grad=tensor([0.0358])\n",
      "=======================377==================\n",
      "hypothesis=\n",
      "tensor([[1.0833],\n",
      "        [2.0179],\n",
      "        [2.9524]], grad_fn=<AddBackward0>)\n",
      "loss=0.0031742602586746216\n",
      "before backward() : W=tensor([0.9346], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1488], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9346], requires_grad=True), W.grad=tensor([-0.0157]), b=tensor([0.1488], requires_grad=True), b.grad=tensor([0.0358])\n",
      "after  step() : W=tensor([0.9347], requires_grad=True), W.grad=tensor([-0.0157]), b=tensor([0.1484], requires_grad=True), b.grad=tensor([0.0358])\n",
      "=======================378==================\n",
      "hypothesis=\n",
      "tensor([[1.0831],\n",
      "        [2.0178],\n",
      "        [2.9526]], grad_fn=<AddBackward0>)\n",
      "loss=0.0031590082217007875\n",
      "before backward() : W=tensor([0.9347], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1484], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9347], requires_grad=True), W.grad=tensor([-0.0157]), b=tensor([0.1484], requires_grad=True), b.grad=tensor([0.0357])\n",
      "after  step() : W=tensor([0.9349], requires_grad=True), W.grad=tensor([-0.0157]), b=tensor([0.1480], requires_grad=True), b.grad=tensor([0.0357])\n",
      "=======================379==================\n",
      "hypothesis=\n",
      "tensor([[1.0829],\n",
      "        [2.0178],\n",
      "        [2.9527]], grad_fn=<AddBackward0>)\n",
      "loss=0.003143841400742531\n",
      "before backward() : W=tensor([0.9349], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1480], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9349], requires_grad=True), W.grad=tensor([-0.0157]), b=tensor([0.1480], requires_grad=True), b.grad=tensor([0.0356])\n",
      "after  step() : W=tensor([0.9350], requires_grad=True), W.grad=tensor([-0.0157]), b=tensor([0.1477], requires_grad=True), b.grad=tensor([0.0356])\n",
      "=======================380==================\n",
      "hypothesis=\n",
      "tensor([[1.0827],\n",
      "        [2.0178],\n",
      "        [2.9528]], grad_fn=<AddBackward0>)\n",
      "loss=0.0031287444289773703\n",
      "before backward() : W=tensor([0.9350], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1477], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9350], requires_grad=True), W.grad=tensor([-0.0156]), b=tensor([0.1477], requires_grad=True), b.grad=tensor([0.0355])\n",
      "after  step() : W=tensor([0.9352], requires_grad=True), W.grad=tensor([-0.0156]), b=tensor([0.1473], requires_grad=True), b.grad=tensor([0.0355])\n",
      "=======================381==================\n",
      "hypothesis=\n",
      "tensor([[1.0825],\n",
      "        [2.0177],\n",
      "        [2.9529]], grad_fn=<AddBackward0>)\n",
      "loss=0.0031137203332036734\n",
      "before backward() : W=tensor([0.9352], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1473], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9352], requires_grad=True), W.grad=tensor([-0.0156]), b=tensor([0.1473], requires_grad=True), b.grad=tensor([0.0354])\n",
      "after  step() : W=tensor([0.9353], requires_grad=True), W.grad=tensor([-0.0156]), b=tensor([0.1470], requires_grad=True), b.grad=tensor([0.0354])\n",
      "=======================382==================\n",
      "hypothesis=\n",
      "tensor([[1.0823],\n",
      "        [2.0177],\n",
      "        [2.9530]], grad_fn=<AddBackward0>)\n",
      "loss=0.003098761662840843\n",
      "before backward() : W=tensor([0.9353], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1470], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9353], requires_grad=True), W.grad=tensor([-0.0155]), b=tensor([0.1470], requires_grad=True), b.grad=tensor([0.0353])\n",
      "after  step() : W=tensor([0.9355], requires_grad=True), W.grad=tensor([-0.0155]), b=tensor([0.1466], requires_grad=True), b.grad=tensor([0.0353])\n",
      "=======================383==================\n",
      "hypothesis=\n",
      "tensor([[1.0821],\n",
      "        [2.0176],\n",
      "        [2.9531]], grad_fn=<AddBackward0>)\n",
      "loss=0.0030838840175420046\n",
      "before backward() : W=tensor([0.9355], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1466], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9355], requires_grad=True), W.grad=tensor([-0.0155]), b=tensor([0.1466], requires_grad=True), b.grad=tensor([0.0352])\n",
      "after  step() : W=tensor([0.9357], requires_grad=True), W.grad=tensor([-0.0155]), b=tensor([0.1463], requires_grad=True), b.grad=tensor([0.0352])\n",
      "=======================384==================\n",
      "hypothesis=\n",
      "tensor([[1.0819],\n",
      "        [2.0176],\n",
      "        [2.9532]], grad_fn=<AddBackward0>)\n",
      "loss=0.0030690792482346296\n",
      "before backward() : W=tensor([0.9357], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1463], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9357], requires_grad=True), W.grad=tensor([-0.0155]), b=tensor([0.1463], requires_grad=True), b.grad=tensor([0.0352])\n",
      "after  step() : W=tensor([0.9358], requires_grad=True), W.grad=tensor([-0.0155]), b=tensor([0.1459], requires_grad=True), b.grad=tensor([0.0352])\n",
      "=======================385==================\n",
      "hypothesis=\n",
      "tensor([[1.0817],\n",
      "        [2.0175],\n",
      "        [2.9534]], grad_fn=<AddBackward0>)\n",
      "loss=0.003054338274523616\n",
      "before backward() : W=tensor([0.9358], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1459], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9358], requires_grad=True), W.grad=tensor([-0.0154]), b=tensor([0.1459], requires_grad=True), b.grad=tensor([0.0351])\n",
      "after  step() : W=tensor([0.9360], requires_grad=True), W.grad=tensor([-0.0154]), b=tensor([0.1456], requires_grad=True), b.grad=tensor([0.0351])\n",
      "=======================386==================\n",
      "hypothesis=\n",
      "tensor([[1.0815],\n",
      "        [2.0175],\n",
      "        [2.9535]], grad_fn=<AddBackward0>)\n",
      "loss=0.003039673902094364\n",
      "before backward() : W=tensor([0.9360], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1456], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9360], requires_grad=True), W.grad=tensor([-0.0154]), b=tensor([0.1456], requires_grad=True), b.grad=tensor([0.0350])\n",
      "after  step() : W=tensor([0.9361], requires_grad=True), W.grad=tensor([-0.0154]), b=tensor([0.1452], requires_grad=True), b.grad=tensor([0.0350])\n",
      "=======================387==================\n",
      "hypothesis=\n",
      "tensor([[1.0813],\n",
      "        [2.0175],\n",
      "        [2.9536]], grad_fn=<AddBackward0>)\n",
      "loss=0.0030250733252614737\n",
      "before backward() : W=tensor([0.9361], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1452], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9361], requires_grad=True), W.grad=tensor([-0.0154]), b=tensor([0.1452], requires_grad=True), b.grad=tensor([0.0349])\n",
      "after  step() : W=tensor([0.9363], requires_grad=True), W.grad=tensor([-0.0154]), b=tensor([0.1449], requires_grad=True), b.grad=tensor([0.0349])\n",
      "=======================388==================\n",
      "hypothesis=\n",
      "tensor([[1.0811],\n",
      "        [2.0174],\n",
      "        [2.9537]], grad_fn=<AddBackward0>)\n",
      "loss=0.0030105484183877707\n",
      "before backward() : W=tensor([0.9363], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1449], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9363], requires_grad=True), W.grad=tensor([-0.0153]), b=tensor([0.1449], requires_grad=True), b.grad=tensor([0.0348])\n",
      "after  step() : W=tensor([0.9364], requires_grad=True), W.grad=tensor([-0.0153]), b=tensor([0.1445], requires_grad=True), b.grad=tensor([0.0348])\n",
      "=======================389==================\n",
      "hypothesis=\n",
      "tensor([[1.0809],\n",
      "        [2.0174],\n",
      "        [2.9538]], grad_fn=<AddBackward0>)\n",
      "loss=0.002996092429384589\n",
      "before backward() : W=tensor([0.9364], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1445], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9364], requires_grad=True), W.grad=tensor([-0.0153]), b=tensor([0.1445], requires_grad=True), b.grad=tensor([0.0347])\n",
      "after  step() : W=tensor([0.9366], requires_grad=True), W.grad=tensor([-0.0153]), b=tensor([0.1442], requires_grad=True), b.grad=tensor([0.0347])\n",
      "=======================390==================\n",
      "hypothesis=\n",
      "tensor([[1.0807],\n",
      "        [2.0173],\n",
      "        [2.9539]], grad_fn=<AddBackward0>)\n",
      "loss=0.0029817058239132166\n",
      "before backward() : W=tensor([0.9366], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1442], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9366], requires_grad=True), W.grad=tensor([-0.0152]), b=tensor([0.1442], requires_grad=True), b.grad=tensor([0.0347])\n",
      "after  step() : W=tensor([0.9367], requires_grad=True), W.grad=tensor([-0.0152]), b=tensor([0.1438], requires_grad=True), b.grad=tensor([0.0347])\n",
      "=======================391==================\n",
      "hypothesis=\n",
      "tensor([[1.0806],\n",
      "        [2.0173],\n",
      "        [2.9540]], grad_fn=<AddBackward0>)\n",
      "loss=0.0029673899989575148\n",
      "before backward() : W=tensor([0.9367], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1438], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9367], requires_grad=True), W.grad=tensor([-0.0152]), b=tensor([0.1438], requires_grad=True), b.grad=tensor([0.0346])\n",
      "after  step() : W=tensor([0.9369], requires_grad=True), W.grad=tensor([-0.0152]), b=tensor([0.1435], requires_grad=True), b.grad=tensor([0.0346])\n",
      "=======================392==================\n",
      "hypothesis=\n",
      "tensor([[1.0804],\n",
      "        [2.0172],\n",
      "        [2.9541]], grad_fn=<AddBackward0>)\n",
      "loss=0.0029531409963965416\n",
      "before backward() : W=tensor([0.9369], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1435], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9369], requires_grad=True), W.grad=tensor([-0.0152]), b=tensor([0.1435], requires_grad=True), b.grad=tensor([0.0345])\n",
      "after  step() : W=tensor([0.9370], requires_grad=True), W.grad=tensor([-0.0152]), b=tensor([0.1431], requires_grad=True), b.grad=tensor([0.0345])\n",
      "=======================393==================\n",
      "hypothesis=\n",
      "tensor([[1.0802],\n",
      "        [2.0172],\n",
      "        [2.9542]], grad_fn=<AddBackward0>)\n",
      "loss=0.0029389604460448027\n",
      "before backward() : W=tensor([0.9370], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1431], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9370], requires_grad=True), W.grad=tensor([-0.0151]), b=tensor([0.1431], requires_grad=True), b.grad=tensor([0.0344])\n",
      "after  step() : W=tensor([0.9372], requires_grad=True), W.grad=tensor([-0.0151]), b=tensor([0.1428], requires_grad=True), b.grad=tensor([0.0344])\n",
      "=======================394==================\n",
      "hypothesis=\n",
      "tensor([[1.0800],\n",
      "        [2.0172],\n",
      "        [2.9543]], grad_fn=<AddBackward0>)\n",
      "loss=0.002924839034676552\n",
      "before backward() : W=tensor([0.9372], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1428], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9372], requires_grad=True), W.grad=tensor([-0.0151]), b=tensor([0.1428], requires_grad=True), b.grad=tensor([0.0343])\n",
      "after  step() : W=tensor([0.9373], requires_grad=True), W.grad=tensor([-0.0151]), b=tensor([0.1424], requires_grad=True), b.grad=tensor([0.0343])\n",
      "=======================395==================\n",
      "hypothesis=\n",
      "tensor([[1.0798],\n",
      "        [2.0171],\n",
      "        [2.9545]], grad_fn=<AddBackward0>)\n",
      "loss=0.0029107939917594194\n",
      "before backward() : W=tensor([0.9373], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1424], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9373], requires_grad=True), W.grad=tensor([-0.0151]), b=tensor([0.1424], requires_grad=True), b.grad=tensor([0.0342])\n",
      "after  step() : W=tensor([0.9375], requires_grad=True), W.grad=tensor([-0.0151]), b=tensor([0.1421], requires_grad=True), b.grad=tensor([0.0342])\n",
      "=======================396==================\n",
      "hypothesis=\n",
      "tensor([[1.0796],\n",
      "        [2.0171],\n",
      "        [2.9546]], grad_fn=<AddBackward0>)\n",
      "loss=0.0028968292754143476\n",
      "before backward() : W=tensor([0.9375], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1421], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9375], requires_grad=True), W.grad=tensor([-0.0150]), b=tensor([0.1421], requires_grad=True), b.grad=tensor([0.0342])\n",
      "after  step() : W=tensor([0.9376], requires_grad=True), W.grad=tensor([-0.0150]), b=tensor([0.1418], requires_grad=True), b.grad=tensor([0.0342])\n",
      "=======================397==================\n",
      "hypothesis=\n",
      "tensor([[1.0794],\n",
      "        [2.0170],\n",
      "        [2.9547]], grad_fn=<AddBackward0>)\n",
      "loss=0.002882913453504443\n",
      "before backward() : W=tensor([0.9376], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1418], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9376], requires_grad=True), W.grad=tensor([-0.0150]), b=tensor([0.1418], requires_grad=True), b.grad=tensor([0.0341])\n",
      "after  step() : W=tensor([0.9378], requires_grad=True), W.grad=tensor([-0.0150]), b=tensor([0.1414], requires_grad=True), b.grad=tensor([0.0341])\n",
      "=======================398==================\n",
      "hypothesis=\n",
      "tensor([[1.0792],\n",
      "        [2.0170],\n",
      "        [2.9548]], grad_fn=<AddBackward0>)\n",
      "loss=0.0028690623585134745\n",
      "before backward() : W=tensor([0.9378], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1414], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9378], requires_grad=True), W.grad=tensor([-0.0150]), b=tensor([0.1414], requires_grad=True), b.grad=tensor([0.0340])\n",
      "after  step() : W=tensor([0.9379], requires_grad=True), W.grad=tensor([-0.0150]), b=tensor([0.1411], requires_grad=True), b.grad=tensor([0.0340])\n",
      "=======================399==================\n",
      "hypothesis=\n",
      "tensor([[1.0790],\n",
      "        [2.0170],\n",
      "        [2.9549]], grad_fn=<AddBackward0>)\n",
      "loss=0.0028552941512316465\n",
      "before backward() : W=tensor([0.9379], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1411], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9379], requires_grad=True), W.grad=tensor([-0.0149]), b=tensor([0.1411], requires_grad=True), b.grad=tensor([0.0339])\n",
      "after  step() : W=tensor([0.9381], requires_grad=True), W.grad=tensor([-0.0149]), b=tensor([0.1407], requires_grad=True), b.grad=tensor([0.0339])\n",
      "=======================400==================\n",
      "hypothesis=\n",
      "tensor([[1.0788],\n",
      "        [2.0169],\n",
      "        [2.9550]], grad_fn=<AddBackward0>)\n",
      "loss=0.0028415836859494448\n",
      "before backward() : W=tensor([0.9381], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1407], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9381], requires_grad=True), W.grad=tensor([-0.0149]), b=tensor([0.1407], requires_grad=True), b.grad=tensor([0.0338])\n",
      "after  step() : W=tensor([0.9382], requires_grad=True), W.grad=tensor([-0.0149]), b=tensor([0.1404], requires_grad=True), b.grad=tensor([0.0338])\n",
      "=======================401==================\n",
      "hypothesis=\n",
      "tensor([[1.0786],\n",
      "        [2.0169],\n",
      "        [2.9551]], grad_fn=<AddBackward0>)\n",
      "loss=0.0028279328253120184\n",
      "before backward() : W=tensor([0.9382], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1404], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9382], requires_grad=True), W.grad=tensor([-0.0148]), b=tensor([0.1404], requires_grad=True), b.grad=tensor([0.0338])\n",
      "after  step() : W=tensor([0.9384], requires_grad=True), W.grad=tensor([-0.0148]), b=tensor([0.1401], requires_grad=True), b.grad=tensor([0.0338])\n",
      "=======================402==================\n",
      "hypothesis=\n",
      "tensor([[1.0785],\n",
      "        [2.0168],\n",
      "        [2.9552]], grad_fn=<AddBackward0>)\n",
      "loss=0.0028143490198999643\n",
      "before backward() : W=tensor([0.9384], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1401], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9384], requires_grad=True), W.grad=tensor([-0.0148]), b=tensor([0.1401], requires_grad=True), b.grad=tensor([0.0337])\n",
      "after  step() : W=tensor([0.9385], requires_grad=True), W.grad=tensor([-0.0148]), b=tensor([0.1397], requires_grad=True), b.grad=tensor([0.0337])\n",
      "=======================403==================\n",
      "hypothesis=\n",
      "tensor([[1.0783],\n",
      "        [2.0168],\n",
      "        [2.9553]], grad_fn=<AddBackward0>)\n",
      "loss=0.0028008371591567993\n",
      "before backward() : W=tensor([0.9385], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1397], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9385], requires_grad=True), W.grad=tensor([-0.0148]), b=tensor([0.1397], requires_grad=True), b.grad=tensor([0.0336])\n",
      "after  step() : W=tensor([0.9387], requires_grad=True), W.grad=tensor([-0.0148]), b=tensor([0.1394], requires_grad=True), b.grad=tensor([0.0336])\n",
      "=======================404==================\n",
      "hypothesis=\n",
      "tensor([[1.0781],\n",
      "        [2.0168],\n",
      "        [2.9554]], grad_fn=<AddBackward0>)\n",
      "loss=0.0027873925864696503\n",
      "before backward() : W=tensor([0.9387], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1394], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9387], requires_grad=True), W.grad=tensor([-0.0147]), b=tensor([0.1394], requires_grad=True), b.grad=tensor([0.0335])\n",
      "after  step() : W=tensor([0.9388], requires_grad=True), W.grad=tensor([-0.0147]), b=tensor([0.1391], requires_grad=True), b.grad=tensor([0.0335])\n",
      "=======================405==================\n",
      "hypothesis=\n",
      "tensor([[1.0779],\n",
      "        [2.0167],\n",
      "        [2.9555]], grad_fn=<AddBackward0>)\n",
      "loss=0.002774004591628909\n",
      "before backward() : W=tensor([0.9388], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1391], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9388], requires_grad=True), W.grad=tensor([-0.0147]), b=tensor([0.1391], requires_grad=True), b.grad=tensor([0.0334])\n",
      "after  step() : W=tensor([0.9390], requires_grad=True), W.grad=tensor([-0.0147]), b=tensor([0.1387], requires_grad=True), b.grad=tensor([0.0334])\n",
      "=======================406==================\n",
      "hypothesis=\n",
      "tensor([[1.0777],\n",
      "        [2.0167],\n",
      "        [2.9556]], grad_fn=<AddBackward0>)\n",
      "loss=0.0027606775984168053\n",
      "before backward() : W=tensor([0.9390], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1387], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9390], requires_grad=True), W.grad=tensor([-0.0147]), b=tensor([0.1387], requires_grad=True), b.grad=tensor([0.0333])\n",
      "after  step() : W=tensor([0.9391], requires_grad=True), W.grad=tensor([-0.0147]), b=tensor([0.1384], requires_grad=True), b.grad=tensor([0.0333])\n",
      "=======================407==================\n",
      "hypothesis=\n",
      "tensor([[1.0775],\n",
      "        [2.0166],\n",
      "        [2.9558]], grad_fn=<AddBackward0>)\n",
      "loss=0.002747425576671958\n",
      "before backward() : W=tensor([0.9391], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1384], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9391], requires_grad=True), W.grad=tensor([-0.0146]), b=tensor([0.1384], requires_grad=True), b.grad=tensor([0.0333])\n",
      "after  step() : W=tensor([0.9393], requires_grad=True), W.grad=tensor([-0.0146]), b=tensor([0.1381], requires_grad=True), b.grad=tensor([0.0333])\n",
      "=======================408==================\n",
      "hypothesis=\n",
      "tensor([[1.0773],\n",
      "        [2.0166],\n",
      "        [2.9559]], grad_fn=<AddBackward0>)\n",
      "loss=0.002734242007136345\n",
      "before backward() : W=tensor([0.9393], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1381], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9393], requires_grad=True), W.grad=tensor([-0.0146]), b=tensor([0.1381], requires_grad=True), b.grad=tensor([0.0332])\n",
      "after  step() : W=tensor([0.9394], requires_grad=True), W.grad=tensor([-0.0146]), b=tensor([0.1377], requires_grad=True), b.grad=tensor([0.0332])\n",
      "=======================409==================\n",
      "hypothesis=\n",
      "tensor([[1.0771],\n",
      "        [2.0166],\n",
      "        [2.9560]], grad_fn=<AddBackward0>)\n",
      "loss=0.00272110546939075\n",
      "before backward() : W=tensor([0.9394], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1377], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9394], requires_grad=True), W.grad=tensor([-0.0146]), b=tensor([0.1377], requires_grad=True), b.grad=tensor([0.0331])\n",
      "after  step() : W=tensor([0.9396], requires_grad=True), W.grad=tensor([-0.0146]), b=tensor([0.1374], requires_grad=True), b.grad=tensor([0.0331])\n",
      "=======================410==================\n",
      "hypothesis=\n",
      "tensor([[1.0770],\n",
      "        [2.0165],\n",
      "        [2.9561]], grad_fn=<AddBackward0>)\n",
      "loss=0.0027080371510237455\n",
      "before backward() : W=tensor([0.9396], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1374], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9396], requires_grad=True), W.grad=tensor([-0.0145]), b=tensor([0.1374], requires_grad=True), b.grad=tensor([0.0330])\n",
      "after  step() : W=tensor([0.9397], requires_grad=True), W.grad=tensor([-0.0145]), b=tensor([0.1371], requires_grad=True), b.grad=tensor([0.0330])\n",
      "=======================411==================\n",
      "hypothesis=\n",
      "tensor([[1.0768],\n",
      "        [2.0165],\n",
      "        [2.9562]], grad_fn=<AddBackward0>)\n",
      "loss=0.0026950256433337927\n",
      "before backward() : W=tensor([0.9397], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1371], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9397], requires_grad=True), W.grad=tensor([-0.0145]), b=tensor([0.1371], requires_grad=True), b.grad=tensor([0.0329])\n",
      "after  step() : W=tensor([0.9399], requires_grad=True), W.grad=tensor([-0.0145]), b=tensor([0.1367], requires_grad=True), b.grad=tensor([0.0329])\n",
      "=======================412==================\n",
      "hypothesis=\n",
      "tensor([[1.0766],\n",
      "        [2.0164],\n",
      "        [2.9563]], grad_fn=<AddBackward0>)\n",
      "loss=0.0026820835191756487\n",
      "before backward() : W=tensor([0.9399], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1367], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9399], requires_grad=True), W.grad=tensor([-0.0145]), b=tensor([0.1367], requires_grad=True), b.grad=tensor([0.0329])\n",
      "after  step() : W=tensor([0.9400], requires_grad=True), W.grad=tensor([-0.0145]), b=tensor([0.1364], requires_grad=True), b.grad=tensor([0.0329])\n",
      "=======================413==================\n",
      "hypothesis=\n",
      "tensor([[1.0764],\n",
      "        [2.0164],\n",
      "        [2.9564]], grad_fn=<AddBackward0>)\n",
      "loss=0.00266921054571867\n",
      "before backward() : W=tensor([0.9400], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1364], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9400], requires_grad=True), W.grad=tensor([-0.0144]), b=tensor([0.1364], requires_grad=True), b.grad=tensor([0.0328])\n",
      "after  step() : W=tensor([0.9401], requires_grad=True), W.grad=tensor([-0.0144]), b=tensor([0.1361], requires_grad=True), b.grad=tensor([0.0328])\n",
      "=======================414==================\n",
      "hypothesis=\n",
      "tensor([[1.0762],\n",
      "        [2.0164],\n",
      "        [2.9565]], grad_fn=<AddBackward0>)\n",
      "loss=0.0026563939172774553\n",
      "before backward() : W=tensor([0.9401], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1361], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9401], requires_grad=True), W.grad=tensor([-0.0144]), b=tensor([0.1361], requires_grad=True), b.grad=tensor([0.0327])\n",
      "after  step() : W=tensor([0.9403], requires_grad=True), W.grad=tensor([-0.0144]), b=tensor([0.1358], requires_grad=True), b.grad=tensor([0.0327])\n",
      "=======================415==================\n",
      "hypothesis=\n",
      "tensor([[1.0760],\n",
      "        [2.0163],\n",
      "        [2.9566]], grad_fn=<AddBackward0>)\n",
      "loss=0.002643641782924533\n",
      "before backward() : W=tensor([0.9403], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1358], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9403], requires_grad=True), W.grad=tensor([-0.0144]), b=tensor([0.1358], requires_grad=True), b.grad=tensor([0.0326])\n",
      "after  step() : W=tensor([0.9404], requires_grad=True), W.grad=tensor([-0.0144]), b=tensor([0.1354], requires_grad=True), b.grad=tensor([0.0326])\n",
      "=======================416==================\n",
      "hypothesis=\n",
      "tensor([[1.0759],\n",
      "        [2.0163],\n",
      "        [2.9567]], grad_fn=<AddBackward0>)\n",
      "loss=0.0026309399399906397\n",
      "before backward() : W=tensor([0.9404], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1354], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9404], requires_grad=True), W.grad=tensor([-0.0143]), b=tensor([0.1354], requires_grad=True), b.grad=tensor([0.0326])\n",
      "after  step() : W=tensor([0.9406], requires_grad=True), W.grad=tensor([-0.0143]), b=tensor([0.1351], requires_grad=True), b.grad=tensor([0.0326])\n",
      "=======================417==================\n",
      "hypothesis=\n",
      "tensor([[1.0757],\n",
      "        [2.0162],\n",
      "        [2.9568]], grad_fn=<AddBackward0>)\n",
      "loss=0.0026183065492659807\n",
      "before backward() : W=tensor([0.9406], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1351], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9406], requires_grad=True), W.grad=tensor([-0.0143]), b=tensor([0.1351], requires_grad=True), b.grad=tensor([0.0325])\n",
      "after  step() : W=tensor([0.9407], requires_grad=True), W.grad=tensor([-0.0143]), b=tensor([0.1348], requires_grad=True), b.grad=tensor([0.0325])\n",
      "=======================418==================\n",
      "hypothesis=\n",
      "tensor([[1.0755],\n",
      "        [2.0162],\n",
      "        [2.9569]], grad_fn=<AddBackward0>)\n",
      "loss=0.002605738118290901\n",
      "before backward() : W=tensor([0.9407], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1348], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9407], requires_grad=True), W.grad=tensor([-0.0143]), b=tensor([0.1348], requires_grad=True), b.grad=tensor([0.0324])\n",
      "after  step() : W=tensor([0.9409], requires_grad=True), W.grad=tensor([-0.0143]), b=tensor([0.1344], requires_grad=True), b.grad=tensor([0.0324])\n",
      "=======================419==================\n",
      "hypothesis=\n",
      "tensor([[1.0753],\n",
      "        [2.0162],\n",
      "        [2.9570]], grad_fn=<AddBackward0>)\n",
      "loss=0.00259322184138\n",
      "before backward() : W=tensor([0.9409], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1344], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9409], requires_grad=True), W.grad=tensor([-0.0142]), b=tensor([0.1344], requires_grad=True), b.grad=tensor([0.0323])\n",
      "after  step() : W=tensor([0.9410], requires_grad=True), W.grad=tensor([-0.0142]), b=tensor([0.1341], requires_grad=True), b.grad=tensor([0.0323])\n",
      "=======================420==================\n",
      "hypothesis=\n",
      "tensor([[1.0751],\n",
      "        [2.0161],\n",
      "        [2.9571]], grad_fn=<AddBackward0>)\n",
      "loss=0.0025807691272348166\n",
      "before backward() : W=tensor([0.9410], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1341], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9410], requires_grad=True), W.grad=tensor([-0.0142]), b=tensor([0.1341], requires_grad=True), b.grad=tensor([0.0322])\n",
      "after  step() : W=tensor([0.9411], requires_grad=True), W.grad=tensor([-0.0142]), b=tensor([0.1338], requires_grad=True), b.grad=tensor([0.0322])\n",
      "=======================421==================\n",
      "hypothesis=\n",
      "tensor([[1.0749],\n",
      "        [2.0161],\n",
      "        [2.9572]], grad_fn=<AddBackward0>)\n",
      "loss=0.002568378346040845\n",
      "before backward() : W=tensor([0.9411], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1338], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9411], requires_grad=True), W.grad=tensor([-0.0141]), b=tensor([0.1338], requires_grad=True), b.grad=tensor([0.0322])\n",
      "after  step() : W=tensor([0.9413], requires_grad=True), W.grad=tensor([-0.0141]), b=tensor([0.1335], requires_grad=True), b.grad=tensor([0.0322])\n",
      "=======================422==================\n",
      "hypothesis=\n",
      "tensor([[1.0748],\n",
      "        [2.0160],\n",
      "        [2.9573]], grad_fn=<AddBackward0>)\n",
      "loss=0.002556034130975604\n",
      "before backward() : W=tensor([0.9413], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1335], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9413], requires_grad=True), W.grad=tensor([-0.0141]), b=tensor([0.1335], requires_grad=True), b.grad=tensor([0.0321])\n",
      "after  step() : W=tensor([0.9414], requires_grad=True), W.grad=tensor([-0.0141]), b=tensor([0.1332], requires_grad=True), b.grad=tensor([0.0321])\n",
      "=======================423==================\n",
      "hypothesis=\n",
      "tensor([[1.0746],\n",
      "        [2.0160],\n",
      "        [2.9574]], grad_fn=<AddBackward0>)\n",
      "loss=0.0025437672156840563\n",
      "before backward() : W=tensor([0.9414], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1332], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9414], requires_grad=True), W.grad=tensor([-0.0141]), b=tensor([0.1332], requires_grad=True), b.grad=tensor([0.0320])\n",
      "after  step() : W=tensor([0.9416], requires_grad=True), W.grad=tensor([-0.0141]), b=tensor([0.1328], requires_grad=True), b.grad=tensor([0.0320])\n",
      "=======================424==================\n",
      "hypothesis=\n",
      "tensor([[1.0744],\n",
      "        [2.0160],\n",
      "        [2.9575]], grad_fn=<AddBackward0>)\n",
      "loss=0.002531553152948618\n",
      "before backward() : W=tensor([0.9416], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1328], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9416], requires_grad=True), W.grad=tensor([-0.0140]), b=tensor([0.1328], requires_grad=True), b.grad=tensor([0.0319])\n",
      "after  step() : W=tensor([0.9417], requires_grad=True), W.grad=tensor([-0.0140]), b=tensor([0.1325], requires_grad=True), b.grad=tensor([0.0319])\n",
      "=======================425==================\n",
      "hypothesis=\n",
      "tensor([[1.0742],\n",
      "        [2.0159],\n",
      "        [2.9576]], grad_fn=<AddBackward0>)\n",
      "loss=0.0025193935725837946\n",
      "before backward() : W=tensor([0.9417], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1325], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9417], requires_grad=True), W.grad=tensor([-0.0140]), b=tensor([0.1325], requires_grad=True), b.grad=tensor([0.0319])\n",
      "after  step() : W=tensor([0.9418], requires_grad=True), W.grad=tensor([-0.0140]), b=tensor([0.1322], requires_grad=True), b.grad=tensor([0.0319])\n",
      "=======================426==================\n",
      "hypothesis=\n",
      "tensor([[1.0740],\n",
      "        [2.0159],\n",
      "        [2.9577]], grad_fn=<AddBackward0>)\n",
      "loss=0.0025072942953556776\n",
      "before backward() : W=tensor([0.9418], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1322], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9418], requires_grad=True), W.grad=tensor([-0.0140]), b=tensor([0.1322], requires_grad=True), b.grad=tensor([0.0318])\n",
      "after  step() : W=tensor([0.9420], requires_grad=True), W.grad=tensor([-0.0140]), b=tensor([0.1319], requires_grad=True), b.grad=tensor([0.0318])\n",
      "=======================427==================\n",
      "hypothesis=\n",
      "tensor([[1.0739],\n",
      "        [2.0159],\n",
      "        [2.9578]], grad_fn=<AddBackward0>)\n",
      "loss=0.0024952504318207502\n",
      "before backward() : W=tensor([0.9420], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1319], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9420], requires_grad=True), W.grad=tensor([-0.0139]), b=tensor([0.1319], requires_grad=True), b.grad=tensor([0.0317])\n",
      "after  step() : W=tensor([0.9421], requires_grad=True), W.grad=tensor([-0.0139]), b=tensor([0.1316], requires_grad=True), b.grad=tensor([0.0317])\n",
      "=======================428==================\n",
      "hypothesis=\n",
      "tensor([[1.0737],\n",
      "        [2.0158],\n",
      "        [2.9579]], grad_fn=<AddBackward0>)\n",
      "loss=0.002483268501237035\n",
      "before backward() : W=tensor([0.9421], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1316], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9421], requires_grad=True), W.grad=tensor([-0.0139]), b=tensor([0.1316], requires_grad=True), b.grad=tensor([0.0316])\n",
      "after  step() : W=tensor([0.9423], requires_grad=True), W.grad=tensor([-0.0139]), b=tensor([0.1313], requires_grad=True), b.grad=tensor([0.0316])\n",
      "=======================429==================\n",
      "hypothesis=\n",
      "tensor([[1.0735],\n",
      "        [2.0158],\n",
      "        [2.9580]], grad_fn=<AddBackward0>)\n",
      "loss=0.002471349900588393\n",
      "before backward() : W=tensor([0.9423], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1313], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9423], requires_grad=True), W.grad=tensor([-0.0139]), b=tensor([0.1313], requires_grad=True), b.grad=tensor([0.0316])\n",
      "after  step() : W=tensor([0.9424], requires_grad=True), W.grad=tensor([-0.0139]), b=tensor([0.1309], requires_grad=True), b.grad=tensor([0.0316])\n",
      "=======================430==================\n",
      "hypothesis=\n",
      "tensor([[1.0733],\n",
      "        [2.0157],\n",
      "        [2.9581]], grad_fn=<AddBackward0>)\n",
      "loss=0.0024594792630523443\n",
      "before backward() : W=tensor([0.9424], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1309], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9424], requires_grad=True), W.grad=tensor([-0.0138]), b=tensor([0.1309], requires_grad=True), b.grad=tensor([0.0315])\n",
      "after  step() : W=tensor([0.9425], requires_grad=True), W.grad=tensor([-0.0138]), b=tensor([0.1306], requires_grad=True), b.grad=tensor([0.0315])\n",
      "=======================431==================\n",
      "hypothesis=\n",
      "tensor([[1.0732],\n",
      "        [2.0157],\n",
      "        [2.9582]], grad_fn=<AddBackward0>)\n",
      "loss=0.0024476677644997835\n",
      "before backward() : W=tensor([0.9425], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1306], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9425], requires_grad=True), W.grad=tensor([-0.0138]), b=tensor([0.1306], requires_grad=True), b.grad=tensor([0.0314])\n",
      "after  step() : W=tensor([0.9427], requires_grad=True), W.grad=tensor([-0.0138]), b=tensor([0.1303], requires_grad=True), b.grad=tensor([0.0314])\n",
      "=======================432==================\n",
      "hypothesis=\n",
      "tensor([[1.0730],\n",
      "        [2.0157],\n",
      "        [2.9583]], grad_fn=<AddBackward0>)\n",
      "loss=0.0024359170347452164\n",
      "before backward() : W=tensor([0.9427], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1303], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9427], requires_grad=True), W.grad=tensor([-0.0138]), b=tensor([0.1303], requires_grad=True), b.grad=tensor([0.0313])\n",
      "after  step() : W=tensor([0.9428], requires_grad=True), W.grad=tensor([-0.0138]), b=tensor([0.1300], requires_grad=True), b.grad=tensor([0.0313])\n",
      "=======================433==================\n",
      "hypothesis=\n",
      "tensor([[1.0728],\n",
      "        [2.0156],\n",
      "        [2.9584]], grad_fn=<AddBackward0>)\n",
      "loss=0.0024242184590548277\n",
      "before backward() : W=tensor([0.9428], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1300], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9428], requires_grad=True), W.grad=tensor([-0.0137]), b=tensor([0.1300], requires_grad=True), b.grad=tensor([0.0313])\n",
      "after  step() : W=tensor([0.9430], requires_grad=True), W.grad=tensor([-0.0137]), b=tensor([0.1297], requires_grad=True), b.grad=tensor([0.0313])\n",
      "=======================434==================\n",
      "hypothesis=\n",
      "tensor([[1.0726],\n",
      "        [2.0156],\n",
      "        [2.9585]], grad_fn=<AddBackward0>)\n",
      "loss=0.0024125787895172834\n",
      "before backward() : W=tensor([0.9430], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1297], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9430], requires_grad=True), W.grad=tensor([-0.0137]), b=tensor([0.1297], requires_grad=True), b.grad=tensor([0.0312])\n",
      "after  step() : W=tensor([0.9431], requires_grad=True), W.grad=tensor([-0.0137]), b=tensor([0.1294], requires_grad=True), b.grad=tensor([0.0312])\n",
      "=======================435==================\n",
      "hypothesis=\n",
      "tensor([[1.0725],\n",
      "        [2.0155],\n",
      "        [2.9586]], grad_fn=<AddBackward0>)\n",
      "loss=0.0024009912740439177\n",
      "before backward() : W=tensor([0.9431], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1294], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9431], requires_grad=True), W.grad=tensor([-0.0137]), b=tensor([0.1294], requires_grad=True), b.grad=tensor([0.0311])\n",
      "after  step() : W=tensor([0.9432], requires_grad=True), W.grad=tensor([-0.0137]), b=tensor([0.1291], requires_grad=True), b.grad=tensor([0.0311])\n",
      "=======================436==================\n",
      "hypothesis=\n",
      "tensor([[1.0723],\n",
      "        [2.0155],\n",
      "        [2.9587]], grad_fn=<AddBackward0>)\n",
      "loss=0.002389456843957305\n",
      "before backward() : W=tensor([0.9432], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1291], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9432], requires_grad=True), W.grad=tensor([-0.0136]), b=tensor([0.1291], requires_grad=True), b.grad=tensor([0.0310])\n",
      "after  step() : W=tensor([0.9434], requires_grad=True), W.grad=tensor([-0.0136]), b=tensor([0.1287], requires_grad=True), b.grad=tensor([0.0310])\n",
      "=======================437==================\n",
      "hypothesis=\n",
      "tensor([[1.0721],\n",
      "        [2.0155],\n",
      "        [2.9588]], grad_fn=<AddBackward0>)\n",
      "loss=0.0023779876064509153\n",
      "before backward() : W=tensor([0.9434], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1287], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9434], requires_grad=True), W.grad=tensor([-0.0136]), b=tensor([0.1287], requires_grad=True), b.grad=tensor([0.0310])\n",
      "after  step() : W=tensor([0.9435], requires_grad=True), W.grad=tensor([-0.0136]), b=tensor([0.1284], requires_grad=True), b.grad=tensor([0.0310])\n",
      "=======================438==================\n",
      "hypothesis=\n",
      "tensor([[1.0719],\n",
      "        [2.0154],\n",
      "        [2.9589]], grad_fn=<AddBackward0>)\n",
      "loss=0.002366564003750682\n",
      "before backward() : W=tensor([0.9435], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1284], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9435], requires_grad=True), W.grad=tensor([-0.0136]), b=tensor([0.1284], requires_grad=True), b.grad=tensor([0.0309])\n",
      "after  step() : W=tensor([0.9436], requires_grad=True), W.grad=tensor([-0.0136]), b=tensor([0.1281], requires_grad=True), b.grad=tensor([0.0309])\n",
      "=======================439==================\n",
      "hypothesis=\n",
      "tensor([[1.0718],\n",
      "        [2.0154],\n",
      "        [2.9590]], grad_fn=<AddBackward0>)\n",
      "loss=0.0023552027996629477\n",
      "before backward() : W=tensor([0.9436], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1281], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9436], requires_grad=True), W.grad=tensor([-0.0135]), b=tensor([0.1281], requires_grad=True), b.grad=tensor([0.0308])\n",
      "after  step() : W=tensor([0.9438], requires_grad=True), W.grad=tensor([-0.0135]), b=tensor([0.1278], requires_grad=True), b.grad=tensor([0.0308])\n",
      "=======================440==================\n",
      "hypothesis=\n",
      "tensor([[1.0716],\n",
      "        [2.0154],\n",
      "        [2.9591]], grad_fn=<AddBackward0>)\n",
      "loss=0.002343889093026519\n",
      "before backward() : W=tensor([0.9438], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1278], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9438], requires_grad=True), W.grad=tensor([-0.0135]), b=tensor([0.1278], requires_grad=True), b.grad=tensor([0.0307])\n",
      "after  step() : W=tensor([0.9439], requires_grad=True), W.grad=tensor([-0.0135]), b=tensor([0.1275], requires_grad=True), b.grad=tensor([0.0307])\n",
      "=======================441==================\n",
      "hypothesis=\n",
      "tensor([[1.0714],\n",
      "        [2.0153],\n",
      "        [2.9592]], grad_fn=<AddBackward0>)\n",
      "loss=0.002332638017833233\n",
      "before backward() : W=tensor([0.9439], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1275], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9439], requires_grad=True), W.grad=tensor([-0.0135]), b=tensor([0.1275], requires_grad=True), b.grad=tensor([0.0307])\n",
      "after  step() : W=tensor([0.9440], requires_grad=True), W.grad=tensor([-0.0135]), b=tensor([0.1272], requires_grad=True), b.grad=tensor([0.0307])\n",
      "=======================442==================\n",
      "hypothesis=\n",
      "tensor([[1.0712],\n",
      "        [2.0153],\n",
      "        [2.9593]], grad_fn=<AddBackward0>)\n",
      "loss=0.002321431413292885\n",
      "before backward() : W=tensor([0.9440], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1272], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9440], requires_grad=True), W.grad=tensor([-0.0135]), b=tensor([0.1272], requires_grad=True), b.grad=tensor([0.0306])\n",
      "after  step() : W=tensor([0.9442], requires_grad=True), W.grad=tensor([-0.0135]), b=tensor([0.1269], requires_grad=True), b.grad=tensor([0.0306])\n",
      "=======================443==================\n",
      "hypothesis=\n",
      "tensor([[1.0711],\n",
      "        [2.0153],\n",
      "        [2.9594]], grad_fn=<AddBackward0>)\n",
      "loss=0.002310289768502116\n",
      "before backward() : W=tensor([0.9442], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1269], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9442], requires_grad=True), W.grad=tensor([-0.0134]), b=tensor([0.1269], requires_grad=True), b.grad=tensor([0.0305])\n",
      "after  step() : W=tensor([0.9443], requires_grad=True), W.grad=tensor([-0.0134]), b=tensor([0.1266], requires_grad=True), b.grad=tensor([0.0305])\n",
      "=======================444==================\n",
      "hypothesis=\n",
      "tensor([[1.0709],\n",
      "        [2.0152],\n",
      "        [2.9595]], grad_fn=<AddBackward0>)\n",
      "loss=0.0022991911973804235\n",
      "before backward() : W=tensor([0.9443], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1266], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9443], requires_grad=True), W.grad=tensor([-0.0134]), b=tensor([0.1266], requires_grad=True), b.grad=tensor([0.0304])\n",
      "after  step() : W=tensor([0.9444], requires_grad=True), W.grad=tensor([-0.0134]), b=tensor([0.1263], requires_grad=True), b.grad=tensor([0.0304])\n",
      "=======================445==================\n",
      "hypothesis=\n",
      "tensor([[1.0707],\n",
      "        [2.0152],\n",
      "        [2.9596]], grad_fn=<AddBackward0>)\n",
      "loss=0.0022881512995809317\n",
      "before backward() : W=tensor([0.9444], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1263], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9444], requires_grad=True), W.grad=tensor([-0.0134]), b=tensor([0.1263], requires_grad=True), b.grad=tensor([0.0304])\n",
      "after  step() : W=tensor([0.9446], requires_grad=True), W.grad=tensor([-0.0134]), b=tensor([0.1260], requires_grad=True), b.grad=tensor([0.0304])\n",
      "=======================446==================\n",
      "hypothesis=\n",
      "tensor([[1.0706],\n",
      "        [2.0151],\n",
      "        [2.9597]], grad_fn=<AddBackward0>)\n",
      "loss=0.0022771668154746294\n",
      "before backward() : W=tensor([0.9446], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1260], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9446], requires_grad=True), W.grad=tensor([-0.0133]), b=tensor([0.1260], requires_grad=True), b.grad=tensor([0.0303])\n",
      "after  step() : W=tensor([0.9447], requires_grad=True), W.grad=tensor([-0.0133]), b=tensor([0.1257], requires_grad=True), b.grad=tensor([0.0303])\n",
      "=======================447==================\n",
      "hypothesis=\n",
      "tensor([[1.0704],\n",
      "        [2.0151],\n",
      "        [2.9598]], grad_fn=<AddBackward0>)\n",
      "loss=0.002266225405037403\n",
      "before backward() : W=tensor([0.9447], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1257], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9447], requires_grad=True), W.grad=tensor([-0.0133]), b=tensor([0.1257], requires_grad=True), b.grad=tensor([0.0302])\n",
      "after  step() : W=tensor([0.9448], requires_grad=True), W.grad=tensor([-0.0133]), b=tensor([0.1254], requires_grad=True), b.grad=tensor([0.0302])\n",
      "=======================448==================\n",
      "hypothesis=\n",
      "tensor([[1.0702],\n",
      "        [2.0151],\n",
      "        [2.9599]], grad_fn=<AddBackward0>)\n",
      "loss=0.0022553487215191126\n",
      "before backward() : W=tensor([0.9448], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1254], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9448], requires_grad=True), W.grad=tensor([-0.0133]), b=tensor([0.1254], requires_grad=True), b.grad=tensor([0.0301])\n",
      "after  step() : W=tensor([0.9450], requires_grad=True), W.grad=tensor([-0.0133]), b=tensor([0.1251], requires_grad=True), b.grad=tensor([0.0301])\n",
      "=======================449==================\n",
      "hypothesis=\n",
      "tensor([[1.0701],\n",
      "        [2.0150],\n",
      "        [2.9600]], grad_fn=<AddBackward0>)\n",
      "loss=0.0022445141803473234\n",
      "before backward() : W=tensor([0.9450], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1251], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9450], requires_grad=True), W.grad=tensor([-0.0132]), b=tensor([0.1251], requires_grad=True), b.grad=tensor([0.0301])\n",
      "after  step() : W=tensor([0.9451], requires_grad=True), W.grad=tensor([-0.0132]), b=tensor([0.1248], requires_grad=True), b.grad=tensor([0.0301])\n",
      "=======================450==================\n",
      "hypothesis=\n",
      "tensor([[1.0699],\n",
      "        [2.0150],\n",
      "        [2.9601]], grad_fn=<AddBackward0>)\n",
      "loss=0.0022337406408041716\n",
      "before backward() : W=tensor([0.9451], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1248], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9451], requires_grad=True), W.grad=tensor([-0.0132]), b=tensor([0.1248], requires_grad=True), b.grad=tensor([0.0300])\n",
      "after  step() : W=tensor([0.9452], requires_grad=True), W.grad=tensor([-0.0132]), b=tensor([0.1245], requires_grad=True), b.grad=tensor([0.0300])\n",
      "=======================451==================\n",
      "hypothesis=\n",
      "tensor([[1.0697],\n",
      "        [2.0150],\n",
      "        [2.9602]], grad_fn=<AddBackward0>)\n",
      "loss=0.002223006682470441\n",
      "before backward() : W=tensor([0.9452], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1245], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9452], requires_grad=True), W.grad=tensor([-0.0132]), b=tensor([0.1245], requires_grad=True), b.grad=tensor([0.0299])\n",
      "after  step() : W=tensor([0.9454], requires_grad=True), W.grad=tensor([-0.0132]), b=tensor([0.1242], requires_grad=True), b.grad=tensor([0.0299])\n",
      "=======================452==================\n",
      "hypothesis=\n",
      "tensor([[1.0696],\n",
      "        [2.0149],\n",
      "        [2.9603]], grad_fn=<AddBackward0>)\n",
      "loss=0.0022123288363218307\n",
      "before backward() : W=tensor([0.9454], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1242], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9454], requires_grad=True), W.grad=tensor([-0.0131]), b=tensor([0.1242], requires_grad=True), b.grad=tensor([0.0299])\n",
      "after  step() : W=tensor([0.9455], requires_grad=True), W.grad=tensor([-0.0131]), b=tensor([0.1239], requires_grad=True), b.grad=tensor([0.0299])\n",
      "=======================453==================\n",
      "hypothesis=\n",
      "tensor([[1.0694],\n",
      "        [2.0149],\n",
      "        [2.9604]], grad_fn=<AddBackward0>)\n",
      "loss=0.0022017094306647778\n",
      "before backward() : W=tensor([0.9455], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1239], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9455], requires_grad=True), W.grad=tensor([-0.0131]), b=tensor([0.1239], requires_grad=True), b.grad=tensor([0.0298])\n",
      "after  step() : W=tensor([0.9456], requires_grad=True), W.grad=tensor([-0.0131]), b=tensor([0.1236], requires_grad=True), b.grad=tensor([0.0298])\n",
      "=======================454==================\n",
      "hypothesis=\n",
      "tensor([[1.0692],\n",
      "        [2.0149],\n",
      "        [2.9605]], grad_fn=<AddBackward0>)\n",
      "loss=0.0021911412477493286\n",
      "before backward() : W=tensor([0.9456], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1236], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9456], requires_grad=True), W.grad=tensor([-0.0131]), b=tensor([0.1236], requires_grad=True), b.grad=tensor([0.0297])\n",
      "after  step() : W=tensor([0.9458], requires_grad=True), W.grad=tensor([-0.0131]), b=tensor([0.1233], requires_grad=True), b.grad=tensor([0.0297])\n",
      "=======================455==================\n",
      "hypothesis=\n",
      "tensor([[1.0691],\n",
      "        [2.0148],\n",
      "        [2.9606]], grad_fn=<AddBackward0>)\n",
      "loss=0.002180619863793254\n",
      "before backward() : W=tensor([0.9458], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1233], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9458], requires_grad=True), W.grad=tensor([-0.0130]), b=tensor([0.1233], requires_grad=True), b.grad=tensor([0.0296])\n",
      "after  step() : W=tensor([0.9459], requires_grad=True), W.grad=tensor([-0.0130]), b=tensor([0.1230], requires_grad=True), b.grad=tensor([0.0296])\n",
      "=======================456==================\n",
      "hypothesis=\n",
      "tensor([[1.0689],\n",
      "        [2.0148],\n",
      "        [2.9607]], grad_fn=<AddBackward0>)\n",
      "loss=0.0021701492369174957\n",
      "before backward() : W=tensor([0.9459], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1230], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9459], requires_grad=True), W.grad=tensor([-0.0130]), b=tensor([0.1230], requires_grad=True), b.grad=tensor([0.0296])\n",
      "after  step() : W=tensor([0.9460], requires_grad=True), W.grad=tensor([-0.0130]), b=tensor([0.1227], requires_grad=True), b.grad=tensor([0.0296])\n",
      "=======================457==================\n",
      "hypothesis=\n",
      "tensor([[1.0687],\n",
      "        [2.0147],\n",
      "        [2.9608]], grad_fn=<AddBackward0>)\n",
      "loss=0.0021597258746623993\n",
      "before backward() : W=tensor([0.9460], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1227], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9460], requires_grad=True), W.grad=tensor([-0.0130]), b=tensor([0.1227], requires_grad=True), b.grad=tensor([0.0295])\n",
      "after  step() : W=tensor([0.9462], requires_grad=True), W.grad=tensor([-0.0130]), b=tensor([0.1224], requires_grad=True), b.grad=tensor([0.0295])\n",
      "=======================458==================\n",
      "hypothesis=\n",
      "tensor([[1.0686],\n",
      "        [2.0147],\n",
      "        [2.9609]], grad_fn=<AddBackward0>)\n",
      "loss=0.002149356761947274\n",
      "before backward() : W=tensor([0.9462], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1224], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9462], requires_grad=True), W.grad=tensor([-0.0129]), b=tensor([0.1224], requires_grad=True), b.grad=tensor([0.0294])\n",
      "after  step() : W=tensor([0.9463], requires_grad=True), W.grad=tensor([-0.0129]), b=tensor([0.1221], requires_grad=True), b.grad=tensor([0.0294])\n",
      "=======================459==================\n",
      "hypothesis=\n",
      "tensor([[1.0684],\n",
      "        [2.0147],\n",
      "        [2.9610]], grad_fn=<AddBackward0>)\n",
      "loss=0.0021390244364738464\n",
      "before backward() : W=tensor([0.9463], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1221], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9463], requires_grad=True), W.grad=tensor([-0.0129]), b=tensor([0.1221], requires_grad=True), b.grad=tensor([0.0294])\n",
      "after  step() : W=tensor([0.9464], requires_grad=True), W.grad=tensor([-0.0129]), b=tensor([0.1218], requires_grad=True), b.grad=tensor([0.0294])\n",
      "=======================460==================\n",
      "hypothesis=\n",
      "tensor([[1.0682],\n",
      "        [2.0146],\n",
      "        [2.9611]], grad_fn=<AddBackward0>)\n",
      "loss=0.0021287614945322275\n",
      "before backward() : W=tensor([0.9464], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1218], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9464], requires_grad=True), W.grad=tensor([-0.0129]), b=tensor([0.1218], requires_grad=True), b.grad=tensor([0.0293])\n",
      "after  step() : W=tensor([0.9465], requires_grad=True), W.grad=tensor([-0.0129]), b=tensor([0.1215], requires_grad=True), b.grad=tensor([0.0293])\n",
      "=======================461==================\n",
      "hypothesis=\n",
      "tensor([[1.0681],\n",
      "        [2.0146],\n",
      "        [2.9611]], grad_fn=<AddBackward0>)\n",
      "loss=0.002118537900969386\n",
      "before backward() : W=tensor([0.9465], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1215], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9465], requires_grad=True), W.grad=tensor([-0.0129]), b=tensor([0.1215], requires_grad=True), b.grad=tensor([0.0292])\n",
      "after  step() : W=tensor([0.9467], requires_grad=True), W.grad=tensor([-0.0129]), b=tensor([0.1212], requires_grad=True), b.grad=tensor([0.0292])\n",
      "=======================462==================\n",
      "hypothesis=\n",
      "tensor([[1.0679],\n",
      "        [2.0146],\n",
      "        [2.9612]], grad_fn=<AddBackward0>)\n",
      "loss=0.0021083587780594826\n",
      "before backward() : W=tensor([0.9467], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1212], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9467], requires_grad=True), W.grad=tensor([-0.0128]), b=tensor([0.1212], requires_grad=True), b.grad=tensor([0.0291])\n",
      "after  step() : W=tensor([0.9468], requires_grad=True), W.grad=tensor([-0.0128]), b=tensor([0.1209], requires_grad=True), b.grad=tensor([0.0291])\n",
      "=======================463==================\n",
      "hypothesis=\n",
      "tensor([[1.0677],\n",
      "        [2.0145],\n",
      "        [2.9613]], grad_fn=<AddBackward0>)\n",
      "loss=0.0020982364658266306\n",
      "before backward() : W=tensor([0.9468], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1209], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9468], requires_grad=True), W.grad=tensor([-0.0128]), b=tensor([0.1209], requires_grad=True), b.grad=tensor([0.0291])\n",
      "after  step() : W=tensor([0.9469], requires_grad=True), W.grad=tensor([-0.0128]), b=tensor([0.1206], requires_grad=True), b.grad=tensor([0.0291])\n",
      "=======================464==================\n",
      "hypothesis=\n",
      "tensor([[1.0676],\n",
      "        [2.0145],\n",
      "        [2.9614]], grad_fn=<AddBackward0>)\n",
      "loss=0.0020881646778434515\n",
      "before backward() : W=tensor([0.9469], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1206], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9469], requires_grad=True), W.grad=tensor([-0.0128]), b=tensor([0.1206], requires_grad=True), b.grad=tensor([0.0290])\n",
      "after  step() : W=tensor([0.9471], requires_grad=True), W.grad=tensor([-0.0128]), b=tensor([0.1204], requires_grad=True), b.grad=tensor([0.0290])\n",
      "=======================465==================\n",
      "hypothesis=\n",
      "tensor([[1.0674],\n",
      "        [2.0145],\n",
      "        [2.9615]], grad_fn=<AddBackward0>)\n",
      "loss=0.00207813479937613\n",
      "before backward() : W=tensor([0.9471], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1204], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9471], requires_grad=True), W.grad=tensor([-0.0127]), b=tensor([0.1204], requires_grad=True), b.grad=tensor([0.0289])\n",
      "after  step() : W=tensor([0.9472], requires_grad=True), W.grad=tensor([-0.0127]), b=tensor([0.1201], requires_grad=True), b.grad=tensor([0.0289])\n",
      "=======================466==================\n",
      "hypothesis=\n",
      "tensor([[1.0673],\n",
      "        [2.0144],\n",
      "        [2.9616]], grad_fn=<AddBackward0>)\n",
      "loss=0.002068151952698827\n",
      "before backward() : W=tensor([0.9472], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1201], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9472], requires_grad=True), W.grad=tensor([-0.0127]), b=tensor([0.1201], requires_grad=True), b.grad=tensor([0.0289])\n",
      "after  step() : W=tensor([0.9473], requires_grad=True), W.grad=tensor([-0.0127]), b=tensor([0.1198], requires_grad=True), b.grad=tensor([0.0289])\n",
      "=======================467==================\n",
      "hypothesis=\n",
      "tensor([[1.0671],\n",
      "        [2.0144],\n",
      "        [2.9617]], grad_fn=<AddBackward0>)\n",
      "loss=0.002058222657069564\n",
      "before backward() : W=tensor([0.9473], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1198], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9473], requires_grad=True), W.grad=tensor([-0.0127]), b=tensor([0.1198], requires_grad=True), b.grad=tensor([0.0288])\n",
      "after  step() : W=tensor([0.9474], requires_grad=True), W.grad=tensor([-0.0127]), b=tensor([0.1195], requires_grad=True), b.grad=tensor([0.0288])\n",
      "=======================468==================\n",
      "hypothesis=\n",
      "tensor([[1.0669],\n",
      "        [2.0144],\n",
      "        [2.9618]], grad_fn=<AddBackward0>)\n",
      "loss=0.002048337133601308\n",
      "before backward() : W=tensor([0.9474], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1195], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9474], requires_grad=True), W.grad=tensor([-0.0126]), b=tensor([0.1195], requires_grad=True), b.grad=tensor([0.0287])\n",
      "after  step() : W=tensor([0.9476], requires_grad=True), W.grad=tensor([-0.0126]), b=tensor([0.1192], requires_grad=True), b.grad=tensor([0.0287])\n",
      "=======================469==================\n",
      "hypothesis=\n",
      "tensor([[1.0668],\n",
      "        [2.0143],\n",
      "        [2.9619]], grad_fn=<AddBackward0>)\n",
      "loss=0.0020384995732456446\n",
      "before backward() : W=tensor([0.9476], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1192], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9476], requires_grad=True), W.grad=tensor([-0.0126]), b=tensor([0.1192], requires_grad=True), b.grad=tensor([0.0287])\n",
      "after  step() : W=tensor([0.9477], requires_grad=True), W.grad=tensor([-0.0126]), b=tensor([0.1189], requires_grad=True), b.grad=tensor([0.0287])\n",
      "=======================470==================\n",
      "hypothesis=\n",
      "tensor([[1.0666],\n",
      "        [2.0143],\n",
      "        [2.9620]], grad_fn=<AddBackward0>)\n",
      "loss=0.002028715331107378\n",
      "before backward() : W=tensor([0.9477], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1189], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9477], requires_grad=True), W.grad=tensor([-0.0126]), b=tensor([0.1189], requires_grad=True), b.grad=tensor([0.0286])\n",
      "after  step() : W=tensor([0.9478], requires_grad=True), W.grad=tensor([-0.0126]), b=tensor([0.1186], requires_grad=True), b.grad=tensor([0.0286])\n",
      "=======================471==================\n",
      "hypothesis=\n",
      "tensor([[1.0664],\n",
      "        [2.0143],\n",
      "        [2.9621]], grad_fn=<AddBackward0>)\n",
      "loss=0.0020189702045172453\n",
      "before backward() : W=tensor([0.9478], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1186], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9478], requires_grad=True), W.grad=tensor([-0.0125]), b=tensor([0.1186], requires_grad=True), b.grad=tensor([0.0285])\n",
      "after  step() : W=tensor([0.9479], requires_grad=True), W.grad=tensor([-0.0125]), b=tensor([0.1183], requires_grad=True), b.grad=tensor([0.0285])\n",
      "=======================472==================\n",
      "hypothesis=\n",
      "tensor([[1.0663],\n",
      "        [2.0142],\n",
      "        [2.9622]], grad_fn=<AddBackward0>)\n",
      "loss=0.0020092767663300037\n",
      "before backward() : W=tensor([0.9479], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1183], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9479], requires_grad=True), W.grad=tensor([-0.0125]), b=tensor([0.1183], requires_grad=True), b.grad=tensor([0.0285])\n",
      "after  step() : W=tensor([0.9481], requires_grad=True), W.grad=tensor([-0.0125]), b=tensor([0.1181], requires_grad=True), b.grad=tensor([0.0285])\n",
      "=======================473==================\n",
      "hypothesis=\n",
      "tensor([[1.0661],\n",
      "        [2.0142],\n",
      "        [2.9623]], grad_fn=<AddBackward0>)\n",
      "loss=0.0019996296614408493\n",
      "before backward() : W=tensor([0.9481], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1181], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9481], requires_grad=True), W.grad=tensor([-0.0125]), b=tensor([0.1181], requires_grad=True), b.grad=tensor([0.0284])\n",
      "after  step() : W=tensor([0.9482], requires_grad=True), W.grad=tensor([-0.0125]), b=tensor([0.1178], requires_grad=True), b.grad=tensor([0.0284])\n",
      "=======================474==================\n",
      "hypothesis=\n",
      "tensor([[1.0660],\n",
      "        [2.0142],\n",
      "        [2.9623]], grad_fn=<AddBackward0>)\n",
      "loss=0.0019900284241884947\n",
      "before backward() : W=tensor([0.9482], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1178], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9482], requires_grad=True), W.grad=tensor([-0.0125]), b=tensor([0.1178], requires_grad=True), b.grad=tensor([0.0283])\n",
      "after  step() : W=tensor([0.9483], requires_grad=True), W.grad=tensor([-0.0125]), b=tensor([0.1175], requires_grad=True), b.grad=tensor([0.0283])\n",
      "=======================475==================\n",
      "hypothesis=\n",
      "tensor([[1.0658],\n",
      "        [2.0141],\n",
      "        [2.9624]], grad_fn=<AddBackward0>)\n",
      "loss=0.001980467000976205\n",
      "before backward() : W=tensor([0.9483], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1175], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9483], requires_grad=True), W.grad=tensor([-0.0124]), b=tensor([0.1175], requires_grad=True), b.grad=tensor([0.0282])\n",
      "after  step() : W=tensor([0.9484], requires_grad=True), W.grad=tensor([-0.0124]), b=tensor([0.1172], requires_grad=True), b.grad=tensor([0.0282])\n",
      "=======================476==================\n",
      "hypothesis=\n",
      "tensor([[1.0657],\n",
      "        [2.0141],\n",
      "        [2.9625]], grad_fn=<AddBackward0>)\n",
      "loss=0.0019709605257958174\n",
      "before backward() : W=tensor([0.9484], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1172], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9484], requires_grad=True), W.grad=tensor([-0.0124]), b=tensor([0.1172], requires_grad=True), b.grad=tensor([0.0282])\n",
      "after  step() : W=tensor([0.9486], requires_grad=True), W.grad=tensor([-0.0124]), b=tensor([0.1169], requires_grad=True), b.grad=tensor([0.0282])\n",
      "=======================477==================\n",
      "hypothesis=\n",
      "tensor([[1.0655],\n",
      "        [2.0141],\n",
      "        [2.9626]], grad_fn=<AddBackward0>)\n",
      "loss=0.00196149293333292\n",
      "before backward() : W=tensor([0.9486], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1169], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9486], requires_grad=True), W.grad=tensor([-0.0124]), b=tensor([0.1169], requires_grad=True), b.grad=tensor([0.0281])\n",
      "after  step() : W=tensor([0.9487], requires_grad=True), W.grad=tensor([-0.0124]), b=tensor([0.1167], requires_grad=True), b.grad=tensor([0.0281])\n",
      "=======================478==================\n",
      "hypothesis=\n",
      "tensor([[1.0653],\n",
      "        [2.0140],\n",
      "        [2.9627]], grad_fn=<AddBackward0>)\n",
      "loss=0.0019520785426720977\n",
      "before backward() : W=tensor([0.9487], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1167], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9487], requires_grad=True), W.grad=tensor([-0.0123]), b=tensor([0.1167], requires_grad=True), b.grad=tensor([0.0280])\n",
      "after  step() : W=tensor([0.9488], requires_grad=True), W.grad=tensor([-0.0123]), b=tensor([0.1164], requires_grad=True), b.grad=tensor([0.0280])\n",
      "=======================479==================\n",
      "hypothesis=\n",
      "tensor([[1.0652],\n",
      "        [2.0140],\n",
      "        [2.9628]], grad_fn=<AddBackward0>)\n",
      "loss=0.0019427024526521564\n",
      "before backward() : W=tensor([0.9488], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1164], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9488], requires_grad=True), W.grad=tensor([-0.0123]), b=tensor([0.1164], requires_grad=True), b.grad=tensor([0.0280])\n",
      "after  step() : W=tensor([0.9489], requires_grad=True), W.grad=tensor([-0.0123]), b=tensor([0.1161], requires_grad=True), b.grad=tensor([0.0280])\n",
      "=======================480==================\n",
      "hypothesis=\n",
      "tensor([[1.0650],\n",
      "        [2.0140],\n",
      "        [2.9629]], grad_fn=<AddBackward0>)\n",
      "loss=0.001933374791406095\n",
      "before backward() : W=tensor([0.9489], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1161], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9489], requires_grad=True), W.grad=tensor([-0.0123]), b=tensor([0.1161], requires_grad=True), b.grad=tensor([0.0279])\n",
      "after  step() : W=tensor([0.9491], requires_grad=True), W.grad=tensor([-0.0123]), b=tensor([0.1158], requires_grad=True), b.grad=tensor([0.0279])\n",
      "=======================481==================\n",
      "hypothesis=\n",
      "tensor([[1.0649],\n",
      "        [2.0139],\n",
      "        [2.9630]], grad_fn=<AddBackward0>)\n",
      "loss=0.001924088574014604\n",
      "before backward() : W=tensor([0.9491], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1158], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9491], requires_grad=True), W.grad=tensor([-0.0122]), b=tensor([0.1158], requires_grad=True), b.grad=tensor([0.0278])\n",
      "after  step() : W=tensor([0.9492], requires_grad=True), W.grad=tensor([-0.0122]), b=tensor([0.1155], requires_grad=True), b.grad=tensor([0.0278])\n",
      "=======================482==================\n",
      "hypothesis=\n",
      "tensor([[1.0647],\n",
      "        [2.0139],\n",
      "        [2.9631]], grad_fn=<AddBackward0>)\n",
      "loss=0.0019148518331348896\n",
      "before backward() : W=tensor([0.9492], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1155], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9492], requires_grad=True), W.grad=tensor([-0.0122]), b=tensor([0.1155], requires_grad=True), b.grad=tensor([0.0278])\n",
      "after  step() : W=tensor([0.9493], requires_grad=True), W.grad=tensor([-0.0122]), b=tensor([0.1153], requires_grad=True), b.grad=tensor([0.0278])\n",
      "=======================483==================\n",
      "hypothesis=\n",
      "tensor([[1.0646],\n",
      "        [2.0139],\n",
      "        [2.9632]], grad_fn=<AddBackward0>)\n",
      "loss=0.001905652810819447\n",
      "before backward() : W=tensor([0.9493], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1153], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9493], requires_grad=True), W.grad=tensor([-0.0122]), b=tensor([0.1153], requires_grad=True), b.grad=tensor([0.0277])\n",
      "after  step() : W=tensor([0.9494], requires_grad=True), W.grad=tensor([-0.0122]), b=tensor([0.1150], requires_grad=True), b.grad=tensor([0.0277])\n",
      "=======================484==================\n",
      "hypothesis=\n",
      "tensor([[1.0644],\n",
      "        [2.0138],\n",
      "        [2.9632]], grad_fn=<AddBackward0>)\n",
      "loss=0.0018965030321851373\n",
      "before backward() : W=tensor([0.9494], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1150], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9494], requires_grad=True), W.grad=tensor([-0.0122]), b=tensor([0.1150], requires_grad=True), b.grad=tensor([0.0276])\n",
      "after  step() : W=tensor([0.9495], requires_grad=True), W.grad=tensor([-0.0122]), b=tensor([0.1147], requires_grad=True), b.grad=tensor([0.0276])\n",
      "=======================485==================\n",
      "hypothesis=\n",
      "tensor([[1.0642],\n",
      "        [2.0138],\n",
      "        [2.9633]], grad_fn=<AddBackward0>)\n",
      "loss=0.0018873977242037654\n",
      "before backward() : W=tensor([0.9495], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1147], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9495], requires_grad=True), W.grad=tensor([-0.0121]), b=tensor([0.1147], requires_grad=True), b.grad=tensor([0.0276])\n",
      "after  step() : W=tensor([0.9497], requires_grad=True), W.grad=tensor([-0.0121]), b=tensor([0.1144], requires_grad=True), b.grad=tensor([0.0276])\n",
      "=======================486==================\n",
      "hypothesis=\n",
      "tensor([[1.0641],\n",
      "        [2.0138],\n",
      "        [2.9634]], grad_fn=<AddBackward0>)\n",
      "loss=0.001878330484032631\n",
      "before backward() : W=tensor([0.9497], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1144], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9497], requires_grad=True), W.grad=tensor([-0.0121]), b=tensor([0.1144], requires_grad=True), b.grad=tensor([0.0275])\n",
      "after  step() : W=tensor([0.9498], requires_grad=True), W.grad=tensor([-0.0121]), b=tensor([0.1142], requires_grad=True), b.grad=tensor([0.0275])\n",
      "=======================487==================\n",
      "hypothesis=\n",
      "tensor([[1.0639],\n",
      "        [2.0137],\n",
      "        [2.9635]], grad_fn=<AddBackward0>)\n",
      "loss=0.0018693112069740891\n",
      "before backward() : W=tensor([0.9498], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1142], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9498], requires_grad=True), W.grad=tensor([-0.0121]), b=tensor([0.1142], requires_grad=True), b.grad=tensor([0.0274])\n",
      "after  step() : W=tensor([0.9499], requires_grad=True), W.grad=tensor([-0.0121]), b=tensor([0.1139], requires_grad=True), b.grad=tensor([0.0274])\n",
      "=======================488==================\n",
      "hypothesis=\n",
      "tensor([[1.0638],\n",
      "        [2.0137],\n",
      "        [2.9636]], grad_fn=<AddBackward0>)\n",
      "loss=0.0018603317439556122\n",
      "before backward() : W=tensor([0.9499], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1139], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9499], requires_grad=True), W.grad=tensor([-0.0120]), b=tensor([0.1139], requires_grad=True), b.grad=tensor([0.0274])\n",
      "after  step() : W=tensor([0.9500], requires_grad=True), W.grad=tensor([-0.0120]), b=tensor([0.1136], requires_grad=True), b.grad=tensor([0.0274])\n",
      "=======================489==================\n",
      "hypothesis=\n",
      "tensor([[1.0636],\n",
      "        [2.0137],\n",
      "        [2.9637]], grad_fn=<AddBackward0>)\n",
      "loss=0.001851403503678739\n",
      "before backward() : W=tensor([0.9500], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1136], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9500], requires_grad=True), W.grad=tensor([-0.0120]), b=tensor([0.1136], requires_grad=True), b.grad=tensor([0.0273])\n",
      "after  step() : W=tensor([0.9501], requires_grad=True), W.grad=tensor([-0.0120]), b=tensor([0.1133], requires_grad=True), b.grad=tensor([0.0273])\n",
      "=======================490==================\n",
      "hypothesis=\n",
      "tensor([[1.0635],\n",
      "        [2.0136],\n",
      "        [2.9638]], grad_fn=<AddBackward0>)\n",
      "loss=0.0018425103044137359\n",
      "before backward() : W=tensor([0.9501], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1133], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9501], requires_grad=True), W.grad=tensor([-0.0120]), b=tensor([0.1133], requires_grad=True), b.grad=tensor([0.0272])\n",
      "after  step() : W=tensor([0.9503], requires_grad=True), W.grad=tensor([-0.0120]), b=tensor([0.1131], requires_grad=True), b.grad=tensor([0.0272])\n",
      "=======================491==================\n",
      "hypothesis=\n",
      "tensor([[1.0633],\n",
      "        [2.0136],\n",
      "        [2.9639]], grad_fn=<AddBackward0>)\n",
      "loss=0.0018336651846766472\n",
      "before backward() : W=tensor([0.9503], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1131], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9503], requires_grad=True), W.grad=tensor([-0.0120]), b=tensor([0.1131], requires_grad=True), b.grad=tensor([0.0272])\n",
      "after  step() : W=tensor([0.9504], requires_grad=True), W.grad=tensor([-0.0120]), b=tensor([0.1128], requires_grad=True), b.grad=tensor([0.0272])\n",
      "=======================492==================\n",
      "hypothesis=\n",
      "tensor([[1.0632],\n",
      "        [2.0136],\n",
      "        [2.9639]], grad_fn=<AddBackward0>)\n",
      "loss=0.0018248563865199685\n",
      "before backward() : W=tensor([0.9504], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1128], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9504], requires_grad=True), W.grad=tensor([-0.0119]), b=tensor([0.1128], requires_grad=True), b.grad=tensor([0.0271])\n",
      "after  step() : W=tensor([0.9505], requires_grad=True), W.grad=tensor([-0.0119]), b=tensor([0.1125], requires_grad=True), b.grad=tensor([0.0271])\n",
      "=======================493==================\n",
      "hypothesis=\n",
      "tensor([[1.0630],\n",
      "        [2.0135],\n",
      "        [2.9640]], grad_fn=<AddBackward0>)\n",
      "loss=0.0018160953186452389\n",
      "before backward() : W=tensor([0.9505], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1125], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9505], requires_grad=True), W.grad=tensor([-0.0119]), b=tensor([0.1125], requires_grad=True), b.grad=tensor([0.0270])\n",
      "after  step() : W=tensor([0.9506], requires_grad=True), W.grad=tensor([-0.0119]), b=tensor([0.1122], requires_grad=True), b.grad=tensor([0.0270])\n",
      "=======================494==================\n",
      "hypothesis=\n",
      "tensor([[1.0629],\n",
      "        [2.0135],\n",
      "        [2.9641]], grad_fn=<AddBackward0>)\n",
      "loss=0.0018073668470606208\n",
      "before backward() : W=tensor([0.9506], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1122], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9506], requires_grad=True), W.grad=tensor([-0.0119]), b=tensor([0.1122], requires_grad=True), b.grad=tensor([0.0270])\n",
      "after  step() : W=tensor([0.9507], requires_grad=True), W.grad=tensor([-0.0119]), b=tensor([0.1120], requires_grad=True), b.grad=tensor([0.0270])\n",
      "=======================495==================\n",
      "hypothesis=\n",
      "tensor([[1.0627],\n",
      "        [2.0135],\n",
      "        [2.9642]], grad_fn=<AddBackward0>)\n",
      "loss=0.0017986890161409974\n",
      "before backward() : W=tensor([0.9507], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1120], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9507], requires_grad=True), W.grad=tensor([-0.0118]), b=tensor([0.1120], requires_grad=True), b.grad=tensor([0.0269])\n",
      "after  step() : W=tensor([0.9509], requires_grad=True), W.grad=tensor([-0.0118]), b=tensor([0.1117], requires_grad=True), b.grad=tensor([0.0269])\n",
      "=======================496==================\n",
      "hypothesis=\n",
      "tensor([[1.0626],\n",
      "        [2.0134],\n",
      "        [2.9643]], grad_fn=<AddBackward0>)\n",
      "loss=0.0017900563543662429\n",
      "before backward() : W=tensor([0.9509], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1117], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9509], requires_grad=True), W.grad=tensor([-0.0118]), b=tensor([0.1117], requires_grad=True), b.grad=tensor([0.0269])\n",
      "after  step() : W=tensor([0.9510], requires_grad=True), W.grad=tensor([-0.0118]), b=tensor([0.1114], requires_grad=True), b.grad=tensor([0.0269])\n",
      "=======================497==================\n",
      "hypothesis=\n",
      "tensor([[1.0624],\n",
      "        [2.0134],\n",
      "        [2.9644]], grad_fn=<AddBackward0>)\n",
      "loss=0.0017814580351114273\n",
      "before backward() : W=tensor([0.9510], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1114], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9510], requires_grad=True), W.grad=tensor([-0.0118]), b=tensor([0.1114], requires_grad=True), b.grad=tensor([0.0268])\n",
      "after  step() : W=tensor([0.9511], requires_grad=True), W.grad=tensor([-0.0118]), b=tensor([0.1112], requires_grad=True), b.grad=tensor([0.0268])\n",
      "=======================498==================\n",
      "hypothesis=\n",
      "tensor([[1.0623],\n",
      "        [2.0134],\n",
      "        [2.9645]], grad_fn=<AddBackward0>)\n",
      "loss=0.0017729029059410095\n",
      "before backward() : W=tensor([0.9511], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1112], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9511], requires_grad=True), W.grad=tensor([-0.0118]), b=tensor([0.1112], requires_grad=True), b.grad=tensor([0.0267])\n",
      "after  step() : W=tensor([0.9512], requires_grad=True), W.grad=tensor([-0.0118]), b=tensor([0.1109], requires_grad=True), b.grad=tensor([0.0267])\n",
      "=======================499==================\n",
      "hypothesis=\n",
      "tensor([[1.0621],\n",
      "        [2.0133],\n",
      "        [2.9645]], grad_fn=<AddBackward0>)\n",
      "loss=0.0017643887549638748\n",
      "before backward() : W=tensor([0.9512], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1109], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9512], requires_grad=True), W.grad=tensor([-0.0117]), b=tensor([0.1109], requires_grad=True), b.grad=tensor([0.0267])\n",
      "after  step() : W=tensor([0.9513], requires_grad=True), W.grad=tensor([-0.0117]), b=tensor([0.1106], requires_grad=True), b.grad=tensor([0.0267])\n",
      "=======================500==================\n",
      "hypothesis=\n",
      "tensor([[1.0620],\n",
      "        [2.0133],\n",
      "        [2.9646]], grad_fn=<AddBackward0>)\n",
      "loss=0.001755917095579207\n",
      "before backward() : W=tensor([0.9513], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1106], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9513], requires_grad=True), W.grad=tensor([-0.0117]), b=tensor([0.1106], requires_grad=True), b.grad=tensor([0.0266])\n",
      "after  step() : W=tensor([0.9514], requires_grad=True), W.grad=tensor([-0.0117]), b=tensor([0.1104], requires_grad=True), b.grad=tensor([0.0266])\n",
      "=======================501==================\n",
      "hypothesis=\n",
      "tensor([[1.0618],\n",
      "        [2.0133],\n",
      "        [2.9647]], grad_fn=<AddBackward0>)\n",
      "loss=0.00174748117569834\n",
      "before backward() : W=tensor([0.9514], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1104], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9514], requires_grad=True), W.grad=tensor([-0.0117]), b=tensor([0.1104], requires_grad=True), b.grad=tensor([0.0265])\n",
      "after  step() : W=tensor([0.9516], requires_grad=True), W.grad=tensor([-0.0117]), b=tensor([0.1101], requires_grad=True), b.grad=tensor([0.0265])\n",
      "=======================502==================\n",
      "hypothesis=\n",
      "tensor([[1.0617],\n",
      "        [2.0132],\n",
      "        [2.9648]], grad_fn=<AddBackward0>)\n",
      "loss=0.0017390962457284331\n",
      "before backward() : W=tensor([0.9516], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1101], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9516], requires_grad=True), W.grad=tensor([-0.0116]), b=tensor([0.1101], requires_grad=True), b.grad=tensor([0.0265])\n",
      "after  step() : W=tensor([0.9517], requires_grad=True), W.grad=tensor([-0.0116]), b=tensor([0.1098], requires_grad=True), b.grad=tensor([0.0265])\n",
      "=======================503==================\n",
      "hypothesis=\n",
      "tensor([[1.0615],\n",
      "        [2.0132],\n",
      "        [2.9649]], grad_fn=<AddBackward0>)\n",
      "loss=0.0017307434463873506\n",
      "before backward() : W=tensor([0.9517], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1098], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9517], requires_grad=True), W.grad=tensor([-0.0116]), b=tensor([0.1098], requires_grad=True), b.grad=tensor([0.0264])\n",
      "after  step() : W=tensor([0.9518], requires_grad=True), W.grad=tensor([-0.0116]), b=tensor([0.1096], requires_grad=True), b.grad=tensor([0.0264])\n",
      "=======================504==================\n",
      "hypothesis=\n",
      "tensor([[1.0614],\n",
      "        [2.0132],\n",
      "        [2.9650]], grad_fn=<AddBackward0>)\n",
      "loss=0.0017224311595782638\n",
      "before backward() : W=tensor([0.9518], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1096], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9518], requires_grad=True), W.grad=tensor([-0.0116]), b=tensor([0.1096], requires_grad=True), b.grad=tensor([0.0263])\n",
      "after  step() : W=tensor([0.9519], requires_grad=True), W.grad=tensor([-0.0116]), b=tensor([0.1093], requires_grad=True), b.grad=tensor([0.0263])\n",
      "=======================505==================\n",
      "hypothesis=\n",
      "tensor([[1.0612],\n",
      "        [2.0131],\n",
      "        [2.9651]], grad_fn=<AddBackward0>)\n",
      "loss=0.0017141635762527585\n",
      "before backward() : W=tensor([0.9519], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1093], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9519], requires_grad=True), W.grad=tensor([-0.0116]), b=tensor([0.1093], requires_grad=True), b.grad=tensor([0.0263])\n",
      "after  step() : W=tensor([0.9520], requires_grad=True), W.grad=tensor([-0.0116]), b=tensor([0.1090], requires_grad=True), b.grad=tensor([0.0263])\n",
      "=======================506==================\n",
      "hypothesis=\n",
      "tensor([[1.0611],\n",
      "        [2.0131],\n",
      "        [2.9651]], grad_fn=<AddBackward0>)\n",
      "loss=0.001705925795249641\n",
      "before backward() : W=tensor([0.9520], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1090], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9520], requires_grad=True), W.grad=tensor([-0.0115]), b=tensor([0.1090], requires_grad=True), b.grad=tensor([0.0262])\n",
      "after  step() : W=tensor([0.9521], requires_grad=True), W.grad=tensor([-0.0115]), b=tensor([0.1088], requires_grad=True), b.grad=tensor([0.0262])\n",
      "=======================507==================\n",
      "hypothesis=\n",
      "tensor([[1.0609],\n",
      "        [2.0131],\n",
      "        [2.9652]], grad_fn=<AddBackward0>)\n",
      "loss=0.001697735395282507\n",
      "before backward() : W=tensor([0.9521], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1088], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9521], requires_grad=True), W.grad=tensor([-0.0115]), b=tensor([0.1088], requires_grad=True), b.grad=tensor([0.0262])\n",
      "after  step() : W=tensor([0.9523], requires_grad=True), W.grad=tensor([-0.0115]), b=tensor([0.1085], requires_grad=True), b.grad=tensor([0.0262])\n",
      "=======================508==================\n",
      "hypothesis=\n",
      "tensor([[1.0608],\n",
      "        [2.0130],\n",
      "        [2.9653]], grad_fn=<AddBackward0>)\n",
      "loss=0.001689583994448185\n",
      "before backward() : W=tensor([0.9523], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1085], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9523], requires_grad=True), W.grad=tensor([-0.0115]), b=tensor([0.1085], requires_grad=True), b.grad=tensor([0.0261])\n",
      "after  step() : W=tensor([0.9524], requires_grad=True), W.grad=tensor([-0.0115]), b=tensor([0.1083], requires_grad=True), b.grad=tensor([0.0261])\n",
      "=======================509==================\n",
      "hypothesis=\n",
      "tensor([[1.0606],\n",
      "        [2.0130],\n",
      "        [2.9654]], grad_fn=<AddBackward0>)\n",
      "loss=0.001681474968791008\n",
      "before backward() : W=tensor([0.9524], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1083], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9524], requires_grad=True), W.grad=tensor([-0.0114]), b=tensor([0.1083], requires_grad=True), b.grad=tensor([0.0260])\n",
      "after  step() : W=tensor([0.9525], requires_grad=True), W.grad=tensor([-0.0114]), b=tensor([0.1080], requires_grad=True), b.grad=tensor([0.0260])\n",
      "=======================510==================\n",
      "hypothesis=\n",
      "tensor([[1.0605],\n",
      "        [2.0130],\n",
      "        [2.9655]], grad_fn=<AddBackward0>)\n",
      "loss=0.0016733979573473334\n",
      "before backward() : W=tensor([0.9525], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1080], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9525], requires_grad=True), W.grad=tensor([-0.0114]), b=tensor([0.1080], requires_grad=True), b.grad=tensor([0.0260])\n",
      "after  step() : W=tensor([0.9526], requires_grad=True), W.grad=tensor([-0.0114]), b=tensor([0.1077], requires_grad=True), b.grad=tensor([0.0260])\n",
      "=======================511==================\n",
      "hypothesis=\n",
      "tensor([[1.0603],\n",
      "        [2.0130],\n",
      "        [2.9656]], grad_fn=<AddBackward0>)\n",
      "loss=0.0016653621569275856\n",
      "before backward() : W=tensor([0.9526], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1077], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9526], requires_grad=True), W.grad=tensor([-0.0114]), b=tensor([0.1077], requires_grad=True), b.grad=tensor([0.0259])\n",
      "after  step() : W=tensor([0.9527], requires_grad=True), W.grad=tensor([-0.0114]), b=tensor([0.1075], requires_grad=True), b.grad=tensor([0.0259])\n",
      "=======================512==================\n",
      "hypothesis=\n",
      "tensor([[1.0602],\n",
      "        [2.0129],\n",
      "        [2.9656]], grad_fn=<AddBackward0>)\n",
      "loss=0.0016573620960116386\n",
      "before backward() : W=tensor([0.9527], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1075], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9527], requires_grad=True), W.grad=tensor([-0.0114]), b=tensor([0.1075], requires_grad=True), b.grad=tensor([0.0258])\n",
      "after  step() : W=tensor([0.9528], requires_grad=True), W.grad=tensor([-0.0114]), b=tensor([0.1072], requires_grad=True), b.grad=tensor([0.0258])\n",
      "=======================513==================\n",
      "hypothesis=\n",
      "tensor([[1.0601],\n",
      "        [2.0129],\n",
      "        [2.9657]], grad_fn=<AddBackward0>)\n",
      "loss=0.0016494066221639514\n",
      "before backward() : W=tensor([0.9528], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1072], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9528], requires_grad=True), W.grad=tensor([-0.0113]), b=tensor([0.1072], requires_grad=True), b.grad=tensor([0.0258])\n",
      "after  step() : W=tensor([0.9529], requires_grad=True), W.grad=tensor([-0.0113]), b=tensor([0.1070], requires_grad=True), b.grad=tensor([0.0258])\n",
      "=======================514==================\n",
      "hypothesis=\n",
      "tensor([[1.0599],\n",
      "        [2.0129],\n",
      "        [2.9658]], grad_fn=<AddBackward0>)\n",
      "loss=0.0016414844430983067\n",
      "before backward() : W=tensor([0.9529], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1070], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9529], requires_grad=True), W.grad=tensor([-0.0113]), b=tensor([0.1070], requires_grad=True), b.grad=tensor([0.0257])\n",
      "after  step() : W=tensor([0.9531], requires_grad=True), W.grad=tensor([-0.0113]), b=tensor([0.1067], requires_grad=True), b.grad=tensor([0.0257])\n",
      "=======================515==================\n",
      "hypothesis=\n",
      "tensor([[1.0598],\n",
      "        [2.0128],\n",
      "        [2.9659]], grad_fn=<AddBackward0>)\n",
      "loss=0.0016336011467501521\n",
      "before backward() : W=tensor([0.9531], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1067], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9531], requires_grad=True), W.grad=tensor([-0.0113]), b=tensor([0.1067], requires_grad=True), b.grad=tensor([0.0257])\n",
      "after  step() : W=tensor([0.9532], requires_grad=True), W.grad=tensor([-0.0113]), b=tensor([0.1065], requires_grad=True), b.grad=tensor([0.0257])\n",
      "=======================516==================\n",
      "hypothesis=\n",
      "tensor([[1.0596],\n",
      "        [2.0128],\n",
      "        [2.9660]], grad_fn=<AddBackward0>)\n",
      "loss=0.0016257535899057984\n",
      "before backward() : W=tensor([0.9532], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1065], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9532], requires_grad=True), W.grad=tensor([-0.0113]), b=tensor([0.1065], requires_grad=True), b.grad=tensor([0.0256])\n",
      "after  step() : W=tensor([0.9533], requires_grad=True), W.grad=tensor([-0.0113]), b=tensor([0.1062], requires_grad=True), b.grad=tensor([0.0256])\n",
      "=======================517==================\n",
      "hypothesis=\n",
      "tensor([[1.0595],\n",
      "        [2.0128],\n",
      "        [2.9660]], grad_fn=<AddBackward0>)\n",
      "loss=0.0016179493395611644\n",
      "before backward() : W=tensor([0.9533], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1062], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9533], requires_grad=True), W.grad=tensor([-0.0112]), b=tensor([0.1062], requires_grad=True), b.grad=tensor([0.0255])\n",
      "after  step() : W=tensor([0.9534], requires_grad=True), W.grad=tensor([-0.0112]), b=tensor([0.1059], requires_grad=True), b.grad=tensor([0.0255])\n",
      "=======================518==================\n",
      "hypothesis=\n",
      "tensor([[1.0593],\n",
      "        [2.0127],\n",
      "        [2.9661]], grad_fn=<AddBackward0>)\n",
      "loss=0.0016101786168292165\n",
      "before backward() : W=tensor([0.9534], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1059], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9534], requires_grad=True), W.grad=tensor([-0.0112]), b=tensor([0.1059], requires_grad=True), b.grad=tensor([0.0255])\n",
      "after  step() : W=tensor([0.9535], requires_grad=True), W.grad=tensor([-0.0112]), b=tensor([0.1057], requires_grad=True), b.grad=tensor([0.0255])\n",
      "=======================519==================\n",
      "hypothesis=\n",
      "tensor([[1.0592],\n",
      "        [2.0127],\n",
      "        [2.9662]], grad_fn=<AddBackward0>)\n",
      "loss=0.0016024475917220116\n",
      "before backward() : W=tensor([0.9535], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1057], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9535], requires_grad=True), W.grad=tensor([-0.0112]), b=tensor([0.1057], requires_grad=True), b.grad=tensor([0.0254])\n",
      "after  step() : W=tensor([0.9536], requires_grad=True), W.grad=tensor([-0.0112]), b=tensor([0.1054], requires_grad=True), b.grad=tensor([0.0254])\n",
      "=======================520==================\n",
      "hypothesis=\n",
      "tensor([[1.0591],\n",
      "        [2.0127],\n",
      "        [2.9663]], grad_fn=<AddBackward0>)\n",
      "loss=0.0015947497449815273\n",
      "before backward() : W=tensor([0.9536], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1054], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9536], requires_grad=True), W.grad=tensor([-0.0111]), b=tensor([0.1054], requires_grad=True), b.grad=tensor([0.0253])\n",
      "after  step() : W=tensor([0.9537], requires_grad=True), W.grad=tensor([-0.0111]), b=tensor([0.1052], requires_grad=True), b.grad=tensor([0.0253])\n",
      "=======================521==================\n",
      "hypothesis=\n",
      "tensor([[1.0589],\n",
      "        [2.0126],\n",
      "        [2.9664]], grad_fn=<AddBackward0>)\n",
      "loss=0.0015870952047407627\n",
      "before backward() : W=tensor([0.9537], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1052], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9537], requires_grad=True), W.grad=tensor([-0.0111]), b=tensor([0.1052], requires_grad=True), b.grad=tensor([0.0253])\n",
      "after  step() : W=tensor([0.9538], requires_grad=True), W.grad=tensor([-0.0111]), b=tensor([0.1049], requires_grad=True), b.grad=tensor([0.0253])\n",
      "=======================522==================\n",
      "hypothesis=\n",
      "tensor([[1.0588],\n",
      "        [2.0126],\n",
      "        [2.9665]], grad_fn=<AddBackward0>)\n",
      "loss=0.0015794677892699838\n",
      "before backward() : W=tensor([0.9538], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1049], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9538], requires_grad=True), W.grad=tensor([-0.0111]), b=tensor([0.1049], requires_grad=True), b.grad=tensor([0.0252])\n",
      "after  step() : W=tensor([0.9540], requires_grad=True), W.grad=tensor([-0.0111]), b=tensor([0.1047], requires_grad=True), b.grad=tensor([0.0252])\n",
      "=======================523==================\n",
      "hypothesis=\n",
      "tensor([[1.0586],\n",
      "        [2.0126],\n",
      "        [2.9665]], grad_fn=<AddBackward0>)\n",
      "loss=0.00157188531011343\n",
      "before backward() : W=tensor([0.9540], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1047], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9540], requires_grad=True), W.grad=tensor([-0.0111]), b=tensor([0.1047], requires_grad=True), b.grad=tensor([0.0252])\n",
      "after  step() : W=tensor([0.9541], requires_grad=True), W.grad=tensor([-0.0111]), b=tensor([0.1044], requires_grad=True), b.grad=tensor([0.0252])\n",
      "=======================524==================\n",
      "hypothesis=\n",
      "tensor([[1.0585],\n",
      "        [2.0126],\n",
      "        [2.9666]], grad_fn=<AddBackward0>)\n",
      "loss=0.0015643424121662974\n",
      "before backward() : W=tensor([0.9541], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1044], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9541], requires_grad=True), W.grad=tensor([-0.0110]), b=tensor([0.1044], requires_grad=True), b.grad=tensor([0.0251])\n",
      "after  step() : W=tensor([0.9542], requires_grad=True), W.grad=tensor([-0.0110]), b=tensor([0.1042], requires_grad=True), b.grad=tensor([0.0251])\n",
      "=======================525==================\n",
      "hypothesis=\n",
      "tensor([[1.0583],\n",
      "        [2.0125],\n",
      "        [2.9667]], grad_fn=<AddBackward0>)\n",
      "loss=0.001556824310682714\n",
      "before backward() : W=tensor([0.9542], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1042], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9542], requires_grad=True), W.grad=tensor([-0.0110]), b=tensor([0.1042], requires_grad=True), b.grad=tensor([0.0250])\n",
      "after  step() : W=tensor([0.9543], requires_grad=True), W.grad=tensor([-0.0110]), b=tensor([0.1039], requires_grad=True), b.grad=tensor([0.0250])\n",
      "=======================526==================\n",
      "hypothesis=\n",
      "tensor([[1.0582],\n",
      "        [2.0125],\n",
      "        [2.9668]], grad_fn=<AddBackward0>)\n",
      "loss=0.0015493511455133557\n",
      "before backward() : W=tensor([0.9543], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1039], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9543], requires_grad=True), W.grad=tensor([-0.0110]), b=tensor([0.1039], requires_grad=True), b.grad=tensor([0.0250])\n",
      "after  step() : W=tensor([0.9544], requires_grad=True), W.grad=tensor([-0.0110]), b=tensor([0.1037], requires_grad=True), b.grad=tensor([0.0250])\n",
      "=======================527==================\n",
      "hypothesis=\n",
      "tensor([[1.0581],\n",
      "        [2.0125],\n",
      "        [2.9669]], grad_fn=<AddBackward0>)\n",
      "loss=0.0015419101109728217\n",
      "before backward() : W=tensor([0.9544], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1037], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9544], requires_grad=True), W.grad=tensor([-0.0110]), b=tensor([0.1037], requires_grad=True), b.grad=tensor([0.0249])\n",
      "after  step() : W=tensor([0.9545], requires_grad=True), W.grad=tensor([-0.0110]), b=tensor([0.1034], requires_grad=True), b.grad=tensor([0.0249])\n",
      "=======================528==================\n",
      "hypothesis=\n",
      "tensor([[1.0579],\n",
      "        [2.0124],\n",
      "        [2.9669]], grad_fn=<AddBackward0>)\n",
      "loss=0.001534505863673985\n",
      "before backward() : W=tensor([0.9545], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1034], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9545], requires_grad=True), W.grad=tensor([-0.0109]), b=tensor([0.1034], requires_grad=True), b.grad=tensor([0.0249])\n",
      "after  step() : W=tensor([0.9546], requires_grad=True), W.grad=tensor([-0.0109]), b=tensor([0.1032], requires_grad=True), b.grad=tensor([0.0249])\n",
      "=======================529==================\n",
      "hypothesis=\n",
      "tensor([[1.0578],\n",
      "        [2.0124],\n",
      "        [2.9670]], grad_fn=<AddBackward0>)\n",
      "loss=0.0015271404990926385\n",
      "before backward() : W=tensor([0.9546], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1032], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9546], requires_grad=True), W.grad=tensor([-0.0109]), b=tensor([0.1032], requires_grad=True), b.grad=tensor([0.0248])\n",
      "after  step() : W=tensor([0.9547], requires_grad=True), W.grad=tensor([-0.0109]), b=tensor([0.1029], requires_grad=True), b.grad=tensor([0.0248])\n",
      "=======================530==================\n",
      "hypothesis=\n",
      "tensor([[1.0576],\n",
      "        [2.0124],\n",
      "        [2.9671]], grad_fn=<AddBackward0>)\n",
      "loss=0.0015198001638054848\n",
      "before backward() : W=tensor([0.9547], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1029], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9547], requires_grad=True), W.grad=tensor([-0.0109]), b=tensor([0.1029], requires_grad=True), b.grad=tensor([0.0247])\n",
      "after  step() : W=tensor([0.9548], requires_grad=True), W.grad=tensor([-0.0109]), b=tensor([0.1027], requires_grad=True), b.grad=tensor([0.0247])\n",
      "=======================531==================\n",
      "hypothesis=\n",
      "tensor([[1.0575],\n",
      "        [2.0123],\n",
      "        [2.9672]], grad_fn=<AddBackward0>)\n",
      "loss=0.0015125098871067166\n",
      "before backward() : W=tensor([0.9548], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1027], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9548], requires_grad=True), W.grad=tensor([-0.0109]), b=tensor([0.1027], requires_grad=True), b.grad=tensor([0.0247])\n",
      "after  step() : W=tensor([0.9549], requires_grad=True), W.grad=tensor([-0.0109]), b=tensor([0.1024], requires_grad=True), b.grad=tensor([0.0247])\n",
      "=======================532==================\n",
      "hypothesis=\n",
      "tensor([[1.0574],\n",
      "        [2.0123],\n",
      "        [2.9673]], grad_fn=<AddBackward0>)\n",
      "loss=0.0015052392845973372\n",
      "before backward() : W=tensor([0.9549], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1024], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9549], requires_grad=True), W.grad=tensor([-0.0108]), b=tensor([0.1024], requires_grad=True), b.grad=tensor([0.0246])\n",
      "after  step() : W=tensor([0.9550], requires_grad=True), W.grad=tensor([-0.0108]), b=tensor([0.1022], requires_grad=True), b.grad=tensor([0.0246])\n",
      "=======================533==================\n",
      "hypothesis=\n",
      "tensor([[1.0572],\n",
      "        [2.0123],\n",
      "        [2.9673]], grad_fn=<AddBackward0>)\n",
      "loss=0.0014980173436924815\n",
      "before backward() : W=tensor([0.9550], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1022], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9550], requires_grad=True), W.grad=tensor([-0.0108]), b=tensor([0.1022], requires_grad=True), b.grad=tensor([0.0246])\n",
      "after  step() : W=tensor([0.9552], requires_grad=True), W.grad=tensor([-0.0108]), b=tensor([0.1019], requires_grad=True), b.grad=tensor([0.0246])\n",
      "=======================534==================\n",
      "hypothesis=\n",
      "tensor([[1.0571],\n",
      "        [2.0123],\n",
      "        [2.9674]], grad_fn=<AddBackward0>)\n",
      "loss=0.0014908211305737495\n",
      "before backward() : W=tensor([0.9552], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1019], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9552], requires_grad=True), W.grad=tensor([-0.0108]), b=tensor([0.1019], requires_grad=True), b.grad=tensor([0.0245])\n",
      "after  step() : W=tensor([0.9553], requires_grad=True), W.grad=tensor([-0.0108]), b=tensor([0.1017], requires_grad=True), b.grad=tensor([0.0245])\n",
      "=======================535==================\n",
      "hypothesis=\n",
      "tensor([[1.0570],\n",
      "        [2.0122],\n",
      "        [2.9675]], grad_fn=<AddBackward0>)\n",
      "loss=0.001483662985265255\n",
      "before backward() : W=tensor([0.9553], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1017], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9553], requires_grad=True), W.grad=tensor([-0.0108]), b=tensor([0.1017], requires_grad=True), b.grad=tensor([0.0244])\n",
      "after  step() : W=tensor([0.9554], requires_grad=True), W.grad=tensor([-0.0108]), b=tensor([0.1015], requires_grad=True), b.grad=tensor([0.0244])\n",
      "=======================536==================\n",
      "hypothesis=\n",
      "tensor([[1.0568],\n",
      "        [2.0122],\n",
      "        [2.9676]], grad_fn=<AddBackward0>)\n",
      "loss=0.001476539415307343\n",
      "before backward() : W=tensor([0.9554], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1015], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9554], requires_grad=True), W.grad=tensor([-0.0107]), b=tensor([0.1015], requires_grad=True), b.grad=tensor([0.0244])\n",
      "after  step() : W=tensor([0.9555], requires_grad=True), W.grad=tensor([-0.0107]), b=tensor([0.1012], requires_grad=True), b.grad=tensor([0.0244])\n",
      "=======================537==================\n",
      "hypothesis=\n",
      "tensor([[1.0567],\n",
      "        [2.0122],\n",
      "        [2.9676]], grad_fn=<AddBackward0>)\n",
      "loss=0.0014694485580548644\n",
      "before backward() : W=tensor([0.9555], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1012], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9555], requires_grad=True), W.grad=tensor([-0.0107]), b=tensor([0.1012], requires_grad=True), b.grad=tensor([0.0243])\n",
      "after  step() : W=tensor([0.9556], requires_grad=True), W.grad=tensor([-0.0107]), b=tensor([0.1010], requires_grad=True), b.grad=tensor([0.0243])\n",
      "=======================538==================\n",
      "hypothesis=\n",
      "tensor([[1.0566],\n",
      "        [2.0121],\n",
      "        [2.9677]], grad_fn=<AddBackward0>)\n",
      "loss=0.0014623901806771755\n",
      "before backward() : W=tensor([0.9556], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1010], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9556], requires_grad=True), W.grad=tensor([-0.0107]), b=tensor([0.1010], requires_grad=True), b.grad=tensor([0.0243])\n",
      "after  step() : W=tensor([0.9557], requires_grad=True), W.grad=tensor([-0.0107]), b=tensor([0.1007], requires_grad=True), b.grad=tensor([0.0243])\n",
      "=======================539==================\n",
      "hypothesis=\n",
      "tensor([[1.0564],\n",
      "        [2.0121],\n",
      "        [2.9678]], grad_fn=<AddBackward0>)\n",
      "loss=0.0014553660294041038\n",
      "before backward() : W=tensor([0.9557], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1007], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9557], requires_grad=True), W.grad=tensor([-0.0107]), b=tensor([0.1007], requires_grad=True), b.grad=tensor([0.0242])\n",
      "after  step() : W=tensor([0.9558], requires_grad=True), W.grad=tensor([-0.0107]), b=tensor([0.1005], requires_grad=True), b.grad=tensor([0.0242])\n",
      "=======================540==================\n",
      "hypothesis=\n",
      "tensor([[1.0563],\n",
      "        [2.0121],\n",
      "        [2.9679]], grad_fn=<AddBackward0>)\n",
      "loss=0.001448383554816246\n",
      "before backward() : W=tensor([0.9558], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1005], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9558], requires_grad=True), W.grad=tensor([-0.0106]), b=tensor([0.1005], requires_grad=True), b.grad=tensor([0.0242])\n",
      "after  step() : W=tensor([0.9559], requires_grad=True), W.grad=tensor([-0.0106]), b=tensor([0.1002], requires_grad=True), b.grad=tensor([0.0242])\n",
      "=======================541==================\n",
      "hypothesis=\n",
      "tensor([[1.0561],\n",
      "        [2.0120],\n",
      "        [2.9680]], grad_fn=<AddBackward0>)\n",
      "loss=0.0014414237812161446\n",
      "before backward() : W=tensor([0.9559], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1002], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9559], requires_grad=True), W.grad=tensor([-0.0106]), b=tensor([0.1002], requires_grad=True), b.grad=tensor([0.0241])\n",
      "after  step() : W=tensor([0.9560], requires_grad=True), W.grad=tensor([-0.0106]), b=tensor([0.1000], requires_grad=True), b.grad=tensor([0.0241])\n",
      "=======================542==================\n",
      "hypothesis=\n",
      "tensor([[1.0560],\n",
      "        [2.0120],\n",
      "        [2.9680]], grad_fn=<AddBackward0>)\n",
      "loss=0.0014345023082569242\n",
      "before backward() : W=tensor([0.9560], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.1000], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9560], requires_grad=True), W.grad=tensor([-0.0106]), b=tensor([0.1000], requires_grad=True), b.grad=tensor([0.0240])\n",
      "after  step() : W=tensor([0.9561], requires_grad=True), W.grad=tensor([-0.0106]), b=tensor([0.0998], requires_grad=True), b.grad=tensor([0.0240])\n",
      "=======================543==================\n",
      "hypothesis=\n",
      "tensor([[1.0559],\n",
      "        [2.0120],\n",
      "        [2.9681]], grad_fn=<AddBackward0>)\n",
      "loss=0.0014276135480031371\n",
      "before backward() : W=tensor([0.9561], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0998], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9561], requires_grad=True), W.grad=tensor([-0.0105]), b=tensor([0.0998], requires_grad=True), b.grad=tensor([0.0240])\n",
      "after  step() : W=tensor([0.9562], requires_grad=True), W.grad=tensor([-0.0105]), b=tensor([0.0995], requires_grad=True), b.grad=tensor([0.0240])\n",
      "=======================544==================\n",
      "hypothesis=\n",
      "tensor([[1.0557],\n",
      "        [2.0120],\n",
      "        [2.9682]], grad_fn=<AddBackward0>)\n",
      "loss=0.0014207594795152545\n",
      "before backward() : W=tensor([0.9562], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0995], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9562], requires_grad=True), W.grad=tensor([-0.0105]), b=tensor([0.0995], requires_grad=True), b.grad=tensor([0.0239])\n",
      "after  step() : W=tensor([0.9563], requires_grad=True), W.grad=tensor([-0.0105]), b=tensor([0.0993], requires_grad=True), b.grad=tensor([0.0239])\n",
      "=======================545==================\n",
      "hypothesis=\n",
      "tensor([[1.0556],\n",
      "        [2.0119],\n",
      "        [2.9683]], grad_fn=<AddBackward0>)\n",
      "loss=0.0014139363775029778\n",
      "before backward() : W=tensor([0.9563], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0993], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9563], requires_grad=True), W.grad=tensor([-0.0105]), b=tensor([0.0993], requires_grad=True), b.grad=tensor([0.0239])\n",
      "after  step() : W=tensor([0.9564], requires_grad=True), W.grad=tensor([-0.0105]), b=tensor([0.0990], requires_grad=True), b.grad=tensor([0.0239])\n",
      "=======================546==================\n",
      "hypothesis=\n",
      "tensor([[1.0555],\n",
      "        [2.0119],\n",
      "        [2.9683]], grad_fn=<AddBackward0>)\n",
      "loss=0.0014071475015953183\n",
      "before backward() : W=tensor([0.9564], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0990], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9564], requires_grad=True), W.grad=tensor([-0.0105]), b=tensor([0.0990], requires_grad=True), b.grad=tensor([0.0238])\n",
      "after  step() : W=tensor([0.9565], requires_grad=True), W.grad=tensor([-0.0105]), b=tensor([0.0988], requires_grad=True), b.grad=tensor([0.0238])\n",
      "=======================547==================\n",
      "hypothesis=\n",
      "tensor([[1.0553],\n",
      "        [2.0119],\n",
      "        [2.9684]], grad_fn=<AddBackward0>)\n",
      "loss=0.001400390057824552\n",
      "before backward() : W=tensor([0.9565], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0988], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9565], requires_grad=True), W.grad=tensor([-0.0104]), b=tensor([0.0988], requires_grad=True), b.grad=tensor([0.0238])\n",
      "after  step() : W=tensor([0.9566], requires_grad=True), W.grad=tensor([-0.0104]), b=tensor([0.0986], requires_grad=True), b.grad=tensor([0.0238])\n",
      "=======================548==================\n",
      "hypothesis=\n",
      "tensor([[1.0552],\n",
      "        [2.0118],\n",
      "        [2.9685]], grad_fn=<AddBackward0>)\n",
      "loss=0.0013936619507148862\n",
      "before backward() : W=tensor([0.9566], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0986], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9566], requires_grad=True), W.grad=tensor([-0.0104]), b=tensor([0.0986], requires_grad=True), b.grad=tensor([0.0237])\n",
      "after  step() : W=tensor([0.9567], requires_grad=True), W.grad=tensor([-0.0104]), b=tensor([0.0983], requires_grad=True), b.grad=tensor([0.0237])\n",
      "=======================549==================\n",
      "hypothesis=\n",
      "tensor([[1.0551],\n",
      "        [2.0118],\n",
      "        [2.9686]], grad_fn=<AddBackward0>)\n",
      "loss=0.0013869699323549867\n",
      "before backward() : W=tensor([0.9567], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0983], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9567], requires_grad=True), W.grad=tensor([-0.0104]), b=tensor([0.0983], requires_grad=True), b.grad=tensor([0.0236])\n",
      "after  step() : W=tensor([0.9568], requires_grad=True), W.grad=tensor([-0.0104]), b=tensor([0.0981], requires_grad=True), b.grad=tensor([0.0236])\n",
      "=======================550==================\n",
      "hypothesis=\n",
      "tensor([[1.0549],\n",
      "        [2.0118],\n",
      "        [2.9686]], grad_fn=<AddBackward0>)\n",
      "loss=0.0013803079491481185\n",
      "before backward() : W=tensor([0.9568], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0981], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9568], requires_grad=True), W.grad=tensor([-0.0104]), b=tensor([0.0981], requires_grad=True), b.grad=tensor([0.0236])\n",
      "after  step() : W=tensor([0.9570], requires_grad=True), W.grad=tensor([-0.0104]), b=tensor([0.0979], requires_grad=True), b.grad=tensor([0.0236])\n",
      "=======================551==================\n",
      "hypothesis=\n",
      "tensor([[1.0548],\n",
      "        [2.0118],\n",
      "        [2.9687]], grad_fn=<AddBackward0>)\n",
      "loss=0.0013736780965700746\n",
      "before backward() : W=tensor([0.9570], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0979], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9570], requires_grad=True), W.grad=tensor([-0.0103]), b=tensor([0.0979], requires_grad=True), b.grad=tensor([0.0235])\n",
      "after  step() : W=tensor([0.9571], requires_grad=True), W.grad=tensor([-0.0103]), b=tensor([0.0976], requires_grad=True), b.grad=tensor([0.0235])\n",
      "=======================552==================\n",
      "hypothesis=\n",
      "tensor([[1.0547],\n",
      "        [2.0117],\n",
      "        [2.9688]], grad_fn=<AddBackward0>)\n",
      "loss=0.0013670859625563025\n",
      "before backward() : W=tensor([0.9571], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0976], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9571], requires_grad=True), W.grad=tensor([-0.0103]), b=tensor([0.0976], requires_grad=True), b.grad=tensor([0.0235])\n",
      "after  step() : W=tensor([0.9572], requires_grad=True), W.grad=tensor([-0.0103]), b=tensor([0.0974], requires_grad=True), b.grad=tensor([0.0235])\n",
      "=======================553==================\n",
      "hypothesis=\n",
      "tensor([[1.0545],\n",
      "        [2.0117],\n",
      "        [2.9689]], grad_fn=<AddBackward0>)\n",
      "loss=0.0013605226995423436\n",
      "before backward() : W=tensor([0.9572], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0974], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9572], requires_grad=True), W.grad=tensor([-0.0103]), b=tensor([0.0974], requires_grad=True), b.grad=tensor([0.0234])\n",
      "after  step() : W=tensor([0.9573], requires_grad=True), W.grad=tensor([-0.0103]), b=tensor([0.0972], requires_grad=True), b.grad=tensor([0.0234])\n",
      "=======================554==================\n",
      "hypothesis=\n",
      "tensor([[1.0544],\n",
      "        [2.0117],\n",
      "        [2.9689]], grad_fn=<AddBackward0>)\n",
      "loss=0.001353988773189485\n",
      "before backward() : W=tensor([0.9573], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0972], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9573], requires_grad=True), W.grad=tensor([-0.0103]), b=tensor([0.0972], requires_grad=True), b.grad=tensor([0.0234])\n",
      "after  step() : W=tensor([0.9574], requires_grad=True), W.grad=tensor([-0.0103]), b=tensor([0.0969], requires_grad=True), b.grad=tensor([0.0234])\n",
      "=======================555==================\n",
      "hypothesis=\n",
      "tensor([[1.0543],\n",
      "        [2.0116],\n",
      "        [2.9690]], grad_fn=<AddBackward0>)\n",
      "loss=0.001347483485005796\n",
      "before backward() : W=tensor([0.9574], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0969], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9574], requires_grad=True), W.grad=tensor([-0.0102]), b=tensor([0.0969], requires_grad=True), b.grad=tensor([0.0233])\n",
      "after  step() : W=tensor([0.9575], requires_grad=True), W.grad=tensor([-0.0102]), b=tensor([0.0967], requires_grad=True), b.grad=tensor([0.0233])\n",
      "=======================556==================\n",
      "hypothesis=\n",
      "tensor([[1.0542],\n",
      "        [2.0116],\n",
      "        [2.9691]], grad_fn=<AddBackward0>)\n",
      "loss=0.0013410173123702407\n",
      "before backward() : W=tensor([0.9575], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0967], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9575], requires_grad=True), W.grad=tensor([-0.0102]), b=tensor([0.0967], requires_grad=True), b.grad=tensor([0.0232])\n",
      "after  step() : W=tensor([0.9576], requires_grad=True), W.grad=tensor([-0.0102]), b=tensor([0.0965], requires_grad=True), b.grad=tensor([0.0232])\n",
      "=======================557==================\n",
      "hypothesis=\n",
      "tensor([[1.0540],\n",
      "        [2.0116],\n",
      "        [2.9692]], grad_fn=<AddBackward0>)\n",
      "loss=0.0013345768675208092\n",
      "before backward() : W=tensor([0.9576], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0965], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9576], requires_grad=True), W.grad=tensor([-0.0102]), b=tensor([0.0965], requires_grad=True), b.grad=tensor([0.0232])\n",
      "after  step() : W=tensor([0.9577], requires_grad=True), W.grad=tensor([-0.0102]), b=tensor([0.0962], requires_grad=True), b.grad=tensor([0.0232])\n",
      "=======================558==================\n",
      "hypothesis=\n",
      "tensor([[1.0539],\n",
      "        [2.0116],\n",
      "        [2.9692]], grad_fn=<AddBackward0>)\n",
      "loss=0.0013281693682074547\n",
      "before backward() : W=tensor([0.9577], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0962], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9577], requires_grad=True), W.grad=tensor([-0.0102]), b=tensor([0.0962], requires_grad=True), b.grad=tensor([0.0231])\n",
      "after  step() : W=tensor([0.9578], requires_grad=True), W.grad=tensor([-0.0102]), b=tensor([0.0960], requires_grad=True), b.grad=tensor([0.0231])\n",
      "=======================559==================\n",
      "hypothesis=\n",
      "tensor([[1.0538],\n",
      "        [2.0115],\n",
      "        [2.9693]], grad_fn=<AddBackward0>)\n",
      "loss=0.0013217856176197529\n",
      "before backward() : W=tensor([0.9578], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0960], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9578], requires_grad=True), W.grad=tensor([-0.0102]), b=tensor([0.0960], requires_grad=True), b.grad=tensor([0.0231])\n",
      "after  step() : W=tensor([0.9579], requires_grad=True), W.grad=tensor([-0.0102]), b=tensor([0.0958], requires_grad=True), b.grad=tensor([0.0231])\n",
      "=======================560==================\n",
      "hypothesis=\n",
      "tensor([[1.0536],\n",
      "        [2.0115],\n",
      "        [2.9694]], grad_fn=<AddBackward0>)\n",
      "loss=0.001315441564656794\n",
      "before backward() : W=tensor([0.9579], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0958], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9579], requires_grad=True), W.grad=tensor([-0.0101]), b=tensor([0.0958], requires_grad=True), b.grad=tensor([0.0230])\n",
      "after  step() : W=tensor([0.9580], requires_grad=True), W.grad=tensor([-0.0101]), b=tensor([0.0955], requires_grad=True), b.grad=tensor([0.0230])\n",
      "=======================561==================\n",
      "hypothesis=\n",
      "tensor([[1.0535],\n",
      "        [2.0115],\n",
      "        [2.9695]], grad_fn=<AddBackward0>)\n",
      "loss=0.0013091258006170392\n",
      "before backward() : W=tensor([0.9580], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0955], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9580], requires_grad=True), W.grad=tensor([-0.0101]), b=tensor([0.0955], requires_grad=True), b.grad=tensor([0.0230])\n",
      "after  step() : W=tensor([0.9581], requires_grad=True), W.grad=tensor([-0.0101]), b=tensor([0.0953], requires_grad=True), b.grad=tensor([0.0230])\n",
      "=======================562==================\n",
      "hypothesis=\n",
      "tensor([[1.0534],\n",
      "        [2.0115],\n",
      "        [2.9695]], grad_fn=<AddBackward0>)\n",
      "loss=0.0013028383255004883\n",
      "before backward() : W=tensor([0.9581], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0953], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9581], requires_grad=True), W.grad=tensor([-0.0101]), b=tensor([0.0953], requires_grad=True), b.grad=tensor([0.0229])\n",
      "after  step() : W=tensor([0.9582], requires_grad=True), W.grad=tensor([-0.0101]), b=tensor([0.0951], requires_grad=True), b.grad=tensor([0.0229])\n",
      "=======================563==================\n",
      "hypothesis=\n",
      "tensor([[1.0532],\n",
      "        [2.0114],\n",
      "        [2.9696]], grad_fn=<AddBackward0>)\n",
      "loss=0.0012965812347829342\n",
      "before backward() : W=tensor([0.9582], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0951], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9582], requires_grad=True), W.grad=tensor([-0.0101]), b=tensor([0.0951], requires_grad=True), b.grad=tensor([0.0229])\n",
      "after  step() : W=tensor([0.9583], requires_grad=True), W.grad=tensor([-0.0101]), b=tensor([0.0948], requires_grad=True), b.grad=tensor([0.0229])\n",
      "=======================564==================\n",
      "hypothesis=\n",
      "tensor([[1.0531],\n",
      "        [2.0114],\n",
      "        [2.9697]], grad_fn=<AddBackward0>)\n",
      "loss=0.0012903560418635607\n",
      "before backward() : W=tensor([0.9583], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0948], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9583], requires_grad=True), W.grad=tensor([-0.0100]), b=tensor([0.0948], requires_grad=True), b.grad=tensor([0.0228])\n",
      "after  step() : W=tensor([0.9584], requires_grad=True), W.grad=tensor([-0.0100]), b=tensor([0.0946], requires_grad=True), b.grad=tensor([0.0228])\n",
      "=======================565==================\n",
      "hypothesis=\n",
      "tensor([[1.0530],\n",
      "        [2.0114],\n",
      "        [2.9698]], grad_fn=<AddBackward0>)\n",
      "loss=0.0012841590214520693\n",
      "before backward() : W=tensor([0.9584], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0946], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9584], requires_grad=True), W.grad=tensor([-0.0100]), b=tensor([0.0946], requires_grad=True), b.grad=tensor([0.0227])\n",
      "after  step() : W=tensor([0.9585], requires_grad=True), W.grad=tensor([-0.0100]), b=tensor([0.0944], requires_grad=True), b.grad=tensor([0.0227])\n",
      "=======================566==================\n",
      "hypothesis=\n",
      "tensor([[1.0529],\n",
      "        [2.0113],\n",
      "        [2.9698]], grad_fn=<AddBackward0>)\n",
      "loss=0.0012779919197782874\n",
      "before backward() : W=tensor([0.9585], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0944], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9585], requires_grad=True), W.grad=tensor([-0.0100]), b=tensor([0.0944], requires_grad=True), b.grad=tensor([0.0227])\n",
      "after  step() : W=tensor([0.9586], requires_grad=True), W.grad=tensor([-0.0100]), b=tensor([0.0942], requires_grad=True), b.grad=tensor([0.0227])\n",
      "=======================567==================\n",
      "hypothesis=\n",
      "tensor([[1.0527],\n",
      "        [2.0113],\n",
      "        [2.9699]], grad_fn=<AddBackward0>)\n",
      "loss=0.0012718522921204567\n",
      "before backward() : W=tensor([0.9586], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0942], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9586], requires_grad=True), W.grad=tensor([-0.0100]), b=tensor([0.0942], requires_grad=True), b.grad=tensor([0.0226])\n",
      "after  step() : W=tensor([0.9587], requires_grad=True), W.grad=tensor([-0.0100]), b=tensor([0.0939], requires_grad=True), b.grad=tensor([0.0226])\n",
      "=======================568==================\n",
      "hypothesis=\n",
      "tensor([[1.0526],\n",
      "        [2.0113],\n",
      "        [2.9700]], grad_fn=<AddBackward0>)\n",
      "loss=0.001265747589059174\n",
      "before backward() : W=tensor([0.9587], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0939], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9587], requires_grad=True), W.grad=tensor([-0.0099]), b=tensor([0.0939], requires_grad=True), b.grad=tensor([0.0226])\n",
      "after  step() : W=tensor([0.9588], requires_grad=True), W.grad=tensor([-0.0099]), b=tensor([0.0937], requires_grad=True), b.grad=tensor([0.0226])\n",
      "=======================569==================\n",
      "hypothesis=\n",
      "tensor([[1.0525],\n",
      "        [2.0113],\n",
      "        [2.9700]], grad_fn=<AddBackward0>)\n",
      "loss=0.001259668031707406\n",
      "before backward() : W=tensor([0.9588], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0937], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9588], requires_grad=True), W.grad=tensor([-0.0099]), b=tensor([0.0937], requires_grad=True), b.grad=tensor([0.0225])\n",
      "after  step() : W=tensor([0.9589], requires_grad=True), W.grad=tensor([-0.0099]), b=tensor([0.0935], requires_grad=True), b.grad=tensor([0.0225])\n",
      "=======================570==================\n",
      "hypothesis=\n",
      "tensor([[1.0524],\n",
      "        [2.0112],\n",
      "        [2.9701]], grad_fn=<AddBackward0>)\n",
      "loss=0.0012536206049844623\n",
      "before backward() : W=tensor([0.9589], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0935], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9589], requires_grad=True), W.grad=tensor([-0.0099]), b=tensor([0.0935], requires_grad=True), b.grad=tensor([0.0225])\n",
      "after  step() : W=tensor([0.9590], requires_grad=True), W.grad=tensor([-0.0099]), b=tensor([0.0933], requires_grad=True), b.grad=tensor([0.0225])\n",
      "=======================571==================\n",
      "hypothesis=\n",
      "tensor([[1.0522],\n",
      "        [2.0112],\n",
      "        [2.9702]], grad_fn=<AddBackward0>)\n",
      "loss=0.0012475973926484585\n",
      "before backward() : W=tensor([0.9590], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0933], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9590], requires_grad=True), W.grad=tensor([-0.0099]), b=tensor([0.0933], requires_grad=True), b.grad=tensor([0.0224])\n",
      "after  step() : W=tensor([0.9591], requires_grad=True), W.grad=tensor([-0.0099]), b=tensor([0.0930], requires_grad=True), b.grad=tensor([0.0224])\n",
      "=======================572==================\n",
      "hypothesis=\n",
      "tensor([[1.0521],\n",
      "        [2.0112],\n",
      "        [2.9703]], grad_fn=<AddBackward0>)\n",
      "loss=0.0012416058452799916\n",
      "before backward() : W=tensor([0.9591], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0930], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9591], requires_grad=True), W.grad=tensor([-0.0098]), b=tensor([0.0930], requires_grad=True), b.grad=tensor([0.0224])\n",
      "after  step() : W=tensor([0.9592], requires_grad=True), W.grad=tensor([-0.0098]), b=tensor([0.0928], requires_grad=True), b.grad=tensor([0.0224])\n",
      "=======================573==================\n",
      "hypothesis=\n",
      "tensor([[1.0520],\n",
      "        [2.0112],\n",
      "        [2.9703]], grad_fn=<AddBackward0>)\n",
      "loss=0.0012356474762782454\n",
      "before backward() : W=tensor([0.9592], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0928], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9592], requires_grad=True), W.grad=tensor([-0.0098]), b=tensor([0.0928], requires_grad=True), b.grad=tensor([0.0223])\n",
      "after  step() : W=tensor([0.9593], requires_grad=True), W.grad=tensor([-0.0098]), b=tensor([0.0926], requires_grad=True), b.grad=tensor([0.0223])\n",
      "=======================574==================\n",
      "hypothesis=\n",
      "tensor([[1.0519],\n",
      "        [2.0111],\n",
      "        [2.9704]], grad_fn=<AddBackward0>)\n",
      "loss=0.0012297164648771286\n",
      "before backward() : W=tensor([0.9593], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0926], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9593], requires_grad=True), W.grad=tensor([-0.0098]), b=tensor([0.0926], requires_grad=True), b.grad=tensor([0.0223])\n",
      "after  step() : W=tensor([0.9594], requires_grad=True), W.grad=tensor([-0.0098]), b=tensor([0.0924], requires_grad=True), b.grad=tensor([0.0223])\n",
      "=======================575==================\n",
      "hypothesis=\n",
      "tensor([[1.0517],\n",
      "        [2.0111],\n",
      "        [2.9705]], grad_fn=<AddBackward0>)\n",
      "loss=0.0012238080380484462\n",
      "before backward() : W=tensor([0.9594], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0924], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9594], requires_grad=True), W.grad=tensor([-0.0098]), b=tensor([0.0924], requires_grad=True), b.grad=tensor([0.0222])\n",
      "after  step() : W=tensor([0.9595], requires_grad=True), W.grad=tensor([-0.0098]), b=tensor([0.0921], requires_grad=True), b.grad=tensor([0.0222])\n",
      "=======================576==================\n",
      "hypothesis=\n",
      "tensor([[1.0516],\n",
      "        [2.0111],\n",
      "        [2.9705]], grad_fn=<AddBackward0>)\n",
      "loss=0.0012179305776953697\n",
      "before backward() : W=tensor([0.9595], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0921], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9595], requires_grad=True), W.grad=tensor([-0.0097]), b=tensor([0.0921], requires_grad=True), b.grad=tensor([0.0222])\n",
      "after  step() : W=tensor([0.9596], requires_grad=True), W.grad=tensor([-0.0097]), b=tensor([0.0919], requires_grad=True), b.grad=tensor([0.0222])\n",
      "=======================577==================\n",
      "hypothesis=\n",
      "tensor([[1.0515],\n",
      "        [2.0110],\n",
      "        [2.9706]], grad_fn=<AddBackward0>)\n",
      "loss=0.0012120803585276008\n",
      "before backward() : W=tensor([0.9596], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0919], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9596], requires_grad=True), W.grad=tensor([-0.0097]), b=tensor([0.0919], requires_grad=True), b.grad=tensor([0.0221])\n",
      "after  step() : W=tensor([0.9597], requires_grad=True), W.grad=tensor([-0.0097]), b=tensor([0.0917], requires_grad=True), b.grad=tensor([0.0221])\n",
      "=======================578==================\n",
      "hypothesis=\n",
      "tensor([[1.0514],\n",
      "        [2.0110],\n",
      "        [2.9707]], grad_fn=<AddBackward0>)\n",
      "loss=0.0012062628520652652\n",
      "before backward() : W=tensor([0.9597], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0917], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9597], requires_grad=True), W.grad=tensor([-0.0097]), b=tensor([0.0917], requires_grad=True), b.grad=tensor([0.0220])\n",
      "after  step() : W=tensor([0.9598], requires_grad=True), W.grad=tensor([-0.0097]), b=tensor([0.0915], requires_grad=True), b.grad=tensor([0.0220])\n",
      "=======================579==================\n",
      "hypothesis=\n",
      "tensor([[1.0512],\n",
      "        [2.0110],\n",
      "        [2.9708]], grad_fn=<AddBackward0>)\n",
      "loss=0.0012004667660221457\n",
      "before backward() : W=tensor([0.9598], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0915], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9598], requires_grad=True), W.grad=tensor([-0.0097]), b=tensor([0.0915], requires_grad=True), b.grad=tensor([0.0220])\n",
      "after  step() : W=tensor([0.9599], requires_grad=True), W.grad=tensor([-0.0097]), b=tensor([0.0913], requires_grad=True), b.grad=tensor([0.0220])\n",
      "=======================580==================\n",
      "hypothesis=\n",
      "tensor([[1.0511],\n",
      "        [2.0110],\n",
      "        [2.9708]], grad_fn=<AddBackward0>)\n",
      "loss=0.001194702577777207\n",
      "before backward() : W=tensor([0.9599], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0913], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9599], requires_grad=True), W.grad=tensor([-0.0096]), b=tensor([0.0913], requires_grad=True), b.grad=tensor([0.0219])\n",
      "after  step() : W=tensor([0.9600], requires_grad=True), W.grad=tensor([-0.0096]), b=tensor([0.0910], requires_grad=True), b.grad=tensor([0.0219])\n",
      "=======================581==================\n",
      "hypothesis=\n",
      "tensor([[1.0510],\n",
      "        [2.0109],\n",
      "        [2.9709]], grad_fn=<AddBackward0>)\n",
      "loss=0.001188964699395001\n",
      "before backward() : W=tensor([0.9600], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0910], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9600], requires_grad=True), W.grad=tensor([-0.0096]), b=tensor([0.0910], requires_grad=True), b.grad=tensor([0.0219])\n",
      "after  step() : W=tensor([0.9600], requires_grad=True), W.grad=tensor([-0.0096]), b=tensor([0.0908], requires_grad=True), b.grad=tensor([0.0219])\n",
      "=======================582==================\n",
      "hypothesis=\n",
      "tensor([[1.0509],\n",
      "        [2.0109],\n",
      "        [2.9710]], grad_fn=<AddBackward0>)\n",
      "loss=0.0011832588352262974\n",
      "before backward() : W=tensor([0.9600], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0908], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9600], requires_grad=True), W.grad=tensor([-0.0096]), b=tensor([0.0908], requires_grad=True), b.grad=tensor([0.0218])\n",
      "after  step() : W=tensor([0.9601], requires_grad=True), W.grad=tensor([-0.0096]), b=tensor([0.0906], requires_grad=True), b.grad=tensor([0.0218])\n",
      "=======================583==================\n",
      "hypothesis=\n",
      "tensor([[1.0507],\n",
      "        [2.0109],\n",
      "        [2.9710]], grad_fn=<AddBackward0>)\n",
      "loss=0.0011775748571380973\n",
      "before backward() : W=tensor([0.9601], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0906], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9601], requires_grad=True), W.grad=tensor([-0.0096]), b=tensor([0.0906], requires_grad=True), b.grad=tensor([0.0218])\n",
      "after  step() : W=tensor([0.9602], requires_grad=True), W.grad=tensor([-0.0096]), b=tensor([0.0904], requires_grad=True), b.grad=tensor([0.0218])\n",
      "=======================584==================\n",
      "hypothesis=\n",
      "tensor([[1.0506],\n",
      "        [2.0109],\n",
      "        [2.9711]], grad_fn=<AddBackward0>)\n",
      "loss=0.0011719213798642159\n",
      "before backward() : W=tensor([0.9602], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0904], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9602], requires_grad=True), W.grad=tensor([-0.0096]), b=tensor([0.0904], requires_grad=True), b.grad=tensor([0.0217])\n",
      "after  step() : W=tensor([0.9603], requires_grad=True), W.grad=tensor([-0.0096]), b=tensor([0.0902], requires_grad=True), b.grad=tensor([0.0217])\n",
      "=======================585==================\n",
      "hypothesis=\n",
      "tensor([[1.0505],\n",
      "        [2.0108],\n",
      "        [2.9712]], grad_fn=<AddBackward0>)\n",
      "loss=0.0011662897886708379\n",
      "before backward() : W=tensor([0.9603], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0902], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9603], requires_grad=True), W.grad=tensor([-0.0095]), b=tensor([0.0902], requires_grad=True), b.grad=tensor([0.0217])\n",
      "after  step() : W=tensor([0.9604], requires_grad=True), W.grad=tensor([-0.0095]), b=tensor([0.0899], requires_grad=True), b.grad=tensor([0.0217])\n",
      "=======================586==================\n",
      "hypothesis=\n",
      "tensor([[1.0504],\n",
      "        [2.0108],\n",
      "        [2.9712]], grad_fn=<AddBackward0>)\n",
      "loss=0.001160689746029675\n",
      "before backward() : W=tensor([0.9604], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0899], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9604], requires_grad=True), W.grad=tensor([-0.0095]), b=tensor([0.0899], requires_grad=True), b.grad=tensor([0.0216])\n",
      "after  step() : W=tensor([0.9605], requires_grad=True), W.grad=tensor([-0.0095]), b=tensor([0.0897], requires_grad=True), b.grad=tensor([0.0216])\n",
      "=======================587==================\n",
      "hypothesis=\n",
      "tensor([[1.0503],\n",
      "        [2.0108],\n",
      "        [2.9713]], grad_fn=<AddBackward0>)\n",
      "loss=0.0011551156640052795\n",
      "before backward() : W=tensor([0.9605], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0897], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9605], requires_grad=True), W.grad=tensor([-0.0095]), b=tensor([0.0897], requires_grad=True), b.grad=tensor([0.0216])\n",
      "after  step() : W=tensor([0.9606], requires_grad=True), W.grad=tensor([-0.0095]), b=tensor([0.0895], requires_grad=True), b.grad=tensor([0.0216])\n",
      "=======================588==================\n",
      "hypothesis=\n",
      "tensor([[1.0501],\n",
      "        [2.0108],\n",
      "        [2.9714]], grad_fn=<AddBackward0>)\n",
      "loss=0.0011495713843032718\n",
      "before backward() : W=tensor([0.9606], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0895], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9606], requires_grad=True), W.grad=tensor([-0.0095]), b=tensor([0.0895], requires_grad=True), b.grad=tensor([0.0215])\n",
      "after  step() : W=tensor([0.9607], requires_grad=True), W.grad=tensor([-0.0095]), b=tensor([0.0893], requires_grad=True), b.grad=tensor([0.0215])\n",
      "=======================589==================\n",
      "hypothesis=\n",
      "tensor([[1.0500],\n",
      "        [2.0107],\n",
      "        [2.9714]], grad_fn=<AddBackward0>)\n",
      "loss=0.0011440491070970893\n",
      "before backward() : W=tensor([0.9607], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0893], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9607], requires_grad=True), W.grad=tensor([-0.0094]), b=tensor([0.0893], requires_grad=True), b.grad=tensor([0.0215])\n",
      "after  step() : W=tensor([0.9608], requires_grad=True), W.grad=tensor([-0.0094]), b=tensor([0.0891], requires_grad=True), b.grad=tensor([0.0215])\n",
      "=======================590==================\n",
      "hypothesis=\n",
      "tensor([[1.0499],\n",
      "        [2.0107],\n",
      "        [2.9715]], grad_fn=<AddBackward0>)\n",
      "loss=0.001138557679951191\n",
      "before backward() : W=tensor([0.9608], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0891], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9608], requires_grad=True), W.grad=tensor([-0.0094]), b=tensor([0.0891], requires_grad=True), b.grad=tensor([0.0214])\n",
      "after  step() : W=tensor([0.9609], requires_grad=True), W.grad=tensor([-0.0094]), b=tensor([0.0889], requires_grad=True), b.grad=tensor([0.0214])\n",
      "=======================591==================\n",
      "hypothesis=\n",
      "tensor([[1.0498],\n",
      "        [2.0107],\n",
      "        [2.9716]], grad_fn=<AddBackward0>)\n",
      "loss=0.0011330873239785433\n",
      "before backward() : W=tensor([0.9609], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0889], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9609], requires_grad=True), W.grad=tensor([-0.0094]), b=tensor([0.0889], requires_grad=True), b.grad=tensor([0.0214])\n",
      "after  step() : W=tensor([0.9610], requires_grad=True), W.grad=tensor([-0.0094]), b=tensor([0.0887], requires_grad=True), b.grad=tensor([0.0214])\n",
      "=======================592==================\n",
      "hypothesis=\n",
      "tensor([[1.0497],\n",
      "        [2.0107],\n",
      "        [2.9717]], grad_fn=<AddBackward0>)\n",
      "loss=0.0011276532895863056\n",
      "before backward() : W=tensor([0.9610], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0887], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9610], requires_grad=True), W.grad=tensor([-0.0094]), b=tensor([0.0887], requires_grad=True), b.grad=tensor([0.0213])\n",
      "after  step() : W=tensor([0.9611], requires_grad=True), W.grad=tensor([-0.0094]), b=tensor([0.0884], requires_grad=True), b.grad=tensor([0.0213])\n",
      "=======================593==================\n",
      "hypothesis=\n",
      "tensor([[1.0495],\n",
      "        [2.0106],\n",
      "        [2.9717]], grad_fn=<AddBackward0>)\n",
      "loss=0.001122233341448009\n",
      "before backward() : W=tensor([0.9611], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0884], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9611], requires_grad=True), W.grad=tensor([-0.0094]), b=tensor([0.0884], requires_grad=True), b.grad=tensor([0.0213])\n",
      "after  step() : W=tensor([0.9612], requires_grad=True), W.grad=tensor([-0.0094]), b=tensor([0.0882], requires_grad=True), b.grad=tensor([0.0213])\n",
      "=======================594==================\n",
      "hypothesis=\n",
      "tensor([[1.0494],\n",
      "        [2.0106],\n",
      "        [2.9718]], grad_fn=<AddBackward0>)\n",
      "loss=0.0011168451746925712\n",
      "before backward() : W=tensor([0.9612], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0882], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9612], requires_grad=True), W.grad=tensor([-0.0093]), b=tensor([0.0882], requires_grad=True), b.grad=tensor([0.0212])\n",
      "after  step() : W=tensor([0.9613], requires_grad=True), W.grad=tensor([-0.0093]), b=tensor([0.0880], requires_grad=True), b.grad=tensor([0.0212])\n",
      "=======================595==================\n",
      "hypothesis=\n",
      "tensor([[1.0493],\n",
      "        [2.0106],\n",
      "        [2.9719]], grad_fn=<AddBackward0>)\n",
      "loss=0.001111482153646648\n",
      "before backward() : W=tensor([0.9613], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0880], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9613], requires_grad=True), W.grad=tensor([-0.0093]), b=tensor([0.0880], requires_grad=True), b.grad=tensor([0.0212])\n",
      "after  step() : W=tensor([0.9614], requires_grad=True), W.grad=tensor([-0.0093]), b=tensor([0.0878], requires_grad=True), b.grad=tensor([0.0212])\n",
      "=======================596==================\n",
      "hypothesis=\n",
      "tensor([[1.0492],\n",
      "        [2.0106],\n",
      "        [2.9719]], grad_fn=<AddBackward0>)\n",
      "loss=0.0011061435798183084\n",
      "before backward() : W=tensor([0.9614], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0878], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9614], requires_grad=True), W.grad=tensor([-0.0093]), b=tensor([0.0878], requires_grad=True), b.grad=tensor([0.0211])\n",
      "after  step() : W=tensor([0.9615], requires_grad=True), W.grad=tensor([-0.0093]), b=tensor([0.0876], requires_grad=True), b.grad=tensor([0.0211])\n",
      "=======================597==================\n",
      "hypothesis=\n",
      "tensor([[1.0491],\n",
      "        [2.0105],\n",
      "        [2.9720]], grad_fn=<AddBackward0>)\n",
      "loss=0.0011008321307599545\n",
      "before backward() : W=tensor([0.9615], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0876], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9615], requires_grad=True), W.grad=tensor([-0.0093]), b=tensor([0.0876], requires_grad=True), b.grad=tensor([0.0211])\n",
      "after  step() : W=tensor([0.9616], requires_grad=True), W.grad=tensor([-0.0093]), b=tensor([0.0874], requires_grad=True), b.grad=tensor([0.0211])\n",
      "=======================598==================\n",
      "hypothesis=\n",
      "tensor([[1.0489],\n",
      "        [2.0105],\n",
      "        [2.9721]], grad_fn=<AddBackward0>)\n",
      "loss=0.0010955437319353223\n",
      "before backward() : W=tensor([0.9616], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0874], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9616], requires_grad=True), W.grad=tensor([-0.0092]), b=tensor([0.0874], requires_grad=True), b.grad=tensor([0.0210])\n",
      "after  step() : W=tensor([0.9616], requires_grad=True), W.grad=tensor([-0.0092]), b=tensor([0.0872], requires_grad=True), b.grad=tensor([0.0210])\n",
      "=======================599==================\n",
      "hypothesis=\n",
      "tensor([[1.0488],\n",
      "        [2.0105],\n",
      "        [2.9721]], grad_fn=<AddBackward0>)\n",
      "loss=0.0010902815265581012\n",
      "before backward() : W=tensor([0.9616], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0872], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9616], requires_grad=True), W.grad=tensor([-0.0092]), b=tensor([0.0872], requires_grad=True), b.grad=tensor([0.0210])\n",
      "after  step() : W=tensor([0.9617], requires_grad=True), W.grad=tensor([-0.0092]), b=tensor([0.0870], requires_grad=True), b.grad=tensor([0.0210])\n",
      "=======================600==================\n",
      "hypothesis=\n",
      "tensor([[1.0487],\n",
      "        [2.0105],\n",
      "        [2.9722]], grad_fn=<AddBackward0>)\n",
      "loss=0.001085049589164555\n",
      "before backward() : W=tensor([0.9617], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0870], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9617], requires_grad=True), W.grad=tensor([-0.0092]), b=tensor([0.0870], requires_grad=True), b.grad=tensor([0.0209])\n",
      "after  step() : W=tensor([0.9618], requires_grad=True), W.grad=tensor([-0.0092]), b=tensor([0.0868], requires_grad=True), b.grad=tensor([0.0209])\n",
      "=======================601==================\n",
      "hypothesis=\n",
      "tensor([[1.0486],\n",
      "        [2.0104],\n",
      "        [2.9723]], grad_fn=<AddBackward0>)\n",
      "loss=0.0010798368602991104\n",
      "before backward() : W=tensor([0.9618], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0868], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9618], requires_grad=True), W.grad=tensor([-0.0092]), b=tensor([0.0868], requires_grad=True), b.grad=tensor([0.0209])\n",
      "after  step() : W=tensor([0.9619], requires_grad=True), W.grad=tensor([-0.0092]), b=tensor([0.0866], requires_grad=True), b.grad=tensor([0.0209])\n",
      "=======================602==================\n",
      "hypothesis=\n",
      "tensor([[1.0485],\n",
      "        [2.0104],\n",
      "        [2.9723]], grad_fn=<AddBackward0>)\n",
      "loss=0.0010746543994173408\n",
      "before backward() : W=tensor([0.9619], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0866], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9619], requires_grad=True), W.grad=tensor([-0.0092]), b=tensor([0.0866], requires_grad=True), b.grad=tensor([0.0208])\n",
      "after  step() : W=tensor([0.9620], requires_grad=True), W.grad=tensor([-0.0092]), b=tensor([0.0863], requires_grad=True), b.grad=tensor([0.0208])\n",
      "=======================603==================\n",
      "hypothesis=\n",
      "tensor([[1.0484],\n",
      "        [2.0104],\n",
      "        [2.9724]], grad_fn=<AddBackward0>)\n",
      "loss=0.0010694958036765456\n",
      "before backward() : W=tensor([0.9620], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0863], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9620], requires_grad=True), W.grad=tensor([-0.0091]), b=tensor([0.0863], requires_grad=True), b.grad=tensor([0.0208])\n",
      "after  step() : W=tensor([0.9621], requires_grad=True), W.grad=tensor([-0.0091]), b=tensor([0.0861], requires_grad=True), b.grad=tensor([0.0208])\n",
      "=======================604==================\n",
      "hypothesis=\n",
      "tensor([[1.0482],\n",
      "        [2.0104],\n",
      "        [2.9725]], grad_fn=<AddBackward0>)\n",
      "loss=0.0010643546702340245\n",
      "before backward() : W=tensor([0.9621], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0861], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9621], requires_grad=True), W.grad=tensor([-0.0091]), b=tensor([0.0861], requires_grad=True), b.grad=tensor([0.0207])\n",
      "after  step() : W=tensor([0.9622], requires_grad=True), W.grad=tensor([-0.0091]), b=tensor([0.0859], requires_grad=True), b.grad=tensor([0.0207])\n",
      "=======================605==================\n",
      "hypothesis=\n",
      "tensor([[1.0481],\n",
      "        [2.0103],\n",
      "        [2.9725]], grad_fn=<AddBackward0>)\n",
      "loss=0.0010592449689283967\n",
      "before backward() : W=tensor([0.9622], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0859], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9622], requires_grad=True), W.grad=tensor([-0.0091]), b=tensor([0.0859], requires_grad=True), b.grad=tensor([0.0207])\n",
      "after  step() : W=tensor([0.9623], requires_grad=True), W.grad=tensor([-0.0091]), b=tensor([0.0857], requires_grad=True), b.grad=tensor([0.0207])\n",
      "=======================606==================\n",
      "hypothesis=\n",
      "tensor([[1.0480],\n",
      "        [2.0103],\n",
      "        [2.9726]], grad_fn=<AddBackward0>)\n",
      "loss=0.0010541569208726287\n",
      "before backward() : W=tensor([0.9623], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0857], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9623], requires_grad=True), W.grad=tensor([-0.0091]), b=tensor([0.0857], requires_grad=True), b.grad=tensor([0.0206])\n",
      "after  step() : W=tensor([0.9624], requires_grad=True), W.grad=tensor([-0.0091]), b=tensor([0.0855], requires_grad=True), b.grad=tensor([0.0206])\n",
      "=======================607==================\n",
      "hypothesis=\n",
      "tensor([[1.0479],\n",
      "        [2.0103],\n",
      "        [2.9727]], grad_fn=<AddBackward0>)\n",
      "loss=0.0010490940185263753\n",
      "before backward() : W=tensor([0.9624], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0855], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9624], requires_grad=True), W.grad=tensor([-0.0090]), b=tensor([0.0855], requires_grad=True), b.grad=tensor([0.0206])\n",
      "after  step() : W=tensor([0.9625], requires_grad=True), W.grad=tensor([-0.0090]), b=tensor([0.0853], requires_grad=True), b.grad=tensor([0.0206])\n",
      "=======================608==================\n",
      "hypothesis=\n",
      "tensor([[1.0478],\n",
      "        [2.0103],\n",
      "        [2.9727]], grad_fn=<AddBackward0>)\n",
      "loss=0.0010440581245347857\n",
      "before backward() : W=tensor([0.9625], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0853], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9625], requires_grad=True), W.grad=tensor([-0.0090]), b=tensor([0.0853], requires_grad=True), b.grad=tensor([0.0205])\n",
      "after  step() : W=tensor([0.9626], requires_grad=True), W.grad=tensor([-0.0090]), b=tensor([0.0851], requires_grad=True), b.grad=tensor([0.0205])\n",
      "=======================609==================\n",
      "hypothesis=\n",
      "tensor([[1.0477],\n",
      "        [2.0102],\n",
      "        [2.9728]], grad_fn=<AddBackward0>)\n",
      "loss=0.001039047259837389\n",
      "before backward() : W=tensor([0.9626], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0851], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9626], requires_grad=True), W.grad=tensor([-0.0090]), b=tensor([0.0851], requires_grad=True), b.grad=tensor([0.0205])\n",
      "after  step() : W=tensor([0.9627], requires_grad=True), W.grad=tensor([-0.0090]), b=tensor([0.0849], requires_grad=True), b.grad=tensor([0.0205])\n",
      "=======================610==================\n",
      "hypothesis=\n",
      "tensor([[1.0476],\n",
      "        [2.0102],\n",
      "        [2.9729]], grad_fn=<AddBackward0>)\n",
      "loss=0.0010340558364987373\n",
      "before backward() : W=tensor([0.9627], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0849], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9627], requires_grad=True), W.grad=tensor([-0.0090]), b=tensor([0.0849], requires_grad=True), b.grad=tensor([0.0204])\n",
      "after  step() : W=tensor([0.9627], requires_grad=True), W.grad=tensor([-0.0090]), b=tensor([0.0847], requires_grad=True), b.grad=tensor([0.0204])\n",
      "=======================611==================\n",
      "hypothesis=\n",
      "tensor([[1.0474],\n",
      "        [2.0102],\n",
      "        [2.9729]], grad_fn=<AddBackward0>)\n",
      "loss=0.0010290927020832896\n",
      "before backward() : W=tensor([0.9627], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0847], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9627], requires_grad=True), W.grad=tensor([-0.0090]), b=tensor([0.0847], requires_grad=True), b.grad=tensor([0.0204])\n",
      "after  step() : W=tensor([0.9628], requires_grad=True), W.grad=tensor([-0.0090]), b=tensor([0.0845], requires_grad=True), b.grad=tensor([0.0204])\n",
      "=======================612==================\n",
      "hypothesis=\n",
      "tensor([[1.0473],\n",
      "        [2.0102],\n",
      "        [2.9730]], grad_fn=<AddBackward0>)\n",
      "loss=0.001024149009026587\n",
      "before backward() : W=tensor([0.9628], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0845], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9628], requires_grad=True), W.grad=tensor([-0.0089]), b=tensor([0.0845], requires_grad=True), b.grad=tensor([0.0203])\n",
      "after  step() : W=tensor([0.9629], requires_grad=True), W.grad=tensor([-0.0089]), b=tensor([0.0843], requires_grad=True), b.grad=tensor([0.0203])\n",
      "=======================613==================\n",
      "hypothesis=\n",
      "tensor([[1.0472],\n",
      "        [2.0101],\n",
      "        [2.9731]], grad_fn=<AddBackward0>)\n",
      "loss=0.0010192301124334335\n",
      "before backward() : W=tensor([0.9629], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0843], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9629], requires_grad=True), W.grad=tensor([-0.0089]), b=tensor([0.0843], requires_grad=True), b.grad=tensor([0.0203])\n",
      "after  step() : W=tensor([0.9630], requires_grad=True), W.grad=tensor([-0.0089]), b=tensor([0.0841], requires_grad=True), b.grad=tensor([0.0203])\n",
      "=======================614==================\n",
      "hypothesis=\n",
      "tensor([[1.0471],\n",
      "        [2.0101],\n",
      "        [2.9731]], grad_fn=<AddBackward0>)\n",
      "loss=0.0010143338004127145\n",
      "before backward() : W=tensor([0.9630], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0841], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9630], requires_grad=True), W.grad=tensor([-0.0089]), b=tensor([0.0841], requires_grad=True), b.grad=tensor([0.0202])\n",
      "after  step() : W=tensor([0.9631], requires_grad=True), W.grad=tensor([-0.0089]), b=tensor([0.0839], requires_grad=True), b.grad=tensor([0.0202])\n",
      "=======================615==================\n",
      "hypothesis=\n",
      "tensor([[1.0470],\n",
      "        [2.0101],\n",
      "        [2.9732]], grad_fn=<AddBackward0>)\n",
      "loss=0.0010094657773151994\n",
      "before backward() : W=tensor([0.9631], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0839], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9631], requires_grad=True), W.grad=tensor([-0.0089]), b=tensor([0.0839], requires_grad=True), b.grad=tensor([0.0202])\n",
      "after  step() : W=tensor([0.9632], requires_grad=True), W.grad=tensor([-0.0089]), b=tensor([0.0837], requires_grad=True), b.grad=tensor([0.0202])\n",
      "=======================616==================\n",
      "hypothesis=\n",
      "tensor([[1.0469],\n",
      "        [2.0101],\n",
      "        [2.9732]], grad_fn=<AddBackward0>)\n",
      "loss=0.0010046131210401654\n",
      "before backward() : W=tensor([0.9632], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0837], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9632], requires_grad=True), W.grad=tensor([-0.0088]), b=tensor([0.0837], requires_grad=True), b.grad=tensor([0.0201])\n",
      "after  step() : W=tensor([0.9633], requires_grad=True), W.grad=tensor([-0.0088]), b=tensor([0.0835], requires_grad=True), b.grad=tensor([0.0201])\n",
      "=======================617==================\n",
      "hypothesis=\n",
      "tensor([[1.0468],\n",
      "        [2.0100],\n",
      "        [2.9733]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009997892193496227\n",
      "before backward() : W=tensor([0.9633], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0835], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9633], requires_grad=True), W.grad=tensor([-0.0088]), b=tensor([0.0835], requires_grad=True), b.grad=tensor([0.0201])\n",
      "after  step() : W=tensor([0.9634], requires_grad=True), W.grad=tensor([-0.0088]), b=tensor([0.0833], requires_grad=True), b.grad=tensor([0.0201])\n",
      "=======================618==================\n",
      "hypothesis=\n",
      "tensor([[1.0466],\n",
      "        [2.0100],\n",
      "        [2.9734]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009949886007234454\n",
      "before backward() : W=tensor([0.9634], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0833], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9634], requires_grad=True), W.grad=tensor([-0.0088]), b=tensor([0.0833], requires_grad=True), b.grad=tensor([0.0200])\n",
      "after  step() : W=tensor([0.9635], requires_grad=True), W.grad=tensor([-0.0088]), b=tensor([0.0831], requires_grad=True), b.grad=tensor([0.0200])\n",
      "=======================619==================\n",
      "hypothesis=\n",
      "tensor([[1.0465],\n",
      "        [2.0100],\n",
      "        [2.9734]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009902153396978974\n",
      "before backward() : W=tensor([0.9635], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0831], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9635], requires_grad=True), W.grad=tensor([-0.0088]), b=tensor([0.0831], requires_grad=True), b.grad=tensor([0.0200])\n",
      "after  step() : W=tensor([0.9635], requires_grad=True), W.grad=tensor([-0.0088]), b=tensor([0.0829], requires_grad=True), b.grad=tensor([0.0200])\n",
      "=======================620==================\n",
      "hypothesis=\n",
      "tensor([[1.0464],\n",
      "        [2.0100],\n",
      "        [2.9735]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009854561649262905\n",
      "before backward() : W=tensor([0.9635], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0829], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9635], requires_grad=True), W.grad=tensor([-0.0088]), b=tensor([0.0829], requires_grad=True), b.grad=tensor([0.0199])\n",
      "after  step() : W=tensor([0.9636], requires_grad=True), W.grad=tensor([-0.0088]), b=tensor([0.0827], requires_grad=True), b.grad=tensor([0.0199])\n",
      "=======================621==================\n",
      "hypothesis=\n",
      "tensor([[1.0463],\n",
      "        [2.0099],\n",
      "        [2.9736]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009807248134166002\n",
      "before backward() : W=tensor([0.9636], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0827], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9636], requires_grad=True), W.grad=tensor([-0.0087]), b=tensor([0.0827], requires_grad=True), b.grad=tensor([0.0199])\n",
      "after  step() : W=tensor([0.9637], requires_grad=True), W.grad=tensor([-0.0087]), b=tensor([0.0825], requires_grad=True), b.grad=tensor([0.0199])\n",
      "=======================622==================\n",
      "hypothesis=\n",
      "tensor([[1.0462],\n",
      "        [2.0099],\n",
      "        [2.9736]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009760131943039596\n",
      "before backward() : W=tensor([0.9637], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0825], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9637], requires_grad=True), W.grad=tensor([-0.0087]), b=tensor([0.0825], requires_grad=True), b.grad=tensor([0.0198])\n",
      "after  step() : W=tensor([0.9638], requires_grad=True), W.grad=tensor([-0.0087]), b=tensor([0.0823], requires_grad=True), b.grad=tensor([0.0198])\n",
      "=======================623==================\n",
      "hypothesis=\n",
      "tensor([[1.0461],\n",
      "        [2.0099],\n",
      "        [2.9737]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009713280014693737\n",
      "before backward() : W=tensor([0.9638], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0823], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9638], requires_grad=True), W.grad=tensor([-0.0087]), b=tensor([0.0823], requires_grad=True), b.grad=tensor([0.0198])\n",
      "after  step() : W=tensor([0.9639], requires_grad=True), W.grad=tensor([-0.0087]), b=tensor([0.0821], requires_grad=True), b.grad=tensor([0.0198])\n",
      "=======================624==================\n",
      "hypothesis=\n",
      "tensor([[1.0460],\n",
      "        [2.0099],\n",
      "        [2.9738]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009666620753705502\n",
      "before backward() : W=tensor([0.9639], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0821], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9639], requires_grad=True), W.grad=tensor([-0.0087]), b=tensor([0.0821], requires_grad=True), b.grad=tensor([0.0197])\n",
      "after  step() : W=tensor([0.9640], requires_grad=True), W.grad=tensor([-0.0087]), b=tensor([0.0819], requires_grad=True), b.grad=tensor([0.0197])\n",
      "=======================625==================\n",
      "hypothesis=\n",
      "tensor([[1.0459],\n",
      "        [2.0098],\n",
      "        [2.9738]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009620239143259823\n",
      "before backward() : W=tensor([0.9640], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0819], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9640], requires_grad=True), W.grad=tensor([-0.0087]), b=tensor([0.0819], requires_grad=True), b.grad=tensor([0.0197])\n",
      "after  step() : W=tensor([0.9641], requires_grad=True), W.grad=tensor([-0.0087]), b=tensor([0.0817], requires_grad=True), b.grad=tensor([0.0197])\n",
      "=======================626==================\n",
      "hypothesis=\n",
      "tensor([[1.0458],\n",
      "        [2.0098],\n",
      "        [2.9739]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009574000723659992\n",
      "before backward() : W=tensor([0.9641], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0817], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9641], requires_grad=True), W.grad=tensor([-0.0086]), b=tensor([0.0817], requires_grad=True), b.grad=tensor([0.0196])\n",
      "after  step() : W=tensor([0.9641], requires_grad=True), W.grad=tensor([-0.0086]), b=tensor([0.0815], requires_grad=True), b.grad=tensor([0.0196])\n",
      "=======================627==================\n",
      "hypothesis=\n",
      "tensor([[1.0456],\n",
      "        [2.0098],\n",
      "        [2.9739]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009528055670671165\n",
      "before backward() : W=tensor([0.9641], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0815], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9641], requires_grad=True), W.grad=tensor([-0.0086]), b=tensor([0.0815], requires_grad=True), b.grad=tensor([0.0196])\n",
      "after  step() : W=tensor([0.9642], requires_grad=True), W.grad=tensor([-0.0086]), b=tensor([0.0813], requires_grad=True), b.grad=tensor([0.0196])\n",
      "=======================628==================\n",
      "hypothesis=\n",
      "tensor([[1.0455],\n",
      "        [2.0098],\n",
      "        [2.9740]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009482318419031799\n",
      "before backward() : W=tensor([0.9642], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0813], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9642], requires_grad=True), W.grad=tensor([-0.0086]), b=tensor([0.0813], requires_grad=True), b.grad=tensor([0.0195])\n",
      "after  step() : W=tensor([0.9643], requires_grad=True), W.grad=tensor([-0.0086]), b=tensor([0.0811], requires_grad=True), b.grad=tensor([0.0195])\n",
      "=======================629==================\n",
      "hypothesis=\n",
      "tensor([[1.0454],\n",
      "        [2.0097],\n",
      "        [2.9741]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009436788968741894\n",
      "before backward() : W=tensor([0.9643], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0811], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9643], requires_grad=True), W.grad=tensor([-0.0086]), b=tensor([0.0811], requires_grad=True), b.grad=tensor([0.0195])\n",
      "after  step() : W=tensor([0.9644], requires_grad=True), W.grad=tensor([-0.0086]), b=tensor([0.0809], requires_grad=True), b.grad=tensor([0.0195])\n",
      "=======================630==================\n",
      "hypothesis=\n",
      "tensor([[1.0453],\n",
      "        [2.0097],\n",
      "        [2.9741]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009391456842422485\n",
      "before backward() : W=tensor([0.9644], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0809], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9644], requires_grad=True), W.grad=tensor([-0.0086]), b=tensor([0.0809], requires_grad=True), b.grad=tensor([0.0195])\n",
      "after  step() : W=tensor([0.9645], requires_grad=True), W.grad=tensor([-0.0086]), b=tensor([0.0807], requires_grad=True), b.grad=tensor([0.0195])\n",
      "=======================631==================\n",
      "hypothesis=\n",
      "tensor([[1.0452],\n",
      "        [2.0097],\n",
      "        [2.9742]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009346347651444376\n",
      "before backward() : W=tensor([0.9645], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0807], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9645], requires_grad=True), W.grad=tensor([-0.0085]), b=tensor([0.0807], requires_grad=True), b.grad=tensor([0.0194])\n",
      "after  step() : W=tensor([0.9646], requires_grad=True), W.grad=tensor([-0.0085]), b=tensor([0.0805], requires_grad=True), b.grad=tensor([0.0194])\n",
      "=======================632==================\n",
      "hypothesis=\n",
      "tensor([[1.0451],\n",
      "        [2.0097],\n",
      "        [2.9743]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009301495156250894\n",
      "before backward() : W=tensor([0.9646], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0805], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9646], requires_grad=True), W.grad=tensor([-0.0085]), b=tensor([0.0805], requires_grad=True), b.grad=tensor([0.0194])\n",
      "after  step() : W=tensor([0.9647], requires_grad=True), W.grad=tensor([-0.0085]), b=tensor([0.0803], requires_grad=True), b.grad=tensor([0.0194])\n",
      "=======================633==================\n",
      "hypothesis=\n",
      "tensor([[1.0450],\n",
      "        [2.0097],\n",
      "        [2.9743]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009256799821741879\n",
      "before backward() : W=tensor([0.9647], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0803], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9647], requires_grad=True), W.grad=tensor([-0.0085]), b=tensor([0.0803], requires_grad=True), b.grad=tensor([0.0193])\n",
      "after  step() : W=tensor([0.9647], requires_grad=True), W.grad=tensor([-0.0085]), b=tensor([0.0801], requires_grad=True), b.grad=tensor([0.0193])\n",
      "=======================634==================\n",
      "hypothesis=\n",
      "tensor([[1.0449],\n",
      "        [2.0096],\n",
      "        [2.9744]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009212361765094101\n",
      "before backward() : W=tensor([0.9647], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0801], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9647], requires_grad=True), W.grad=tensor([-0.0085]), b=tensor([0.0801], requires_grad=True), b.grad=tensor([0.0193])\n",
      "after  step() : W=tensor([0.9648], requires_grad=True), W.grad=tensor([-0.0085]), b=tensor([0.0799], requires_grad=True), b.grad=tensor([0.0193])\n",
      "=======================635==================\n",
      "hypothesis=\n",
      "tensor([[1.0448],\n",
      "        [2.0096],\n",
      "        [2.9744]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009168141987174749\n",
      "before backward() : W=tensor([0.9648], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0799], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9648], requires_grad=True), W.grad=tensor([-0.0085]), b=tensor([0.0799], requires_grad=True), b.grad=tensor([0.0192])\n",
      "after  step() : W=tensor([0.9649], requires_grad=True), W.grad=tensor([-0.0085]), b=tensor([0.0798], requires_grad=True), b.grad=tensor([0.0192])\n",
      "=======================636==================\n",
      "hypothesis=\n",
      "tensor([[1.0447],\n",
      "        [2.0096],\n",
      "        [2.9745]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009124099742621183\n",
      "before backward() : W=tensor([0.9649], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0798], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9649], requires_grad=True), W.grad=tensor([-0.0084]), b=tensor([0.0798], requires_grad=True), b.grad=tensor([0.0192])\n",
      "after  step() : W=tensor([0.9650], requires_grad=True), W.grad=tensor([-0.0084]), b=tensor([0.0796], requires_grad=True), b.grad=tensor([0.0192])\n",
      "=======================637==================\n",
      "hypothesis=\n",
      "tensor([[1.0446],\n",
      "        [2.0096],\n",
      "        [2.9746]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009080275776796043\n",
      "before backward() : W=tensor([0.9650], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0796], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9650], requires_grad=True), W.grad=tensor([-0.0084]), b=tensor([0.0796], requires_grad=True), b.grad=tensor([0.0191])\n",
      "after  step() : W=tensor([0.9651], requires_grad=True), W.grad=tensor([-0.0084]), b=tensor([0.0794], requires_grad=True), b.grad=tensor([0.0191])\n",
      "=======================638==================\n",
      "hypothesis=\n",
      "tensor([[1.0445],\n",
      "        [2.0095],\n",
      "        [2.9746]], grad_fn=<AddBackward0>)\n",
      "loss=0.0009036676492542028\n",
      "before backward() : W=tensor([0.9651], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0794], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9651], requires_grad=True), W.grad=tensor([-0.0084]), b=tensor([0.0794], requires_grad=True), b.grad=tensor([0.0191])\n",
      "after  step() : W=tensor([0.9652], requires_grad=True), W.grad=tensor([-0.0084]), b=tensor([0.0792], requires_grad=True), b.grad=tensor([0.0191])\n",
      "=======================639==================\n",
      "hypothesis=\n",
      "tensor([[1.0443],\n",
      "        [2.0095],\n",
      "        [2.9747]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008993258816190064\n",
      "before backward() : W=tensor([0.9652], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0792], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9652], requires_grad=True), W.grad=tensor([-0.0084]), b=tensor([0.0792], requires_grad=True), b.grad=tensor([0.0190])\n",
      "after  step() : W=tensor([0.9653], requires_grad=True), W.grad=tensor([-0.0084]), b=tensor([0.0790], requires_grad=True), b.grad=tensor([0.0190])\n",
      "=======================640==================\n",
      "hypothesis=\n",
      "tensor([[1.0442],\n",
      "        [2.0095],\n",
      "        [2.9747]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008950090850703418\n",
      "before backward() : W=tensor([0.9653], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0790], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9653], requires_grad=True), W.grad=tensor([-0.0084]), b=tensor([0.0790], requires_grad=True), b.grad=tensor([0.0190])\n",
      "after  step() : W=tensor([0.9653], requires_grad=True), W.grad=tensor([-0.0084]), b=tensor([0.0788], requires_grad=True), b.grad=tensor([0.0190])\n",
      "=======================641==================\n",
      "hypothesis=\n",
      "tensor([[1.0441],\n",
      "        [2.0095],\n",
      "        [2.9748]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008907136507332325\n",
      "before backward() : W=tensor([0.9653], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0788], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9653], requires_grad=True), W.grad=tensor([-0.0083]), b=tensor([0.0788], requires_grad=True), b.grad=tensor([0.0189])\n",
      "after  step() : W=tensor([0.9654], requires_grad=True), W.grad=tensor([-0.0083]), b=tensor([0.0786], requires_grad=True), b.grad=tensor([0.0189])\n",
      "=======================642==================\n",
      "hypothesis=\n",
      "tensor([[1.0440],\n",
      "        [2.0094],\n",
      "        [2.9749]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008864356204867363\n",
      "before backward() : W=tensor([0.9654], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0786], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9654], requires_grad=True), W.grad=tensor([-0.0083]), b=tensor([0.0786], requires_grad=True), b.grad=tensor([0.0189])\n",
      "after  step() : W=tensor([0.9655], requires_grad=True), W.grad=tensor([-0.0083]), b=tensor([0.0784], requires_grad=True), b.grad=tensor([0.0189])\n",
      "=======================643==================\n",
      "hypothesis=\n",
      "tensor([[1.0439],\n",
      "        [2.0094],\n",
      "        [2.9749]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008821763913147151\n",
      "before backward() : W=tensor([0.9655], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0784], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9655], requires_grad=True), W.grad=tensor([-0.0083]), b=tensor([0.0784], requires_grad=True), b.grad=tensor([0.0189])\n",
      "after  step() : W=tensor([0.9656], requires_grad=True), W.grad=tensor([-0.0083]), b=tensor([0.0782], requires_grad=True), b.grad=tensor([0.0189])\n",
      "=======================644==================\n",
      "hypothesis=\n",
      "tensor([[1.0438],\n",
      "        [2.0094],\n",
      "        [2.9750]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008779421914368868\n",
      "before backward() : W=tensor([0.9656], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0782], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9656], requires_grad=True), W.grad=tensor([-0.0083]), b=tensor([0.0782], requires_grad=True), b.grad=tensor([0.0188])\n",
      "after  step() : W=tensor([0.9657], requires_grad=True), W.grad=tensor([-0.0083]), b=tensor([0.0780], requires_grad=True), b.grad=tensor([0.0188])\n",
      "=======================645==================\n",
      "hypothesis=\n",
      "tensor([[1.0437],\n",
      "        [2.0094],\n",
      "        [2.9750]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008737247553654015\n",
      "before backward() : W=tensor([0.9657], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0780], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9657], requires_grad=True), W.grad=tensor([-0.0083]), b=tensor([0.0780], requires_grad=True), b.grad=tensor([0.0188])\n",
      "after  step() : W=tensor([0.9658], requires_grad=True), W.grad=tensor([-0.0083]), b=tensor([0.0779], requires_grad=True), b.grad=tensor([0.0188])\n",
      "=======================646==================\n",
      "hypothesis=\n",
      "tensor([[1.0436],\n",
      "        [2.0094],\n",
      "        [2.9751]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008695302531123161\n",
      "before backward() : W=tensor([0.9658], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0779], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9658], requires_grad=True), W.grad=tensor([-0.0082]), b=tensor([0.0779], requires_grad=True), b.grad=tensor([0.0187])\n",
      "after  step() : W=tensor([0.9658], requires_grad=True), W.grad=tensor([-0.0082]), b=tensor([0.0777], requires_grad=True), b.grad=tensor([0.0187])\n",
      "=======================647==================\n",
      "hypothesis=\n",
      "tensor([[1.0435],\n",
      "        [2.0093],\n",
      "        [2.9752]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008653548429720104\n",
      "before backward() : W=tensor([0.9658], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0777], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9658], requires_grad=True), W.grad=tensor([-0.0082]), b=tensor([0.0777], requires_grad=True), b.grad=tensor([0.0187])\n",
      "after  step() : W=tensor([0.9659], requires_grad=True), W.grad=tensor([-0.0082]), b=tensor([0.0775], requires_grad=True), b.grad=tensor([0.0187])\n",
      "=======================648==================\n",
      "hypothesis=\n",
      "tensor([[1.0434],\n",
      "        [2.0093],\n",
      "        [2.9752]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008611978846602142\n",
      "before backward() : W=tensor([0.9659], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0775], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9659], requires_grad=True), W.grad=tensor([-0.0082]), b=tensor([0.0775], requires_grad=True), b.grad=tensor([0.0186])\n",
      "after  step() : W=tensor([0.9660], requires_grad=True), W.grad=tensor([-0.0082]), b=tensor([0.0773], requires_grad=True), b.grad=tensor([0.0186])\n",
      "=======================649==================\n",
      "hypothesis=\n",
      "tensor([[1.0433],\n",
      "        [2.0093],\n",
      "        [2.9753]], grad_fn=<AddBackward0>)\n",
      "loss=0.000857063103467226\n",
      "before backward() : W=tensor([0.9660], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0773], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9660], requires_grad=True), W.grad=tensor([-0.0082]), b=tensor([0.0773], requires_grad=True), b.grad=tensor([0.0186])\n",
      "after  step() : W=tensor([0.9661], requires_grad=True), W.grad=tensor([-0.0082]), b=tensor([0.0771], requires_grad=True), b.grad=tensor([0.0186])\n",
      "=======================650==================\n",
      "hypothesis=\n",
      "tensor([[1.0432],\n",
      "        [2.0093],\n",
      "        [2.9753]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008529441547580063\n",
      "before backward() : W=tensor([0.9661], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0771], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9661], requires_grad=True), W.grad=tensor([-0.0082]), b=tensor([0.0771], requires_grad=True), b.grad=tensor([0.0185])\n",
      "after  step() : W=tensor([0.9662], requires_grad=True), W.grad=tensor([-0.0082]), b=tensor([0.0769], requires_grad=True), b.grad=tensor([0.0185])\n",
      "=======================651==================\n",
      "hypothesis=\n",
      "tensor([[1.0431],\n",
      "        [2.0092],\n",
      "        [2.9754]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008488493622280657\n",
      "before backward() : W=tensor([0.9662], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0769], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9662], requires_grad=True), W.grad=tensor([-0.0081]), b=tensor([0.0769], requires_grad=True), b.grad=tensor([0.0185])\n",
      "after  step() : W=tensor([0.9662], requires_grad=True), W.grad=tensor([-0.0081]), b=tensor([0.0767], requires_grad=True), b.grad=tensor([0.0185])\n",
      "=======================652==================\n",
      "hypothesis=\n",
      "tensor([[1.0430],\n",
      "        [2.0092],\n",
      "        [2.9755]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008447722066193819\n",
      "before backward() : W=tensor([0.9662], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0767], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9662], requires_grad=True), W.grad=tensor([-0.0081]), b=tensor([0.0767], requires_grad=True), b.grad=tensor([0.0184])\n",
      "after  step() : W=tensor([0.9663], requires_grad=True), W.grad=tensor([-0.0081]), b=tensor([0.0766], requires_grad=True), b.grad=tensor([0.0184])\n",
      "=======================653==================\n",
      "hypothesis=\n",
      "tensor([[1.0429],\n",
      "        [2.0092],\n",
      "        [2.9755]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008407160639762878\n",
      "before backward() : W=tensor([0.9663], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0766], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9663], requires_grad=True), W.grad=tensor([-0.0081]), b=tensor([0.0766], requires_grad=True), b.grad=tensor([0.0184])\n",
      "after  step() : W=tensor([0.9664], requires_grad=True), W.grad=tensor([-0.0081]), b=tensor([0.0764], requires_grad=True), b.grad=tensor([0.0184])\n",
      "=======================654==================\n",
      "hypothesis=\n",
      "tensor([[1.0428],\n",
      "        [2.0092],\n",
      "        [2.9756]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008366803522221744\n",
      "before backward() : W=tensor([0.9664], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0764], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9664], requires_grad=True), W.grad=tensor([-0.0081]), b=tensor([0.0764], requires_grad=True), b.grad=tensor([0.0184])\n",
      "after  step() : W=tensor([0.9665], requires_grad=True), W.grad=tensor([-0.0081]), b=tensor([0.0762], requires_grad=True), b.grad=tensor([0.0184])\n",
      "=======================655==================\n",
      "hypothesis=\n",
      "tensor([[1.0427],\n",
      "        [2.0092],\n",
      "        [2.9756]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008326645474880934\n",
      "before backward() : W=tensor([0.9665], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0762], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9665], requires_grad=True), W.grad=tensor([-0.0081]), b=tensor([0.0762], requires_grad=True), b.grad=tensor([0.0183])\n",
      "after  step() : W=tensor([0.9666], requires_grad=True), W.grad=tensor([-0.0081]), b=tensor([0.0760], requires_grad=True), b.grad=tensor([0.0183])\n",
      "=======================656==================\n",
      "hypothesis=\n",
      "tensor([[1.0426],\n",
      "        [2.0091],\n",
      "        [2.9757]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008286631782539189\n",
      "before backward() : W=tensor([0.9666], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0760], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9666], requires_grad=True), W.grad=tensor([-0.0080]), b=tensor([0.0760], requires_grad=True), b.grad=tensor([0.0183])\n",
      "after  step() : W=tensor([0.9666], requires_grad=True), W.grad=tensor([-0.0080]), b=tensor([0.0758], requires_grad=True), b.grad=tensor([0.0183])\n",
      "=======================657==================\n",
      "hypothesis=\n",
      "tensor([[1.0425],\n",
      "        [2.0091],\n",
      "        [2.9758]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008246854995377362\n",
      "before backward() : W=tensor([0.9666], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0758], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9666], requires_grad=True), W.grad=tensor([-0.0080]), b=tensor([0.0758], requires_grad=True), b.grad=tensor([0.0182])\n",
      "after  step() : W=tensor([0.9667], requires_grad=True), W.grad=tensor([-0.0080]), b=tensor([0.0756], requires_grad=True), b.grad=tensor([0.0182])\n",
      "=======================658==================\n",
      "hypothesis=\n",
      "tensor([[1.0424],\n",
      "        [2.0091],\n",
      "        [2.9758]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008207245846278965\n",
      "before backward() : W=tensor([0.9667], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0756], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9667], requires_grad=True), W.grad=tensor([-0.0080]), b=tensor([0.0756], requires_grad=True), b.grad=tensor([0.0182])\n",
      "after  step() : W=tensor([0.9668], requires_grad=True), W.grad=tensor([-0.0080]), b=tensor([0.0755], requires_grad=True), b.grad=tensor([0.0182])\n",
      "=======================659==================\n",
      "hypothesis=\n",
      "tensor([[1.0423],\n",
      "        [2.0091],\n",
      "        [2.9759]], grad_fn=<AddBackward0>)\n",
      "loss=0.000816787127405405\n",
      "before backward() : W=tensor([0.9668], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0755], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9668], requires_grad=True), W.grad=tensor([-0.0080]), b=tensor([0.0755], requires_grad=True), b.grad=tensor([0.0181])\n",
      "after  step() : W=tensor([0.9669], requires_grad=True), W.grad=tensor([-0.0080]), b=tensor([0.0753], requires_grad=True), b.grad=tensor([0.0181])\n",
      "=======================660==================\n",
      "hypothesis=\n",
      "tensor([[1.0422],\n",
      "        [2.0090],\n",
      "        [2.9759]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008128595654852688\n",
      "before backward() : W=tensor([0.9669], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0753], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9669], requires_grad=True), W.grad=tensor([-0.0080]), b=tensor([0.0753], requires_grad=True), b.grad=tensor([0.0181])\n",
      "after  step() : W=tensor([0.9670], requires_grad=True), W.grad=tensor([-0.0080]), b=tensor([0.0751], requires_grad=True), b.grad=tensor([0.0181])\n",
      "=======================661==================\n",
      "hypothesis=\n",
      "tensor([[1.0421],\n",
      "        [2.0090],\n",
      "        [2.9760]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008089562761597335\n",
      "before backward() : W=tensor([0.9670], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0751], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9670], requires_grad=True), W.grad=tensor([-0.0079]), b=tensor([0.0751], requires_grad=True), b.grad=tensor([0.0181])\n",
      "after  step() : W=tensor([0.9670], requires_grad=True), W.grad=tensor([-0.0079]), b=tensor([0.0749], requires_grad=True), b.grad=tensor([0.0181])\n",
      "=======================662==================\n",
      "hypothesis=\n",
      "tensor([[1.0420],\n",
      "        [2.0090],\n",
      "        [2.9760]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008050748147070408\n",
      "before backward() : W=tensor([0.9670], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0749], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9670], requires_grad=True), W.grad=tensor([-0.0079]), b=tensor([0.0749], requires_grad=True), b.grad=tensor([0.0180])\n",
      "after  step() : W=tensor([0.9671], requires_grad=True), W.grad=tensor([-0.0079]), b=tensor([0.0747], requires_grad=True), b.grad=tensor([0.0180])\n",
      "=======================663==================\n",
      "hypothesis=\n",
      "tensor([[1.0419],\n",
      "        [2.0090],\n",
      "        [2.9761]], grad_fn=<AddBackward0>)\n",
      "loss=0.0008012083708308637\n",
      "before backward() : W=tensor([0.9671], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0747], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9671], requires_grad=True), W.grad=tensor([-0.0079]), b=tensor([0.0747], requires_grad=True), b.grad=tensor([0.0180])\n",
      "after  step() : W=tensor([0.9672], requires_grad=True), W.grad=tensor([-0.0079]), b=tensor([0.0746], requires_grad=True), b.grad=tensor([0.0180])\n",
      "=======================664==================\n",
      "hypothesis=\n",
      "tensor([[1.0418],\n",
      "        [2.0090],\n",
      "        [2.9762]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007973602041602135\n",
      "before backward() : W=tensor([0.9672], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0746], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9672], requires_grad=True), W.grad=tensor([-0.0079]), b=tensor([0.0746], requires_grad=True), b.grad=tensor([0.0179])\n",
      "after  step() : W=tensor([0.9673], requires_grad=True), W.grad=tensor([-0.0079]), b=tensor([0.0744], requires_grad=True), b.grad=tensor([0.0179])\n",
      "=======================665==================\n",
      "hypothesis=\n",
      "tensor([[1.0417],\n",
      "        [2.0089],\n",
      "        [2.9762]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007935331668704748\n",
      "before backward() : W=tensor([0.9673], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0744], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9673], requires_grad=True), W.grad=tensor([-0.0079]), b=tensor([0.0744], requires_grad=True), b.grad=tensor([0.0179])\n",
      "after  step() : W=tensor([0.9674], requires_grad=True), W.grad=tensor([-0.0079]), b=tensor([0.0742], requires_grad=True), b.grad=tensor([0.0179])\n",
      "=======================666==================\n",
      "hypothesis=\n",
      "tensor([[1.0416],\n",
      "        [2.0089],\n",
      "        [2.9763]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007897219620645046\n",
      "before backward() : W=tensor([0.9674], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0742], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9674], requires_grad=True), W.grad=tensor([-0.0078]), b=tensor([0.0742], requires_grad=True), b.grad=tensor([0.0178])\n",
      "after  step() : W=tensor([0.9674], requires_grad=True), W.grad=tensor([-0.0078]), b=tensor([0.0740], requires_grad=True), b.grad=tensor([0.0178])\n",
      "=======================667==================\n",
      "hypothesis=\n",
      "tensor([[1.0415],\n",
      "        [2.0089],\n",
      "        [2.9763]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007859264151193202\n",
      "before backward() : W=tensor([0.9674], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0740], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9674], requires_grad=True), W.grad=tensor([-0.0078]), b=tensor([0.0740], requires_grad=True), b.grad=tensor([0.0178])\n",
      "after  step() : W=tensor([0.9675], requires_grad=True), W.grad=tensor([-0.0078]), b=tensor([0.0738], requires_grad=True), b.grad=tensor([0.0178])\n",
      "=======================668==================\n",
      "hypothesis=\n",
      "tensor([[1.0414],\n",
      "        [2.0089],\n",
      "        [2.9764]], grad_fn=<AddBackward0>)\n",
      "loss=0.000782154209446162\n",
      "before backward() : W=tensor([0.9675], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0738], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9675], requires_grad=True), W.grad=tensor([-0.0078]), b=tensor([0.0738], requires_grad=True), b.grad=tensor([0.0178])\n",
      "after  step() : W=tensor([0.9676], requires_grad=True), W.grad=tensor([-0.0078]), b=tensor([0.0737], requires_grad=True), b.grad=tensor([0.0178])\n",
      "=======================669==================\n",
      "hypothesis=\n",
      "tensor([[1.0413],\n",
      "        [2.0089],\n",
      "        [2.9764]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007783989422023296\n",
      "before backward() : W=tensor([0.9676], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0737], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9676], requires_grad=True), W.grad=tensor([-0.0078]), b=tensor([0.0737], requires_grad=True), b.grad=tensor([0.0177])\n",
      "after  step() : W=tensor([0.9677], requires_grad=True), W.grad=tensor([-0.0078]), b=tensor([0.0735], requires_grad=True), b.grad=tensor([0.0177])\n",
      "=======================670==================\n",
      "hypothesis=\n",
      "tensor([[1.0412],\n",
      "        [2.0088],\n",
      "        [2.9765]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007746616029180586\n",
      "before backward() : W=tensor([0.9677], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0735], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9677], requires_grad=True), W.grad=tensor([-0.0078]), b=tensor([0.0735], requires_grad=True), b.grad=tensor([0.0177])\n",
      "after  step() : W=tensor([0.9678], requires_grad=True), W.grad=tensor([-0.0078]), b=tensor([0.0733], requires_grad=True), b.grad=tensor([0.0177])\n",
      "=======================671==================\n",
      "hypothesis=\n",
      "tensor([[1.0411],\n",
      "        [2.0088],\n",
      "        [2.9766]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007709412020631135\n",
      "before backward() : W=tensor([0.9678], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0733], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9678], requires_grad=True), W.grad=tensor([-0.0078]), b=tensor([0.0733], requires_grad=True), b.grad=tensor([0.0176])\n",
      "after  step() : W=tensor([0.9678], requires_grad=True), W.grad=tensor([-0.0078]), b=tensor([0.0731], requires_grad=True), b.grad=tensor([0.0176])\n",
      "=======================672==================\n",
      "hypothesis=\n",
      "tensor([[1.0410],\n",
      "        [2.0088],\n",
      "        [2.9766]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007672377396374941\n",
      "before backward() : W=tensor([0.9678], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0731], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9678], requires_grad=True), W.grad=tensor([-0.0077]), b=tensor([0.0731], requires_grad=True), b.grad=tensor([0.0176])\n",
      "after  step() : W=tensor([0.9679], requires_grad=True), W.grad=tensor([-0.0077]), b=tensor([0.0730], requires_grad=True), b.grad=tensor([0.0176])\n",
      "=======================673==================\n",
      "hypothesis=\n",
      "tensor([[1.0409],\n",
      "        [2.0088],\n",
      "        [2.9767]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007635519723407924\n",
      "before backward() : W=tensor([0.9679], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0730], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9679], requires_grad=True), W.grad=tensor([-0.0077]), b=tensor([0.0730], requires_grad=True), b.grad=tensor([0.0175])\n",
      "after  step() : W=tensor([0.9680], requires_grad=True), W.grad=tensor([-0.0077]), b=tensor([0.0728], requires_grad=True), b.grad=tensor([0.0175])\n",
      "=======================674==================\n",
      "hypothesis=\n",
      "tensor([[1.0408],\n",
      "        [2.0087],\n",
      "        [2.9767]], grad_fn=<AddBackward0>)\n",
      "loss=0.000759888906031847\n",
      "before backward() : W=tensor([0.9680], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0728], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9680], requires_grad=True), W.grad=tensor([-0.0077]), b=tensor([0.0728], requires_grad=True), b.grad=tensor([0.0175])\n",
      "after  step() : W=tensor([0.9681], requires_grad=True), W.grad=tensor([-0.0077]), b=tensor([0.0726], requires_grad=True), b.grad=tensor([0.0175])\n",
      "=======================675==================\n",
      "hypothesis=\n",
      "tensor([[1.0407],\n",
      "        [2.0087],\n",
      "        [2.9768]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007562355021946132\n",
      "before backward() : W=tensor([0.9681], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0726], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9681], requires_grad=True), W.grad=tensor([-0.0077]), b=tensor([0.0726], requires_grad=True), b.grad=tensor([0.0175])\n",
      "after  step() : W=tensor([0.9681], requires_grad=True), W.grad=tensor([-0.0077]), b=tensor([0.0724], requires_grad=True), b.grad=tensor([0.0175])\n",
      "=======================676==================\n",
      "hypothesis=\n",
      "tensor([[1.0406],\n",
      "        [2.0087],\n",
      "        [2.9768]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007526080589741468\n",
      "before backward() : W=tensor([0.9681], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0724], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9681], requires_grad=True), W.grad=tensor([-0.0077]), b=tensor([0.0724], requires_grad=True), b.grad=tensor([0.0174])\n",
      "after  step() : W=tensor([0.9682], requires_grad=True), W.grad=tensor([-0.0077]), b=tensor([0.0723], requires_grad=True), b.grad=tensor([0.0174])\n",
      "=======================677==================\n",
      "hypothesis=\n",
      "tensor([[1.0405],\n",
      "        [2.0087],\n",
      "        [2.9769]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007489920244552195\n",
      "before backward() : W=tensor([0.9682], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0723], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9682], requires_grad=True), W.grad=tensor([-0.0076]), b=tensor([0.0723], requires_grad=True), b.grad=tensor([0.0174])\n",
      "after  step() : W=tensor([0.9683], requires_grad=True), W.grad=tensor([-0.0076]), b=tensor([0.0721], requires_grad=True), b.grad=tensor([0.0174])\n",
      "=======================678==================\n",
      "hypothesis=\n",
      "tensor([[1.0404],\n",
      "        [2.0087],\n",
      "        [2.9770]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007453925791196525\n",
      "before backward() : W=tensor([0.9683], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0721], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9683], requires_grad=True), W.grad=tensor([-0.0076]), b=tensor([0.0721], requires_grad=True), b.grad=tensor([0.0173])\n",
      "after  step() : W=tensor([0.9684], requires_grad=True), W.grad=tensor([-0.0076]), b=tensor([0.0719], requires_grad=True), b.grad=tensor([0.0173])\n",
      "=======================679==================\n",
      "hypothesis=\n",
      "tensor([[1.0403],\n",
      "        [2.0086],\n",
      "        [2.9770]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007418140885420144\n",
      "before backward() : W=tensor([0.9684], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0719], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9684], requires_grad=True), W.grad=tensor([-0.0076]), b=tensor([0.0719], requires_grad=True), b.grad=tensor([0.0173])\n",
      "after  step() : W=tensor([0.9684], requires_grad=True), W.grad=tensor([-0.0076]), b=tensor([0.0717], requires_grad=True), b.grad=tensor([0.0173])\n",
      "=======================680==================\n",
      "hypothesis=\n",
      "tensor([[1.0402],\n",
      "        [2.0086],\n",
      "        [2.9771]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007382519543170929\n",
      "before backward() : W=tensor([0.9684], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0717], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9684], requires_grad=True), W.grad=tensor([-0.0076]), b=tensor([0.0717], requires_grad=True), b.grad=tensor([0.0172])\n",
      "after  step() : W=tensor([0.9685], requires_grad=True), W.grad=tensor([-0.0076]), b=tensor([0.0716], requires_grad=True), b.grad=tensor([0.0172])\n",
      "=======================681==================\n",
      "hypothesis=\n",
      "tensor([[1.0401],\n",
      "        [2.0086],\n",
      "        [2.9771]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007347120554186404\n",
      "before backward() : W=tensor([0.9685], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0716], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9685], requires_grad=True), W.grad=tensor([-0.0076]), b=tensor([0.0716], requires_grad=True), b.grad=tensor([0.0172])\n",
      "after  step() : W=tensor([0.9686], requires_grad=True), W.grad=tensor([-0.0076]), b=tensor([0.0714], requires_grad=True), b.grad=tensor([0.0172])\n",
      "=======================682==================\n",
      "hypothesis=\n",
      "tensor([[1.0400],\n",
      "        [2.0086],\n",
      "        [2.9772]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007311811204999685\n",
      "before backward() : W=tensor([0.9686], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0714], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9686], requires_grad=True), W.grad=tensor([-0.0075]), b=tensor([0.0714], requires_grad=True), b.grad=tensor([0.0172])\n",
      "after  step() : W=tensor([0.9687], requires_grad=True), W.grad=tensor([-0.0075]), b=tensor([0.0712], requires_grad=True), b.grad=tensor([0.0172])\n",
      "=======================683==================\n",
      "hypothesis=\n",
      "tensor([[1.0399],\n",
      "        [2.0086],\n",
      "        [2.9772]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007276723044924438\n",
      "before backward() : W=tensor([0.9687], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0712], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9687], requires_grad=True), W.grad=tensor([-0.0075]), b=tensor([0.0712], requires_grad=True), b.grad=tensor([0.0171])\n",
      "after  step() : W=tensor([0.9687], requires_grad=True), W.grad=tensor([-0.0075]), b=tensor([0.0710], requires_grad=True), b.grad=tensor([0.0171])\n",
      "=======================684==================\n",
      "hypothesis=\n",
      "tensor([[1.0398],\n",
      "        [2.0085],\n",
      "        [2.9773]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007241736748255789\n",
      "before backward() : W=tensor([0.9687], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0710], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9687], requires_grad=True), W.grad=tensor([-0.0075]), b=tensor([0.0710], requires_grad=True), b.grad=tensor([0.0171])\n",
      "after  step() : W=tensor([0.9688], requires_grad=True), W.grad=tensor([-0.0075]), b=tensor([0.0709], requires_grad=True), b.grad=tensor([0.0171])\n",
      "=======================685==================\n",
      "hypothesis=\n",
      "tensor([[1.0397],\n",
      "        [2.0085],\n",
      "        [2.9773]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007206956506706774\n",
      "before backward() : W=tensor([0.9688], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0709], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9688], requires_grad=True), W.grad=tensor([-0.0075]), b=tensor([0.0709], requires_grad=True), b.grad=tensor([0.0170])\n",
      "after  step() : W=tensor([0.9689], requires_grad=True), W.grad=tensor([-0.0075]), b=tensor([0.0707], requires_grad=True), b.grad=tensor([0.0170])\n",
      "=======================686==================\n",
      "hypothesis=\n",
      "tensor([[1.0396],\n",
      "        [2.0085],\n",
      "        [2.9774]], grad_fn=<AddBackward0>)\n",
      "loss=0.000717233691830188\n",
      "before backward() : W=tensor([0.9689], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0707], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9689], requires_grad=True), W.grad=tensor([-0.0075]), b=tensor([0.0707], requires_grad=True), b.grad=tensor([0.0170])\n",
      "after  step() : W=tensor([0.9690], requires_grad=True), W.grad=tensor([-0.0075]), b=tensor([0.0705], requires_grad=True), b.grad=tensor([0.0170])\n",
      "=======================687==================\n",
      "hypothesis=\n",
      "tensor([[1.0395],\n",
      "        [2.0085],\n",
      "        [2.9774]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007137912325561047\n",
      "before backward() : W=tensor([0.9690], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0705], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9690], requires_grad=True), W.grad=tensor([-0.0075]), b=tensor([0.0705], requires_grad=True), b.grad=tensor([0.0170])\n",
      "after  step() : W=tensor([0.9690], requires_grad=True), W.grad=tensor([-0.0075]), b=tensor([0.0704], requires_grad=True), b.grad=tensor([0.0170])\n",
      "=======================688==================\n",
      "hypothesis=\n",
      "tensor([[1.0394],\n",
      "        [2.0085],\n",
      "        [2.9775]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007103642565198243\n",
      "before backward() : W=tensor([0.9690], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0704], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9690], requires_grad=True), W.grad=tensor([-0.0074]), b=tensor([0.0704], requires_grad=True), b.grad=tensor([0.0169])\n",
      "after  step() : W=tensor([0.9691], requires_grad=True), W.grad=tensor([-0.0074]), b=tensor([0.0702], requires_grad=True), b.grad=tensor([0.0169])\n",
      "=======================689==================\n",
      "hypothesis=\n",
      "tensor([[1.0393],\n",
      "        [2.0084],\n",
      "        [2.9776]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007069557323120534\n",
      "before backward() : W=tensor([0.9691], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0702], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9691], requires_grad=True), W.grad=tensor([-0.0074]), b=tensor([0.0702], requires_grad=True), b.grad=tensor([0.0169])\n",
      "after  step() : W=tensor([0.9692], requires_grad=True), W.grad=tensor([-0.0074]), b=tensor([0.0700], requires_grad=True), b.grad=tensor([0.0169])\n",
      "=======================690==================\n",
      "hypothesis=\n",
      "tensor([[1.0392],\n",
      "        [2.0084],\n",
      "        [2.9776]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007035603630356491\n",
      "before backward() : W=tensor([0.9692], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0700], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9692], requires_grad=True), W.grad=tensor([-0.0074]), b=tensor([0.0700], requires_grad=True), b.grad=tensor([0.0168])\n",
      "after  step() : W=tensor([0.9693], requires_grad=True), W.grad=tensor([-0.0074]), b=tensor([0.0699], requires_grad=True), b.grad=tensor([0.0168])\n",
      "=======================691==================\n",
      "hypothesis=\n",
      "tensor([[1.0391],\n",
      "        [2.0084],\n",
      "        [2.9777]], grad_fn=<AddBackward0>)\n",
      "loss=0.0007001819903962314\n",
      "before backward() : W=tensor([0.9693], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0699], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9693], requires_grad=True), W.grad=tensor([-0.0074]), b=tensor([0.0699], requires_grad=True), b.grad=tensor([0.0168])\n",
      "after  step() : W=tensor([0.9693], requires_grad=True), W.grad=tensor([-0.0074]), b=tensor([0.0697], requires_grad=True), b.grad=tensor([0.0168])\n",
      "=======================692==================\n",
      "hypothesis=\n",
      "tensor([[1.0390],\n",
      "        [2.0084],\n",
      "        [2.9777]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006968202069401741\n",
      "before backward() : W=tensor([0.9693], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0697], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9693], requires_grad=True), W.grad=tensor([-0.0074]), b=tensor([0.0697], requires_grad=True), b.grad=tensor([0.0168])\n",
      "after  step() : W=tensor([0.9694], requires_grad=True), W.grad=tensor([-0.0074]), b=tensor([0.0695], requires_grad=True), b.grad=tensor([0.0168])\n",
      "=======================693==================\n",
      "hypothesis=\n",
      "tensor([[1.0389],\n",
      "        [2.0084],\n",
      "        [2.9778]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006934739649295807\n",
      "before backward() : W=tensor([0.9694], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0695], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9694], requires_grad=True), W.grad=tensor([-0.0074]), b=tensor([0.0695], requires_grad=True), b.grad=tensor([0.0167])\n",
      "after  step() : W=tensor([0.9695], requires_grad=True), W.grad=tensor([-0.0074]), b=tensor([0.0694], requires_grad=True), b.grad=tensor([0.0167])\n",
      "=======================694==================\n",
      "hypothesis=\n",
      "tensor([[1.0388],\n",
      "        [2.0083],\n",
      "        [2.9778]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006901419837959111\n",
      "before backward() : W=tensor([0.9695], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0694], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9695], requires_grad=True), W.grad=tensor([-0.0073]), b=tensor([0.0694], requires_grad=True), b.grad=tensor([0.0167])\n",
      "after  step() : W=tensor([0.9696], requires_grad=True), W.grad=tensor([-0.0073]), b=tensor([0.0692], requires_grad=True), b.grad=tensor([0.0167])\n",
      "=======================695==================\n",
      "hypothesis=\n",
      "tensor([[1.0388],\n",
      "        [2.0083],\n",
      "        [2.9779]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006868268828839064\n",
      "before backward() : W=tensor([0.9696], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0692], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9696], requires_grad=True), W.grad=tensor([-0.0073]), b=tensor([0.0692], requires_grad=True), b.grad=tensor([0.0166])\n",
      "after  step() : W=tensor([0.9696], requires_grad=True), W.grad=tensor([-0.0073]), b=tensor([0.0690], requires_grad=True), b.grad=tensor([0.0166])\n",
      "=======================696==================\n",
      "hypothesis=\n",
      "tensor([[1.0387],\n",
      "        [2.0083],\n",
      "        [2.9779]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006835315725766122\n",
      "before backward() : W=tensor([0.9696], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0690], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9696], requires_grad=True), W.grad=tensor([-0.0073]), b=tensor([0.0690], requires_grad=True), b.grad=tensor([0.0166])\n",
      "after  step() : W=tensor([0.9697], requires_grad=True), W.grad=tensor([-0.0073]), b=tensor([0.0689], requires_grad=True), b.grad=tensor([0.0166])\n",
      "=======================697==================\n",
      "hypothesis=\n",
      "tensor([[1.0386],\n",
      "        [2.0083],\n",
      "        [2.9780]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006802459247410297\n",
      "before backward() : W=tensor([0.9697], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0689], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9697], requires_grad=True), W.grad=tensor([-0.0073]), b=tensor([0.0689], requires_grad=True), b.grad=tensor([0.0166])\n",
      "after  step() : W=tensor([0.9698], requires_grad=True), W.grad=tensor([-0.0073]), b=tensor([0.0687], requires_grad=True), b.grad=tensor([0.0166])\n",
      "=======================698==================\n",
      "hypothesis=\n",
      "tensor([[1.0385],\n",
      "        [2.0083],\n",
      "        [2.9780]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006769821629859507\n",
      "before backward() : W=tensor([0.9698], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0687], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9698], requires_grad=True), W.grad=tensor([-0.0073]), b=tensor([0.0687], requires_grad=True), b.grad=tensor([0.0165])\n",
      "after  step() : W=tensor([0.9699], requires_grad=True), W.grad=tensor([-0.0073]), b=tensor([0.0685], requires_grad=True), b.grad=tensor([0.0165])\n",
      "=======================699==================\n",
      "hypothesis=\n",
      "tensor([[1.0384],\n",
      "        [2.0082],\n",
      "        [2.9781]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006737294024787843\n",
      "before backward() : W=tensor([0.9699], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0685], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9699], requires_grad=True), W.grad=tensor([-0.0072]), b=tensor([0.0685], requires_grad=True), b.grad=tensor([0.0165])\n",
      "after  step() : W=tensor([0.9699], requires_grad=True), W.grad=tensor([-0.0072]), b=tensor([0.0684], requires_grad=True), b.grad=tensor([0.0165])\n",
      "=======================700==================\n",
      "hypothesis=\n",
      "tensor([[1.0383],\n",
      "        [2.0082],\n",
      "        [2.9781]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006704938714392483\n",
      "before backward() : W=tensor([0.9699], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0684], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9699], requires_grad=True), W.grad=tensor([-0.0072]), b=tensor([0.0684], requires_grad=True), b.grad=tensor([0.0164])\n",
      "after  step() : W=tensor([0.9700], requires_grad=True), W.grad=tensor([-0.0072]), b=tensor([0.0682], requires_grad=True), b.grad=tensor([0.0164])\n",
      "=======================701==================\n",
      "hypothesis=\n",
      "tensor([[1.0382],\n",
      "        [2.0082],\n",
      "        [2.9782]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006672758026979864\n",
      "before backward() : W=tensor([0.9700], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0682], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9700], requires_grad=True), W.grad=tensor([-0.0072]), b=tensor([0.0682], requires_grad=True), b.grad=tensor([0.0164])\n",
      "after  step() : W=tensor([0.9701], requires_grad=True), W.grad=tensor([-0.0072]), b=tensor([0.0680], requires_grad=True), b.grad=tensor([0.0164])\n",
      "=======================702==================\n",
      "hypothesis=\n",
      "tensor([[1.0381],\n",
      "        [2.0082],\n",
      "        [2.9782]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006640684441663325\n",
      "before backward() : W=tensor([0.9701], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0680], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9701], requires_grad=True), W.grad=tensor([-0.0072]), b=tensor([0.0680], requires_grad=True), b.grad=tensor([0.0164])\n",
      "after  step() : W=tensor([0.9701], requires_grad=True), W.grad=tensor([-0.0072]), b=tensor([0.0679], requires_grad=True), b.grad=tensor([0.0164])\n",
      "=======================703==================\n",
      "hypothesis=\n",
      "tensor([[1.0380],\n",
      "        [2.0082],\n",
      "        [2.9783]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006608826224692166\n",
      "before backward() : W=tensor([0.9701], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0679], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9701], requires_grad=True), W.grad=tensor([-0.0072]), b=tensor([0.0679], requires_grad=True), b.grad=tensor([0.0163])\n",
      "after  step() : W=tensor([0.9702], requires_grad=True), W.grad=tensor([-0.0072]), b=tensor([0.0677], requires_grad=True), b.grad=tensor([0.0163])\n",
      "=======================704==================\n",
      "hypothesis=\n",
      "tensor([[1.0379],\n",
      "        [2.0081],\n",
      "        [2.9784]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006577061139978468\n",
      "before backward() : W=tensor([0.9702], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0677], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9702], requires_grad=True), W.grad=tensor([-0.0072]), b=tensor([0.0677], requires_grad=True), b.grad=tensor([0.0163])\n",
      "after  step() : W=tensor([0.9703], requires_grad=True), W.grad=tensor([-0.0072]), b=tensor([0.0675], requires_grad=True), b.grad=tensor([0.0163])\n",
      "=======================705==================\n",
      "hypothesis=\n",
      "tensor([[1.0378],\n",
      "        [2.0081],\n",
      "        [2.9784]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006545481737703085\n",
      "before backward() : W=tensor([0.9703], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0675], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9703], requires_grad=True), W.grad=tensor([-0.0071]), b=tensor([0.0675], requires_grad=True), b.grad=tensor([0.0162])\n",
      "after  step() : W=tensor([0.9704], requires_grad=True), W.grad=tensor([-0.0071]), b=tensor([0.0674], requires_grad=True), b.grad=tensor([0.0162])\n",
      "=======================706==================\n",
      "hypothesis=\n",
      "tensor([[1.0377],\n",
      "        [2.0081],\n",
      "        [2.9785]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006514036213047802\n",
      "before backward() : W=tensor([0.9704], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0674], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9704], requires_grad=True), W.grad=tensor([-0.0071]), b=tensor([0.0674], requires_grad=True), b.grad=tensor([0.0162])\n",
      "after  step() : W=tensor([0.9704], requires_grad=True), W.grad=tensor([-0.0071]), b=tensor([0.0672], requires_grad=True), b.grad=tensor([0.0162])\n",
      "=======================707==================\n",
      "hypothesis=\n",
      "tensor([[1.0377],\n",
      "        [2.0081],\n",
      "        [2.9785]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006482766475528479\n",
      "before backward() : W=tensor([0.9704], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0672], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9704], requires_grad=True), W.grad=tensor([-0.0071]), b=tensor([0.0672], requires_grad=True), b.grad=tensor([0.0162])\n",
      "after  step() : W=tensor([0.9705], requires_grad=True), W.grad=tensor([-0.0071]), b=tensor([0.0671], requires_grad=True), b.grad=tensor([0.0162])\n",
      "=======================708==================\n",
      "hypothesis=\n",
      "tensor([[1.0376],\n",
      "        [2.0081],\n",
      "        [2.9786]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006451638764701784\n",
      "before backward() : W=tensor([0.9705], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0671], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9705], requires_grad=True), W.grad=tensor([-0.0071]), b=tensor([0.0671], requires_grad=True), b.grad=tensor([0.0161])\n",
      "after  step() : W=tensor([0.9706], requires_grad=True), W.grad=tensor([-0.0071]), b=tensor([0.0669], requires_grad=True), b.grad=tensor([0.0161])\n",
      "=======================709==================\n",
      "hypothesis=\n",
      "tensor([[1.0375],\n",
      "        [2.0080],\n",
      "        [2.9786]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006420680438168347\n",
      "before backward() : W=tensor([0.9706], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0669], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9706], requires_grad=True), W.grad=tensor([-0.0071]), b=tensor([0.0669], requires_grad=True), b.grad=tensor([0.0161])\n",
      "after  step() : W=tensor([0.9706], requires_grad=True), W.grad=tensor([-0.0071]), b=tensor([0.0667], requires_grad=True), b.grad=tensor([0.0161])\n",
      "=======================710==================\n",
      "hypothesis=\n",
      "tensor([[1.0374],\n",
      "        [2.0080],\n",
      "        [2.9787]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006389813497662544\n",
      "before backward() : W=tensor([0.9706], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0667], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9706], requires_grad=True), W.grad=tensor([-0.0071]), b=tensor([0.0667], requires_grad=True), b.grad=tensor([0.0160])\n",
      "after  step() : W=tensor([0.9707], requires_grad=True), W.grad=tensor([-0.0071]), b=tensor([0.0666], requires_grad=True), b.grad=tensor([0.0160])\n",
      "=======================711==================\n",
      "hypothesis=\n",
      "tensor([[1.0373],\n",
      "        [2.0080],\n",
      "        [2.9787]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006359168910421431\n",
      "before backward() : W=tensor([0.9707], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0666], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9707], requires_grad=True), W.grad=tensor([-0.0070]), b=tensor([0.0666], requires_grad=True), b.grad=tensor([0.0160])\n",
      "after  step() : W=tensor([0.9708], requires_grad=True), W.grad=tensor([-0.0070]), b=tensor([0.0664], requires_grad=True), b.grad=tensor([0.0160])\n",
      "=======================712==================\n",
      "hypothesis=\n",
      "tensor([[1.0372],\n",
      "        [2.0080],\n",
      "        [2.9788]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006328585441224277\n",
      "before backward() : W=tensor([0.9708], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0664], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9708], requires_grad=True), W.grad=tensor([-0.0070]), b=tensor([0.0664], requires_grad=True), b.grad=tensor([0.0160])\n",
      "after  step() : W=tensor([0.9709], requires_grad=True), W.grad=tensor([-0.0070]), b=tensor([0.0663], requires_grad=True), b.grad=tensor([0.0160])\n",
      "=======================713==================\n",
      "hypothesis=\n",
      "tensor([[1.0371],\n",
      "        [2.0080],\n",
      "        [2.9788]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006298231310211122\n",
      "before backward() : W=tensor([0.9709], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0663], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9709], requires_grad=True), W.grad=tensor([-0.0070]), b=tensor([0.0663], requires_grad=True), b.grad=tensor([0.0159])\n",
      "after  step() : W=tensor([0.9709], requires_grad=True), W.grad=tensor([-0.0070]), b=tensor([0.0661], requires_grad=True), b.grad=tensor([0.0159])\n",
      "=======================714==================\n",
      "hypothesis=\n",
      "tensor([[1.0370],\n",
      "        [2.0079],\n",
      "        [2.9789]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006267987773753703\n",
      "before backward() : W=tensor([0.9709], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0661], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9709], requires_grad=True), W.grad=tensor([-0.0070]), b=tensor([0.0661], requires_grad=True), b.grad=tensor([0.0159])\n",
      "after  step() : W=tensor([0.9710], requires_grad=True), W.grad=tensor([-0.0070]), b=tensor([0.0659], requires_grad=True), b.grad=tensor([0.0159])\n",
      "=======================715==================\n",
      "hypothesis=\n",
      "tensor([[1.0369],\n",
      "        [2.0079],\n",
      "        [2.9789]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006237880443222821\n",
      "before backward() : W=tensor([0.9710], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0659], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9710], requires_grad=True), W.grad=tensor([-0.0070]), b=tensor([0.0659], requires_grad=True), b.grad=tensor([0.0159])\n",
      "after  step() : W=tensor([0.9711], requires_grad=True), W.grad=tensor([-0.0070]), b=tensor([0.0658], requires_grad=True), b.grad=tensor([0.0159])\n",
      "=======================716==================\n",
      "hypothesis=\n",
      "tensor([[1.0368],\n",
      "        [2.0079],\n",
      "        [2.9790]], grad_fn=<AddBackward0>)\n",
      "loss=0.000620790000539273\n",
      "before backward() : W=tensor([0.9711], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0658], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9711], requires_grad=True), W.grad=tensor([-0.0070]), b=tensor([0.0658], requires_grad=True), b.grad=tensor([0.0158])\n",
      "after  step() : W=tensor([0.9711], requires_grad=True), W.grad=tensor([-0.0070]), b=tensor([0.0656], requires_grad=True), b.grad=tensor([0.0158])\n",
      "=======================717==================\n",
      "hypothesis=\n",
      "tensor([[1.0368],\n",
      "        [2.0079],\n",
      "        [2.9790]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006178120383992791\n",
      "before backward() : W=tensor([0.9711], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0656], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9711], requires_grad=True), W.grad=tensor([-0.0069]), b=tensor([0.0656], requires_grad=True), b.grad=tensor([0.0158])\n",
      "after  step() : W=tensor([0.9712], requires_grad=True), W.grad=tensor([-0.0069]), b=tensor([0.0655], requires_grad=True), b.grad=tensor([0.0158])\n",
      "=======================718==================\n",
      "hypothesis=\n",
      "tensor([[1.0367],\n",
      "        [2.0079],\n",
      "        [2.9791]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006148421089164913\n",
      "before backward() : W=tensor([0.9712], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0655], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9712], requires_grad=True), W.grad=tensor([-0.0069]), b=tensor([0.0655], requires_grad=True), b.grad=tensor([0.0157])\n",
      "after  step() : W=tensor([0.9713], requires_grad=True), W.grad=tensor([-0.0069]), b=tensor([0.0653], requires_grad=True), b.grad=tensor([0.0157])\n",
      "=======================719==================\n",
      "hypothesis=\n",
      "tensor([[1.0366],\n",
      "        [2.0078],\n",
      "        [2.9791]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006118901656009257\n",
      "before backward() : W=tensor([0.9713], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0653], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9713], requires_grad=True), W.grad=tensor([-0.0069]), b=tensor([0.0653], requires_grad=True), b.grad=tensor([0.0157])\n",
      "after  step() : W=tensor([0.9713], requires_grad=True), W.grad=tensor([-0.0069]), b=tensor([0.0652], requires_grad=True), b.grad=tensor([0.0157])\n",
      "=======================720==================\n",
      "hypothesis=\n",
      "tensor([[1.0365],\n",
      "        [2.0078],\n",
      "        [2.9792]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006089527159929276\n",
      "before backward() : W=tensor([0.9713], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0652], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9713], requires_grad=True), W.grad=tensor([-0.0069]), b=tensor([0.0652], requires_grad=True), b.grad=tensor([0.0157])\n",
      "after  step() : W=tensor([0.9714], requires_grad=True), W.grad=tensor([-0.0069]), b=tensor([0.0650], requires_grad=True), b.grad=tensor([0.0157])\n",
      "=======================721==================\n",
      "hypothesis=\n",
      "tensor([[1.0364],\n",
      "        [2.0078],\n",
      "        [2.9792]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006060281302779913\n",
      "before backward() : W=tensor([0.9714], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0650], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9714], requires_grad=True), W.grad=tensor([-0.0069]), b=tensor([0.0650], requires_grad=True), b.grad=tensor([0.0156])\n",
      "after  step() : W=tensor([0.9715], requires_grad=True), W.grad=tensor([-0.0069]), b=tensor([0.0648], requires_grad=True), b.grad=tensor([0.0156])\n",
      "=======================722==================\n",
      "hypothesis=\n",
      "tensor([[1.0363],\n",
      "        [2.0078],\n",
      "        [2.9793]], grad_fn=<AddBackward0>)\n",
      "loss=0.0006031164084561169\n",
      "before backward() : W=tensor([0.9715], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0648], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9715], requires_grad=True), W.grad=tensor([-0.0069]), b=tensor([0.0648], requires_grad=True), b.grad=tensor([0.0156])\n",
      "after  step() : W=tensor([0.9715], requires_grad=True), W.grad=tensor([-0.0069]), b=tensor([0.0647], requires_grad=True), b.grad=tensor([0.0156])\n",
      "=======================723==================\n",
      "hypothesis=\n",
      "tensor([[1.0362],\n",
      "        [2.0078],\n",
      "        [2.9793]], grad_fn=<AddBackward0>)\n",
      "loss=0.000600225233938545\n",
      "before backward() : W=tensor([0.9715], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0647], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9715], requires_grad=True), W.grad=tensor([-0.0068]), b=tensor([0.0647], requires_grad=True), b.grad=tensor([0.0155])\n",
      "after  step() : W=tensor([0.9716], requires_grad=True), W.grad=tensor([-0.0068]), b=tensor([0.0645], requires_grad=True), b.grad=tensor([0.0155])\n",
      "=======================724==================\n",
      "hypothesis=\n",
      "tensor([[1.0361],\n",
      "        [2.0078],\n",
      "        [2.9794]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005973393563181162\n",
      "before backward() : W=tensor([0.9716], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0645], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9716], requires_grad=True), W.grad=tensor([-0.0068]), b=tensor([0.0645], requires_grad=True), b.grad=tensor([0.0155])\n",
      "after  step() : W=tensor([0.9717], requires_grad=True), W.grad=tensor([-0.0068]), b=tensor([0.0644], requires_grad=True), b.grad=tensor([0.0155])\n",
      "=======================725==================\n",
      "hypothesis=\n",
      "tensor([[1.0361],\n",
      "        [2.0077],\n",
      "        [2.9794]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005944707081653178\n",
      "before backward() : W=tensor([0.9717], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0644], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9717], requires_grad=True), W.grad=tensor([-0.0068]), b=tensor([0.0644], requires_grad=True), b.grad=tensor([0.0155])\n",
      "after  step() : W=tensor([0.9718], requires_grad=True), W.grad=tensor([-0.0068]), b=tensor([0.0642], requires_grad=True), b.grad=tensor([0.0155])\n",
      "=======================726==================\n",
      "hypothesis=\n",
      "tensor([[1.0360],\n",
      "        [2.0077],\n",
      "        [2.9795]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005916178924962878\n",
      "before backward() : W=tensor([0.9718], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0642], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9718], requires_grad=True), W.grad=tensor([-0.0068]), b=tensor([0.0642], requires_grad=True), b.grad=tensor([0.0154])\n",
      "after  step() : W=tensor([0.9718], requires_grad=True), W.grad=tensor([-0.0068]), b=tensor([0.0641], requires_grad=True), b.grad=tensor([0.0154])\n",
      "=======================727==================\n",
      "hypothesis=\n",
      "tensor([[1.0359],\n",
      "        [2.0077],\n",
      "        [2.9795]], grad_fn=<AddBackward0>)\n",
      "loss=0.000588774390053004\n",
      "before backward() : W=tensor([0.9718], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0641], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9718], requires_grad=True), W.grad=tensor([-0.0068]), b=tensor([0.0641], requires_grad=True), b.grad=tensor([0.0154])\n",
      "after  step() : W=tensor([0.9719], requires_grad=True), W.grad=tensor([-0.0068]), b=tensor([0.0639], requires_grad=True), b.grad=tensor([0.0154])\n",
      "=======================728==================\n",
      "hypothesis=\n",
      "tensor([[1.0358],\n",
      "        [2.0077],\n",
      "        [2.9796]], grad_fn=<AddBackward0>)\n",
      "loss=0.000585945148486644\n",
      "before backward() : W=tensor([0.9719], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0639], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9719], requires_grad=True), W.grad=tensor([-0.0068]), b=tensor([0.0639], requires_grad=True), b.grad=tensor([0.0154])\n",
      "after  step() : W=tensor([0.9720], requires_grad=True), W.grad=tensor([-0.0068]), b=tensor([0.0638], requires_grad=True), b.grad=tensor([0.0154])\n",
      "=======================729==================\n",
      "hypothesis=\n",
      "tensor([[1.0357],\n",
      "        [2.0077],\n",
      "        [2.9796]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005831363960169256\n",
      "before backward() : W=tensor([0.9720], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0638], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9720], requires_grad=True), W.grad=tensor([-0.0067]), b=tensor([0.0638], requires_grad=True), b.grad=tensor([0.0153])\n",
      "after  step() : W=tensor([0.9720], requires_grad=True), W.grad=tensor([-0.0067]), b=tensor([0.0636], requires_grad=True), b.grad=tensor([0.0153])\n",
      "=======================730==================\n",
      "hypothesis=\n",
      "tensor([[1.0356],\n",
      "        [2.0076],\n",
      "        [2.9797]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005803356179967523\n",
      "before backward() : W=tensor([0.9720], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0636], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9720], requires_grad=True), W.grad=tensor([-0.0067]), b=tensor([0.0636], requires_grad=True), b.grad=tensor([0.0153])\n",
      "after  step() : W=tensor([0.9721], requires_grad=True), W.grad=tensor([-0.0067]), b=tensor([0.0635], requires_grad=True), b.grad=tensor([0.0153])\n",
      "=======================731==================\n",
      "hypothesis=\n",
      "tensor([[1.0355],\n",
      "        [2.0076],\n",
      "        [2.9797]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005775468307547271\n",
      "before backward() : W=tensor([0.9721], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0635], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9721], requires_grad=True), W.grad=tensor([-0.0067]), b=tensor([0.0635], requires_grad=True), b.grad=tensor([0.0153])\n",
      "after  step() : W=tensor([0.9722], requires_grad=True), W.grad=tensor([-0.0067]), b=tensor([0.0633], requires_grad=True), b.grad=tensor([0.0153])\n",
      "=======================732==================\n",
      "hypothesis=\n",
      "tensor([[1.0355],\n",
      "        [2.0076],\n",
      "        [2.9798]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005747736431658268\n",
      "before backward() : W=tensor([0.9722], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0633], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9722], requires_grad=True), W.grad=tensor([-0.0067]), b=tensor([0.0633], requires_grad=True), b.grad=tensor([0.0152])\n",
      "after  step() : W=tensor([0.9722], requires_grad=True), W.grad=tensor([-0.0067]), b=tensor([0.0631], requires_grad=True), b.grad=tensor([0.0152])\n",
      "=======================733==================\n",
      "hypothesis=\n",
      "tensor([[1.0354],\n",
      "        [2.0076],\n",
      "        [2.9798]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005720143672078848\n",
      "before backward() : W=tensor([0.9722], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0631], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9722], requires_grad=True), W.grad=tensor([-0.0067]), b=tensor([0.0631], requires_grad=True), b.grad=tensor([0.0152])\n",
      "after  step() : W=tensor([0.9723], requires_grad=True), W.grad=tensor([-0.0067]), b=tensor([0.0630], requires_grad=True), b.grad=tensor([0.0152])\n",
      "=======================734==================\n",
      "hypothesis=\n",
      "tensor([[1.0353],\n",
      "        [2.0076],\n",
      "        [2.9799]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005692660924978554\n",
      "before backward() : W=tensor([0.9723], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0630], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9723], requires_grad=True), W.grad=tensor([-0.0067]), b=tensor([0.0630], requires_grad=True), b.grad=tensor([0.0151])\n",
      "after  step() : W=tensor([0.9724], requires_grad=True), W.grad=tensor([-0.0067]), b=tensor([0.0628], requires_grad=True), b.grad=tensor([0.0151])\n",
      "=======================735==================\n",
      "hypothesis=\n",
      "tensor([[1.0352],\n",
      "        [2.0076],\n",
      "        [2.9799]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005665348144248128\n",
      "before backward() : W=tensor([0.9724], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0628], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9724], requires_grad=True), W.grad=tensor([-0.0066]), b=tensor([0.0628], requires_grad=True), b.grad=tensor([0.0151])\n",
      "after  step() : W=tensor([0.9724], requires_grad=True), W.grad=tensor([-0.0066]), b=tensor([0.0627], requires_grad=True), b.grad=tensor([0.0151])\n",
      "=======================736==================\n",
      "hypothesis=\n",
      "tensor([[1.0351],\n",
      "        [2.0075],\n",
      "        [2.9800]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005638125003315508\n",
      "before backward() : W=tensor([0.9724], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0627], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9724], requires_grad=True), W.grad=tensor([-0.0066]), b=tensor([0.0627], requires_grad=True), b.grad=tensor([0.0151])\n",
      "after  step() : W=tensor([0.9725], requires_grad=True), W.grad=tensor([-0.0066]), b=tensor([0.0625], requires_grad=True), b.grad=tensor([0.0151])\n",
      "=======================737==================\n",
      "hypothesis=\n",
      "tensor([[1.0350],\n",
      "        [2.0075],\n",
      "        [2.9800]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005611054948531091\n",
      "before backward() : W=tensor([0.9725], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0625], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9725], requires_grad=True), W.grad=tensor([-0.0066]), b=tensor([0.0625], requires_grad=True), b.grad=tensor([0.0150])\n",
      "after  step() : W=tensor([0.9726], requires_grad=True), W.grad=tensor([-0.0066]), b=tensor([0.0624], requires_grad=True), b.grad=tensor([0.0150])\n",
      "=======================738==================\n",
      "hypothesis=\n",
      "tensor([[1.0349],\n",
      "        [2.0075],\n",
      "        [2.9801]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005584105965681374\n",
      "before backward() : W=tensor([0.9726], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0624], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9726], requires_grad=True), W.grad=tensor([-0.0066]), b=tensor([0.0624], requires_grad=True), b.grad=tensor([0.0150])\n",
      "after  step() : W=tensor([0.9726], requires_grad=True), W.grad=tensor([-0.0066]), b=tensor([0.0622], requires_grad=True), b.grad=tensor([0.0150])\n",
      "=======================739==================\n",
      "hypothesis=\n",
      "tensor([[1.0349],\n",
      "        [2.0075],\n",
      "        [2.9801]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005557280383072793\n",
      "before backward() : W=tensor([0.9726], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0622], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9726], requires_grad=True), W.grad=tensor([-0.0066]), b=tensor([0.0622], requires_grad=True), b.grad=tensor([0.0150])\n",
      "after  step() : W=tensor([0.9727], requires_grad=True), W.grad=tensor([-0.0066]), b=tensor([0.0621], requires_grad=True), b.grad=tensor([0.0150])\n",
      "=======================740==================\n",
      "hypothesis=\n",
      "tensor([[1.0348],\n",
      "        [2.0075],\n",
      "        [2.9801]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005530618946067989\n",
      "before backward() : W=tensor([0.9727], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0621], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9727], requires_grad=True), W.grad=tensor([-0.0066]), b=tensor([0.0621], requires_grad=True), b.grad=tensor([0.0149])\n",
      "after  step() : W=tensor([0.9728], requires_grad=True), W.grad=tensor([-0.0066]), b=tensor([0.0619], requires_grad=True), b.grad=tensor([0.0149])\n",
      "=======================741==================\n",
      "hypothesis=\n",
      "tensor([[1.0347],\n",
      "        [2.0074],\n",
      "        [2.9802]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005504044820554554\n",
      "before backward() : W=tensor([0.9728], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0619], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9728], requires_grad=True), W.grad=tensor([-0.0065]), b=tensor([0.0619], requires_grad=True), b.grad=tensor([0.0149])\n",
      "after  step() : W=tensor([0.9728], requires_grad=True), W.grad=tensor([-0.0065]), b=tensor([0.0618], requires_grad=True), b.grad=tensor([0.0149])\n",
      "=======================742==================\n",
      "hypothesis=\n",
      "tensor([[1.0346],\n",
      "        [2.0074],\n",
      "        [2.9802]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005477622034959495\n",
      "before backward() : W=tensor([0.9728], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0618], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9728], requires_grad=True), W.grad=tensor([-0.0065]), b=tensor([0.0618], requires_grad=True), b.grad=tensor([0.0149])\n",
      "after  step() : W=tensor([0.9729], requires_grad=True), W.grad=tensor([-0.0065]), b=tensor([0.0616], requires_grad=True), b.grad=tensor([0.0149])\n",
      "=======================743==================\n",
      "hypothesis=\n",
      "tensor([[1.0345],\n",
      "        [2.0074],\n",
      "        [2.9803]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005451292381621897\n",
      "before backward() : W=tensor([0.9729], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0616], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9729], requires_grad=True), W.grad=tensor([-0.0065]), b=tensor([0.0616], requires_grad=True), b.grad=tensor([0.0148])\n",
      "after  step() : W=tensor([0.9729], requires_grad=True), W.grad=tensor([-0.0065]), b=tensor([0.0615], requires_grad=True), b.grad=tensor([0.0148])\n",
      "=======================744==================\n",
      "hypothesis=\n",
      "tensor([[1.0344],\n",
      "        [2.0074],\n",
      "        [2.9803]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005425140843726695\n",
      "before backward() : W=tensor([0.9729], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0615], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9729], requires_grad=True), W.grad=tensor([-0.0065]), b=tensor([0.0615], requires_grad=True), b.grad=tensor([0.0148])\n",
      "after  step() : W=tensor([0.9730], requires_grad=True), W.grad=tensor([-0.0065]), b=tensor([0.0613], requires_grad=True), b.grad=tensor([0.0148])\n",
      "=======================745==================\n",
      "hypothesis=\n",
      "tensor([[1.0344],\n",
      "        [2.0074],\n",
      "        [2.9804]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005399086512625217\n",
      "before backward() : W=tensor([0.9730], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0613], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9730], requires_grad=True), W.grad=tensor([-0.0065]), b=tensor([0.0613], requires_grad=True), b.grad=tensor([0.0147])\n",
      "after  step() : W=tensor([0.9731], requires_grad=True), W.grad=tensor([-0.0065]), b=tensor([0.0612], requires_grad=True), b.grad=tensor([0.0147])\n",
      "=======================746==================\n",
      "hypothesis=\n",
      "tensor([[1.0343],\n",
      "        [2.0074],\n",
      "        [2.9804]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005373146268539131\n",
      "before backward() : W=tensor([0.9731], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0612], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9731], requires_grad=True), W.grad=tensor([-0.0065]), b=tensor([0.0612], requires_grad=True), b.grad=tensor([0.0147])\n",
      "after  step() : W=tensor([0.9731], requires_grad=True), W.grad=tensor([-0.0065]), b=tensor([0.0611], requires_grad=True), b.grad=tensor([0.0147])\n",
      "=======================747==================\n",
      "hypothesis=\n",
      "tensor([[1.0342],\n",
      "        [2.0073],\n",
      "        [2.9805]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005347361438907683\n",
      "before backward() : W=tensor([0.9731], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0611], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9731], requires_grad=True), W.grad=tensor([-0.0065]), b=tensor([0.0611], requires_grad=True), b.grad=tensor([0.0147])\n",
      "after  step() : W=tensor([0.9732], requires_grad=True), W.grad=tensor([-0.0065]), b=tensor([0.0609], requires_grad=True), b.grad=tensor([0.0147])\n",
      "=======================748==================\n",
      "hypothesis=\n",
      "tensor([[1.0341],\n",
      "        [2.0073],\n",
      "        [2.9805]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005321661592461169\n",
      "before backward() : W=tensor([0.9732], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0609], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9732], requires_grad=True), W.grad=tensor([-0.0064]), b=tensor([0.0609], requires_grad=True), b.grad=tensor([0.0146])\n",
      "after  step() : W=tensor([0.9733], requires_grad=True), W.grad=tensor([-0.0064]), b=tensor([0.0608], requires_grad=True), b.grad=tensor([0.0146])\n",
      "=======================749==================\n",
      "hypothesis=\n",
      "tensor([[1.0340],\n",
      "        [2.0073],\n",
      "        [2.9806]], grad_fn=<AddBackward0>)\n",
      "loss=0.00052960857283324\n",
      "before backward() : W=tensor([0.9733], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0608], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9733], requires_grad=True), W.grad=tensor([-0.0064]), b=tensor([0.0608], requires_grad=True), b.grad=tensor([0.0146])\n",
      "after  step() : W=tensor([0.9733], requires_grad=True), W.grad=tensor([-0.0064]), b=tensor([0.0606], requires_grad=True), b.grad=tensor([0.0146])\n",
      "=======================750==================\n",
      "hypothesis=\n",
      "tensor([[1.0339],\n",
      "        [2.0073],\n",
      "        [2.9806]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005270683323033154\n",
      "before backward() : W=tensor([0.9733], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0606], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9733], requires_grad=True), W.grad=tensor([-0.0064]), b=tensor([0.0606], requires_grad=True), b.grad=tensor([0.0146])\n",
      "after  step() : W=tensor([0.9734], requires_grad=True), W.grad=tensor([-0.0064]), b=tensor([0.0605], requires_grad=True), b.grad=tensor([0.0146])\n",
      "=======================751==================\n",
      "hypothesis=\n",
      "tensor([[1.0339],\n",
      "        [2.0073],\n",
      "        [2.9807]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005245367647148669\n",
      "before backward() : W=tensor([0.9734], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0605], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9734], requires_grad=True), W.grad=tensor([-0.0064]), b=tensor([0.0605], requires_grad=True), b.grad=tensor([0.0145])\n",
      "after  step() : W=tensor([0.9735], requires_grad=True), W.grad=tensor([-0.0064]), b=tensor([0.0603], requires_grad=True), b.grad=tensor([0.0145])\n",
      "=======================752==================\n",
      "hypothesis=\n",
      "tensor([[1.0338],\n",
      "        [2.0073],\n",
      "        [2.9807]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005220180028118193\n",
      "before backward() : W=tensor([0.9735], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0603], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9735], requires_grad=True), W.grad=tensor([-0.0064]), b=tensor([0.0603], requires_grad=True), b.grad=tensor([0.0145])\n",
      "after  step() : W=tensor([0.9735], requires_grad=True), W.grad=tensor([-0.0064]), b=tensor([0.0602], requires_grad=True), b.grad=tensor([0.0145])\n",
      "=======================753==================\n",
      "hypothesis=\n",
      "tensor([[1.0337],\n",
      "        [2.0072],\n",
      "        [2.9808]], grad_fn=<AddBackward0>)\n",
      "loss=0.000519510533194989\n",
      "before backward() : W=tensor([0.9735], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0602], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9735], requires_grad=True), W.grad=tensor([-0.0064]), b=tensor([0.0602], requires_grad=True), b.grad=tensor([0.0145])\n",
      "after  step() : W=tensor([0.9736], requires_grad=True), W.grad=tensor([-0.0064]), b=tensor([0.0600], requires_grad=True), b.grad=tensor([0.0145])\n",
      "=======================754==================\n",
      "hypothesis=\n",
      "tensor([[1.0336],\n",
      "        [2.0072],\n",
      "        [2.9808]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005170179065316916\n",
      "before backward() : W=tensor([0.9736], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0600], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9736], requires_grad=True), W.grad=tensor([-0.0063]), b=tensor([0.0600], requires_grad=True), b.grad=tensor([0.0144])\n",
      "after  step() : W=tensor([0.9737], requires_grad=True), W.grad=tensor([-0.0063]), b=tensor([0.0599], requires_grad=True), b.grad=tensor([0.0144])\n",
      "=======================755==================\n",
      "hypothesis=\n",
      "tensor([[1.0335],\n",
      "        [2.0072],\n",
      "        [2.9809]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005145335453562438\n",
      "before backward() : W=tensor([0.9737], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0599], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9737], requires_grad=True), W.grad=tensor([-0.0063]), b=tensor([0.0599], requires_grad=True), b.grad=tensor([0.0144])\n",
      "after  step() : W=tensor([0.9737], requires_grad=True), W.grad=tensor([-0.0063]), b=tensor([0.0597], requires_grad=True), b.grad=tensor([0.0144])\n",
      "=======================756==================\n",
      "hypothesis=\n",
      "tensor([[1.0335],\n",
      "        [2.0072],\n",
      "        [2.9809]], grad_fn=<AddBackward0>)\n",
      "loss=0.00051206472562626\n",
      "before backward() : W=tensor([0.9737], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0597], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9737], requires_grad=True), W.grad=tensor([-0.0063]), b=tensor([0.0597], requires_grad=True), b.grad=tensor([0.0144])\n",
      "after  step() : W=tensor([0.9738], requires_grad=True), W.grad=tensor([-0.0063]), b=tensor([0.0596], requires_grad=True), b.grad=tensor([0.0144])\n",
      "=======================757==================\n",
      "hypothesis=\n",
      "tensor([[1.0334],\n",
      "        [2.0072],\n",
      "        [2.9809]], grad_fn=<AddBackward0>)\n",
      "loss=0.000509601435624063\n",
      "before backward() : W=tensor([0.9738], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0596], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9738], requires_grad=True), W.grad=tensor([-0.0063]), b=tensor([0.0596], requires_grad=True), b.grad=tensor([0.0143])\n",
      "after  step() : W=tensor([0.9738], requires_grad=True), W.grad=tensor([-0.0063]), b=tensor([0.0595], requires_grad=True), b.grad=tensor([0.0143])\n",
      "=======================758==================\n",
      "hypothesis=\n",
      "tensor([[1.0333],\n",
      "        [2.0071],\n",
      "        [2.9810]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005071572959423065\n",
      "before backward() : W=tensor([0.9738], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0595], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9738], requires_grad=True), W.grad=tensor([-0.0063]), b=tensor([0.0595], requires_grad=True), b.grad=tensor([0.0143])\n",
      "after  step() : W=tensor([0.9739], requires_grad=True), W.grad=tensor([-0.0063]), b=tensor([0.0593], requires_grad=True), b.grad=tensor([0.0143])\n",
      "=======================759==================\n",
      "hypothesis=\n",
      "tensor([[1.0332],\n",
      "        [2.0071],\n",
      "        [2.9810]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005047197919338942\n",
      "before backward() : W=tensor([0.9739], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0593], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9739], requires_grad=True), W.grad=tensor([-0.0063]), b=tensor([0.0593], requires_grad=True), b.grad=tensor([0.0143])\n",
      "after  step() : W=tensor([0.9740], requires_grad=True), W.grad=tensor([-0.0063]), b=tensor([0.0592], requires_grad=True), b.grad=tensor([0.0143])\n",
      "=======================760==================\n",
      "hypothesis=\n",
      "tensor([[1.0331],\n",
      "        [2.0071],\n",
      "        [2.9811]], grad_fn=<AddBackward0>)\n",
      "loss=0.0005022994591854513\n",
      "before backward() : W=tensor([0.9740], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0592], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9740], requires_grad=True), W.grad=tensor([-0.0063]), b=tensor([0.0592], requires_grad=True), b.grad=tensor([0.0142])\n",
      "after  step() : W=tensor([0.9740], requires_grad=True), W.grad=tensor([-0.0063]), b=tensor([0.0590], requires_grad=True), b.grad=tensor([0.0142])\n",
      "=======================761==================\n",
      "hypothesis=\n",
      "tensor([[1.0331],\n",
      "        [2.0071],\n",
      "        [2.9811]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004998852382414043\n",
      "before backward() : W=tensor([0.9740], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0590], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9740], requires_grad=True), W.grad=tensor([-0.0062]), b=tensor([0.0590], requires_grad=True), b.grad=tensor([0.0142])\n",
      "after  step() : W=tensor([0.9741], requires_grad=True), W.grad=tensor([-0.0062]), b=tensor([0.0589], requires_grad=True), b.grad=tensor([0.0142])\n",
      "=======================762==================\n",
      "hypothesis=\n",
      "tensor([[1.0330],\n",
      "        [2.0071],\n",
      "        [2.9812]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004974874318577349\n",
      "before backward() : W=tensor([0.9741], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0589], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9741], requires_grad=True), W.grad=tensor([-0.0062]), b=tensor([0.0589], requires_grad=True), b.grad=tensor([0.0142])\n",
      "after  step() : W=tensor([0.9742], requires_grad=True), W.grad=tensor([-0.0062]), b=tensor([0.0587], requires_grad=True), b.grad=tensor([0.0142])\n",
      "=======================763==================\n",
      "hypothesis=\n",
      "tensor([[1.0329],\n",
      "        [2.0071],\n",
      "        [2.9812]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004950956790708005\n",
      "before backward() : W=tensor([0.9742], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0587], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9742], requires_grad=True), W.grad=tensor([-0.0062]), b=tensor([0.0587], requires_grad=True), b.grad=tensor([0.0141])\n",
      "after  step() : W=tensor([0.9742], requires_grad=True), W.grad=tensor([-0.0062]), b=tensor([0.0586], requires_grad=True), b.grad=tensor([0.0141])\n",
      "=======================764==================\n",
      "hypothesis=\n",
      "tensor([[1.0328],\n",
      "        [2.0070],\n",
      "        [2.9813]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004927227855660021\n",
      "before backward() : W=tensor([0.9742], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0586], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9742], requires_grad=True), W.grad=tensor([-0.0062]), b=tensor([0.0586], requires_grad=True), b.grad=tensor([0.0141])\n",
      "after  step() : W=tensor([0.9743], requires_grad=True), W.grad=tensor([-0.0062]), b=tensor([0.0585], requires_grad=True), b.grad=tensor([0.0141])\n",
      "=======================765==================\n",
      "hypothesis=\n",
      "tensor([[1.0327],\n",
      "        [2.0070],\n",
      "        [2.9813]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004903562949039042\n",
      "before backward() : W=tensor([0.9743], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0585], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9743], requires_grad=True), W.grad=tensor([-0.0062]), b=tensor([0.0585], requires_grad=True), b.grad=tensor([0.0141])\n",
      "after  step() : W=tensor([0.9743], requires_grad=True), W.grad=tensor([-0.0062]), b=tensor([0.0583], requires_grad=True), b.grad=tensor([0.0141])\n",
      "=======================766==================\n",
      "hypothesis=\n",
      "tensor([[1.0327],\n",
      "        [2.0070],\n",
      "        [2.9814]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004879981279373169\n",
      "before backward() : W=tensor([0.9743], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0583], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9743], requires_grad=True), W.grad=tensor([-0.0062]), b=tensor([0.0583], requires_grad=True), b.grad=tensor([0.0140])\n",
      "after  step() : W=tensor([0.9744], requires_grad=True), W.grad=tensor([-0.0062]), b=tensor([0.0582], requires_grad=True), b.grad=tensor([0.0140])\n",
      "=======================767==================\n",
      "hypothesis=\n",
      "tensor([[1.0326],\n",
      "        [2.0070],\n",
      "        [2.9814]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004856574523728341\n",
      "before backward() : W=tensor([0.9744], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0582], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9744], requires_grad=True), W.grad=tensor([-0.0062]), b=tensor([0.0582], requires_grad=True), b.grad=tensor([0.0140])\n",
      "after  step() : W=tensor([0.9745], requires_grad=True), W.grad=tensor([-0.0062]), b=tensor([0.0580], requires_grad=True), b.grad=tensor([0.0140])\n",
      "=======================768==================\n",
      "hypothesis=\n",
      "tensor([[1.0325],\n",
      "        [2.0070],\n",
      "        [2.9814]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004833242855966091\n",
      "before backward() : W=tensor([0.9745], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0580], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9745], requires_grad=True), W.grad=tensor([-0.0061]), b=tensor([0.0580], requires_grad=True), b.grad=tensor([0.0140])\n",
      "after  step() : W=tensor([0.9745], requires_grad=True), W.grad=tensor([-0.0061]), b=tensor([0.0579], requires_grad=True), b.grad=tensor([0.0140])\n",
      "=======================769==================\n",
      "hypothesis=\n",
      "tensor([[1.0324],\n",
      "        [2.0070],\n",
      "        [2.9815]], grad_fn=<AddBackward0>)\n",
      "loss=0.00048100261483341455\n",
      "before backward() : W=tensor([0.9745], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0579], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9745], requires_grad=True), W.grad=tensor([-0.0061]), b=tensor([0.0579], requires_grad=True), b.grad=tensor([0.0139])\n",
      "after  step() : W=tensor([0.9746], requires_grad=True), W.grad=tensor([-0.0061]), b=tensor([0.0578], requires_grad=True), b.grad=tensor([0.0139])\n",
      "=======================770==================\n",
      "hypothesis=\n",
      "tensor([[1.0324],\n",
      "        [2.0069],\n",
      "        [2.9815]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004786949430126697\n",
      "before backward() : W=tensor([0.9746], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0578], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9746], requires_grad=True), W.grad=tensor([-0.0061]), b=tensor([0.0578], requires_grad=True), b.grad=tensor([0.0139])\n",
      "after  step() : W=tensor([0.9746], requires_grad=True), W.grad=tensor([-0.0061]), b=tensor([0.0576], requires_grad=True), b.grad=tensor([0.0139])\n",
      "=======================771==================\n",
      "hypothesis=\n",
      "tensor([[1.0323],\n",
      "        [2.0069],\n",
      "        [2.9816]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004763925389852375\n",
      "before backward() : W=tensor([0.9746], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0576], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9746], requires_grad=True), W.grad=tensor([-0.0061]), b=tensor([0.0576], requires_grad=True), b.grad=tensor([0.0139])\n",
      "after  step() : W=tensor([0.9747], requires_grad=True), W.grad=tensor([-0.0061]), b=tensor([0.0575], requires_grad=True), b.grad=tensor([0.0139])\n",
      "=======================772==================\n",
      "hypothesis=\n",
      "tensor([[1.0322],\n",
      "        [2.0069],\n",
      "        [2.9816]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004741066077258438\n",
      "before backward() : W=tensor([0.9747], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0575], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9747], requires_grad=True), W.grad=tensor([-0.0061]), b=tensor([0.0575], requires_grad=True), b.grad=tensor([0.0138])\n",
      "after  step() : W=tensor([0.9748], requires_grad=True), W.grad=tensor([-0.0061]), b=tensor([0.0573], requires_grad=True), b.grad=tensor([0.0138])\n",
      "=======================773==================\n",
      "hypothesis=\n",
      "tensor([[1.0321],\n",
      "        [2.0069],\n",
      "        [2.9817]], grad_fn=<AddBackward0>)\n",
      "loss=0.00047182830167002976\n",
      "before backward() : W=tensor([0.9748], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0573], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9748], requires_grad=True), W.grad=tensor([-0.0061]), b=tensor([0.0573], requires_grad=True), b.grad=tensor([0.0138])\n",
      "after  step() : W=tensor([0.9748], requires_grad=True), W.grad=tensor([-0.0061]), b=tensor([0.0572], requires_grad=True), b.grad=tensor([0.0138])\n",
      "=======================774==================\n",
      "hypothesis=\n",
      "tensor([[1.0320],\n",
      "        [2.0069],\n",
      "        [2.9817]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004695620446000248\n",
      "before backward() : W=tensor([0.9748], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0572], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9748], requires_grad=True), W.grad=tensor([-0.0061]), b=tensor([0.0572], requires_grad=True), b.grad=tensor([0.0138])\n",
      "after  step() : W=tensor([0.9749], requires_grad=True), W.grad=tensor([-0.0061]), b=tensor([0.0571], requires_grad=True), b.grad=tensor([0.0138])\n",
      "=======================775==================\n",
      "hypothesis=\n",
      "tensor([[1.0320],\n",
      "        [2.0069],\n",
      "        [2.9818]], grad_fn=<AddBackward0>)\n",
      "loss=0.00046730818576179445\n",
      "before backward() : W=tensor([0.9749], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0571], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9749], requires_grad=True), W.grad=tensor([-0.0060]), b=tensor([0.0571], requires_grad=True), b.grad=tensor([0.0137])\n",
      "after  step() : W=tensor([0.9750], requires_grad=True), W.grad=tensor([-0.0060]), b=tensor([0.0569], requires_grad=True), b.grad=tensor([0.0137])\n",
      "=======================776==================\n",
      "hypothesis=\n",
      "tensor([[1.0319],\n",
      "        [2.0068],\n",
      "        [2.9818]], grad_fn=<AddBackward0>)\n",
      "loss=0.00046506591024808586\n",
      "before backward() : W=tensor([0.9750], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0569], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9750], requires_grad=True), W.grad=tensor([-0.0060]), b=tensor([0.0569], requires_grad=True), b.grad=tensor([0.0137])\n",
      "after  step() : W=tensor([0.9750], requires_grad=True), W.grad=tensor([-0.0060]), b=tensor([0.0568], requires_grad=True), b.grad=tensor([0.0137])\n",
      "=======================777==================\n",
      "hypothesis=\n",
      "tensor([[1.0318],\n",
      "        [2.0068],\n",
      "        [2.9818]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004628315509762615\n",
      "before backward() : W=tensor([0.9750], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0568], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9750], requires_grad=True), W.grad=tensor([-0.0060]), b=tensor([0.0568], requires_grad=True), b.grad=tensor([0.0137])\n",
      "after  step() : W=tensor([0.9751], requires_grad=True), W.grad=tensor([-0.0060]), b=tensor([0.0567], requires_grad=True), b.grad=tensor([0.0137])\n",
      "=======================778==================\n",
      "hypothesis=\n",
      "tensor([[1.0317],\n",
      "        [2.0068],\n",
      "        [2.9819]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004606061556842178\n",
      "before backward() : W=tensor([0.9751], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0567], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9751], requires_grad=True), W.grad=tensor([-0.0060]), b=tensor([0.0567], requires_grad=True), b.grad=tensor([0.0136])\n",
      "after  step() : W=tensor([0.9751], requires_grad=True), W.grad=tensor([-0.0060]), b=tensor([0.0565], requires_grad=True), b.grad=tensor([0.0136])\n",
      "=======================779==================\n",
      "hypothesis=\n",
      "tensor([[1.0317],\n",
      "        [2.0068],\n",
      "        [2.9819]], grad_fn=<AddBackward0>)\n",
      "loss=0.00045839868835173547\n",
      "before backward() : W=tensor([0.9751], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0565], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9751], requires_grad=True), W.grad=tensor([-0.0060]), b=tensor([0.0565], requires_grad=True), b.grad=tensor([0.0136])\n",
      "after  step() : W=tensor([0.9752], requires_grad=True), W.grad=tensor([-0.0060]), b=tensor([0.0564], requires_grad=True), b.grad=tensor([0.0136])\n",
      "=======================780==================\n",
      "hypothesis=\n",
      "tensor([[1.0316],\n",
      "        [2.0068],\n",
      "        [2.9820]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004561958194244653\n",
      "before backward() : W=tensor([0.9752], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0564], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9752], requires_grad=True), W.grad=tensor([-0.0060]), b=tensor([0.0564], requires_grad=True), b.grad=tensor([0.0136])\n",
      "after  step() : W=tensor([0.9753], requires_grad=True), W.grad=tensor([-0.0060]), b=tensor([0.0563], requires_grad=True), b.grad=tensor([0.0136])\n",
      "=======================781==================\n",
      "hypothesis=\n",
      "tensor([[1.0315],\n",
      "        [2.0068],\n",
      "        [2.9820]], grad_fn=<AddBackward0>)\n",
      "loss=0.00045400470844469965\n",
      "before backward() : W=tensor([0.9753], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0563], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9753], requires_grad=True), W.grad=tensor([-0.0059]), b=tensor([0.0563], requires_grad=True), b.grad=tensor([0.0135])\n",
      "after  step() : W=tensor([0.9753], requires_grad=True), W.grad=tensor([-0.0059]), b=tensor([0.0561], requires_grad=True), b.grad=tensor([0.0135])\n",
      "=======================782==================\n",
      "hypothesis=\n",
      "tensor([[1.0314],\n",
      "        [2.0067],\n",
      "        [2.9821]], grad_fn=<AddBackward0>)\n",
      "loss=0.00045182384201325476\n",
      "before backward() : W=tensor([0.9753], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0561], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9753], requires_grad=True), W.grad=tensor([-0.0059]), b=tensor([0.0561], requires_grad=True), b.grad=tensor([0.0135])\n",
      "after  step() : W=tensor([0.9754], requires_grad=True), W.grad=tensor([-0.0059]), b=tensor([0.0560], requires_grad=True), b.grad=tensor([0.0135])\n",
      "=======================783==================\n",
      "hypothesis=\n",
      "tensor([[1.0314],\n",
      "        [2.0067],\n",
      "        [2.9821]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004496535984799266\n",
      "before backward() : W=tensor([0.9754], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0560], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9754], requires_grad=True), W.grad=tensor([-0.0059]), b=tensor([0.0560], requires_grad=True), b.grad=tensor([0.0135])\n",
      "after  step() : W=tensor([0.9754], requires_grad=True), W.grad=tensor([-0.0059]), b=tensor([0.0559], requires_grad=True), b.grad=tensor([0.0135])\n",
      "=======================784==================\n",
      "hypothesis=\n",
      "tensor([[1.0313],\n",
      "        [2.0067],\n",
      "        [2.9821]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004474966845009476\n",
      "before backward() : W=tensor([0.9754], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0559], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9754], requires_grad=True), W.grad=tensor([-0.0059]), b=tensor([0.0559], requires_grad=True), b.grad=tensor([0.0134])\n",
      "after  step() : W=tensor([0.9755], requires_grad=True), W.grad=tensor([-0.0059]), b=tensor([0.0557], requires_grad=True), b.grad=tensor([0.0134])\n",
      "=======================785==================\n",
      "hypothesis=\n",
      "tensor([[1.0312],\n",
      "        [2.0067],\n",
      "        [2.9822]], grad_fn=<AddBackward0>)\n",
      "loss=0.00044534538756124675\n",
      "before backward() : W=tensor([0.9755], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0557], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9755], requires_grad=True), W.grad=tensor([-0.0059]), b=tensor([0.0557], requires_grad=True), b.grad=tensor([0.0134])\n",
      "after  step() : W=tensor([0.9755], requires_grad=True), W.grad=tensor([-0.0059]), b=tensor([0.0556], requires_grad=True), b.grad=tensor([0.0134])\n",
      "=======================786==================\n",
      "hypothesis=\n",
      "tensor([[1.0311],\n",
      "        [2.0067],\n",
      "        [2.9822]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004432087589520961\n",
      "before backward() : W=tensor([0.9755], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0556], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9755], requires_grad=True), W.grad=tensor([-0.0059]), b=tensor([0.0556], requires_grad=True), b.grad=tensor([0.0134])\n",
      "after  step() : W=tensor([0.9756], requires_grad=True), W.grad=tensor([-0.0059]), b=tensor([0.0554], requires_grad=True), b.grad=tensor([0.0134])\n",
      "=======================787==================\n",
      "hypothesis=\n",
      "tensor([[1.0311],\n",
      "        [2.0067],\n",
      "        [2.9823]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004410797555465251\n",
      "before backward() : W=tensor([0.9756], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0554], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9756], requires_grad=True), W.grad=tensor([-0.0059]), b=tensor([0.0554], requires_grad=True), b.grad=tensor([0.0133])\n",
      "after  step() : W=tensor([0.9757], requires_grad=True), W.grad=tensor([-0.0059]), b=tensor([0.0553], requires_grad=True), b.grad=tensor([0.0133])\n",
      "=======================788==================\n",
      "hypothesis=\n",
      "tensor([[1.0310],\n",
      "        [2.0066],\n",
      "        [2.9823]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004389618698041886\n",
      "before backward() : W=tensor([0.9757], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0553], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9757], requires_grad=True), W.grad=tensor([-0.0058]), b=tensor([0.0553], requires_grad=True), b.grad=tensor([0.0133])\n",
      "after  step() : W=tensor([0.9757], requires_grad=True), W.grad=tensor([-0.0058]), b=tensor([0.0552], requires_grad=True), b.grad=tensor([0.0133])\n",
      "=======================789==================\n",
      "hypothesis=\n",
      "tensor([[1.0309],\n",
      "        [2.0066],\n",
      "        [2.9824]], grad_fn=<AddBackward0>)\n",
      "loss=0.00043685431592166424\n",
      "before backward() : W=tensor([0.9757], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0552], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9757], requires_grad=True), W.grad=tensor([-0.0058]), b=tensor([0.0552], requires_grad=True), b.grad=tensor([0.0133])\n",
      "after  step() : W=tensor([0.9758], requires_grad=True), W.grad=tensor([-0.0058]), b=tensor([0.0551], requires_grad=True), b.grad=tensor([0.0133])\n",
      "=======================790==================\n",
      "hypothesis=\n",
      "tensor([[1.0308],\n",
      "        [2.0066],\n",
      "        [2.9824]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004347567446529865\n",
      "before backward() : W=tensor([0.9758], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0551], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9758], requires_grad=True), W.grad=tensor([-0.0058]), b=tensor([0.0551], requires_grad=True), b.grad=tensor([0.0132])\n",
      "after  step() : W=tensor([0.9758], requires_grad=True), W.grad=tensor([-0.0058]), b=tensor([0.0549], requires_grad=True), b.grad=tensor([0.0132])\n",
      "=======================791==================\n",
      "hypothesis=\n",
      "tensor([[1.0308],\n",
      "        [2.0066],\n",
      "        [2.9824]], grad_fn=<AddBackward0>)\n",
      "loss=0.00043266950524412096\n",
      "before backward() : W=tensor([0.9758], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0549], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9758], requires_grad=True), W.grad=tensor([-0.0058]), b=tensor([0.0549], requires_grad=True), b.grad=tensor([0.0132])\n",
      "after  step() : W=tensor([0.9759], requires_grad=True), W.grad=tensor([-0.0058]), b=tensor([0.0548], requires_grad=True), b.grad=tensor([0.0132])\n",
      "=======================792==================\n",
      "hypothesis=\n",
      "tensor([[1.0307],\n",
      "        [2.0066],\n",
      "        [2.9825]], grad_fn=<AddBackward0>)\n",
      "loss=0.000430587911978364\n",
      "before backward() : W=tensor([0.9759], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0548], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9759], requires_grad=True), W.grad=tensor([-0.0058]), b=tensor([0.0548], requires_grad=True), b.grad=tensor([0.0132])\n",
      "after  step() : W=tensor([0.9760], requires_grad=True), W.grad=tensor([-0.0058]), b=tensor([0.0547], requires_grad=True), b.grad=tensor([0.0132])\n",
      "=======================793==================\n",
      "hypothesis=\n",
      "tensor([[1.0306],\n",
      "        [2.0066],\n",
      "        [2.9825]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004285259637981653\n",
      "before backward() : W=tensor([0.9760], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0547], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9760], requires_grad=True), W.grad=tensor([-0.0058]), b=tensor([0.0547], requires_grad=True), b.grad=tensor([0.0131])\n",
      "after  step() : W=tensor([0.9760], requires_grad=True), W.grad=tensor([-0.0058]), b=tensor([0.0545], requires_grad=True), b.grad=tensor([0.0131])\n",
      "=======================794==================\n",
      "hypothesis=\n",
      "tensor([[1.0305],\n",
      "        [2.0066],\n",
      "        [2.9826]], grad_fn=<AddBackward0>)\n",
      "loss=0.00042646683868952096\n",
      "before backward() : W=tensor([0.9760], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0545], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9760], requires_grad=True), W.grad=tensor([-0.0058]), b=tensor([0.0545], requires_grad=True), b.grad=tensor([0.0131])\n",
      "after  step() : W=tensor([0.9761], requires_grad=True), W.grad=tensor([-0.0058]), b=tensor([0.0544], requires_grad=True), b.grad=tensor([0.0131])\n",
      "=======================795==================\n",
      "hypothesis=\n",
      "tensor([[1.0305],\n",
      "        [2.0065],\n",
      "        [2.9826]], grad_fn=<AddBackward0>)\n",
      "loss=0.00042441789992153645\n",
      "before backward() : W=tensor([0.9761], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0544], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9761], requires_grad=True), W.grad=tensor([-0.0058]), b=tensor([0.0544], requires_grad=True), b.grad=tensor([0.0131])\n",
      "after  step() : W=tensor([0.9761], requires_grad=True), W.grad=tensor([-0.0058]), b=tensor([0.0543], requires_grad=True), b.grad=tensor([0.0131])\n",
      "=======================796==================\n",
      "hypothesis=\n",
      "tensor([[1.0304],\n",
      "        [2.0065],\n",
      "        [2.9827]], grad_fn=<AddBackward0>)\n",
      "loss=0.00042237911839038134\n",
      "before backward() : W=tensor([0.9761], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0543], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9761], requires_grad=True), W.grad=tensor([-0.0057]), b=tensor([0.0543], requires_grad=True), b.grad=tensor([0.0130])\n",
      "after  step() : W=tensor([0.9762], requires_grad=True), W.grad=tensor([-0.0057]), b=tensor([0.0541], requires_grad=True), b.grad=tensor([0.0130])\n",
      "=======================797==================\n",
      "hypothesis=\n",
      "tensor([[1.0303],\n",
      "        [2.0065],\n",
      "        [2.9827]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004203534626867622\n",
      "before backward() : W=tensor([0.9762], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0541], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9762], requires_grad=True), W.grad=tensor([-0.0057]), b=tensor([0.0541], requires_grad=True), b.grad=tensor([0.0130])\n",
      "after  step() : W=tensor([0.9762], requires_grad=True), W.grad=tensor([-0.0057]), b=tensor([0.0540], requires_grad=True), b.grad=tensor([0.0130])\n",
      "=======================798==================\n",
      "hypothesis=\n",
      "tensor([[1.0302],\n",
      "        [2.0065],\n",
      "        [2.9827]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004183343844488263\n",
      "before backward() : W=tensor([0.9762], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0540], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9762], requires_grad=True), W.grad=tensor([-0.0057]), b=tensor([0.0540], requires_grad=True), b.grad=tensor([0.0130])\n",
      "after  step() : W=tensor([0.9763], requires_grad=True), W.grad=tensor([-0.0057]), b=tensor([0.0539], requires_grad=True), b.grad=tensor([0.0130])\n",
      "=======================799==================\n",
      "hypothesis=\n",
      "tensor([[1.0302],\n",
      "        [2.0065],\n",
      "        [2.9828]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004163222329225391\n",
      "before backward() : W=tensor([0.9763], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0539], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9763], requires_grad=True), W.grad=tensor([-0.0057]), b=tensor([0.0539], requires_grad=True), b.grad=tensor([0.0130])\n",
      "after  step() : W=tensor([0.9764], requires_grad=True), W.grad=tensor([-0.0057]), b=tensor([0.0537], requires_grad=True), b.grad=tensor([0.0130])\n",
      "=======================800==================\n",
      "hypothesis=\n",
      "tensor([[1.0301],\n",
      "        [2.0065],\n",
      "        [2.9828]], grad_fn=<AddBackward0>)\n",
      "loss=0.00041432344005443156\n",
      "before backward() : W=tensor([0.9764], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0537], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9764], requires_grad=True), W.grad=tensor([-0.0057]), b=tensor([0.0537], requires_grad=True), b.grad=tensor([0.0129])\n",
      "after  step() : W=tensor([0.9764], requires_grad=True), W.grad=tensor([-0.0057]), b=tensor([0.0536], requires_grad=True), b.grad=tensor([0.0129])\n",
      "=======================801==================\n",
      "hypothesis=\n",
      "tensor([[1.0300],\n",
      "        [2.0064],\n",
      "        [2.9829]], grad_fn=<AddBackward0>)\n",
      "loss=0.00041233221418224275\n",
      "before backward() : W=tensor([0.9764], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0536], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9764], requires_grad=True), W.grad=tensor([-0.0057]), b=tensor([0.0536], requires_grad=True), b.grad=tensor([0.0129])\n",
      "after  step() : W=tensor([0.9765], requires_grad=True), W.grad=tensor([-0.0057]), b=tensor([0.0535], requires_grad=True), b.grad=tensor([0.0129])\n",
      "=======================802==================\n",
      "hypothesis=\n",
      "tensor([[1.0300],\n",
      "        [2.0064],\n",
      "        [2.9829]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004103556275367737\n",
      "before backward() : W=tensor([0.9765], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0535], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9765], requires_grad=True), W.grad=tensor([-0.0057]), b=tensor([0.0535], requires_grad=True), b.grad=tensor([0.0129])\n",
      "after  step() : W=tensor([0.9765], requires_grad=True), W.grad=tensor([-0.0057]), b=tensor([0.0534], requires_grad=True), b.grad=tensor([0.0129])\n",
      "=======================803==================\n",
      "hypothesis=\n",
      "tensor([[1.0299],\n",
      "        [2.0064],\n",
      "        [2.9829]], grad_fn=<AddBackward0>)\n",
      "loss=0.00040838620043359697\n",
      "before backward() : W=tensor([0.9765], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0534], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9765], requires_grad=True), W.grad=tensor([-0.0056]), b=tensor([0.0534], requires_grad=True), b.grad=tensor([0.0128])\n",
      "after  step() : W=tensor([0.9766], requires_grad=True), W.grad=tensor([-0.0056]), b=tensor([0.0532], requires_grad=True), b.grad=tensor([0.0128])\n",
      "=======================804==================\n",
      "hypothesis=\n",
      "tensor([[1.0298],\n",
      "        [2.0064],\n",
      "        [2.9830]], grad_fn=<AddBackward0>)\n",
      "loss=0.00040642113890498877\n",
      "before backward() : W=tensor([0.9766], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0532], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9766], requires_grad=True), W.grad=tensor([-0.0056]), b=tensor([0.0532], requires_grad=True), b.grad=tensor([0.0128])\n",
      "after  step() : W=tensor([0.9766], requires_grad=True), W.grad=tensor([-0.0056]), b=tensor([0.0531], requires_grad=True), b.grad=tensor([0.0128])\n",
      "=======================805==================\n",
      "hypothesis=\n",
      "tensor([[1.0297],\n",
      "        [2.0064],\n",
      "        [2.9830]], grad_fn=<AddBackward0>)\n",
      "loss=0.0004044719971716404\n",
      "before backward() : W=tensor([0.9766], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0531], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9766], requires_grad=True), W.grad=tensor([-0.0056]), b=tensor([0.0531], requires_grad=True), b.grad=tensor([0.0128])\n",
      "after  step() : W=tensor([0.9767], requires_grad=True), W.grad=tensor([-0.0056]), b=tensor([0.0530], requires_grad=True), b.grad=tensor([0.0128])\n",
      "=======================806==================\n",
      "hypothesis=\n",
      "tensor([[1.0297],\n",
      "        [2.0064],\n",
      "        [2.9831]], grad_fn=<AddBackward0>)\n",
      "loss=0.00040252882172353566\n",
      "before backward() : W=tensor([0.9767], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0530], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9767], requires_grad=True), W.grad=tensor([-0.0056]), b=tensor([0.0530], requires_grad=True), b.grad=tensor([0.0127])\n",
      "after  step() : W=tensor([0.9768], requires_grad=True), W.grad=tensor([-0.0056]), b=tensor([0.0528], requires_grad=True), b.grad=tensor([0.0127])\n",
      "=======================807==================\n",
      "hypothesis=\n",
      "tensor([[1.0296],\n",
      "        [2.0064],\n",
      "        [2.9831]], grad_fn=<AddBackward0>)\n",
      "loss=0.00040059606544673443\n",
      "before backward() : W=tensor([0.9768], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0528], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9768], requires_grad=True), W.grad=tensor([-0.0056]), b=tensor([0.0528], requires_grad=True), b.grad=tensor([0.0127])\n",
      "after  step() : W=tensor([0.9768], requires_grad=True), W.grad=tensor([-0.0056]), b=tensor([0.0527], requires_grad=True), b.grad=tensor([0.0127])\n",
      "=======================808==================\n",
      "hypothesis=\n",
      "tensor([[1.0295],\n",
      "        [2.0063],\n",
      "        [2.9831]], grad_fn=<AddBackward0>)\n",
      "loss=0.00039867369923740625\n",
      "before backward() : W=tensor([0.9768], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0527], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9768], requires_grad=True), W.grad=tensor([-0.0056]), b=tensor([0.0527], requires_grad=True), b.grad=tensor([0.0127])\n",
      "after  step() : W=tensor([0.9769], requires_grad=True), W.grad=tensor([-0.0056]), b=tensor([0.0526], requires_grad=True), b.grad=tensor([0.0127])\n",
      "=======================809==================\n",
      "hypothesis=\n",
      "tensor([[1.0295],\n",
      "        [2.0063],\n",
      "        [2.9832]], grad_fn=<AddBackward0>)\n",
      "loss=0.00039675660082139075\n",
      "before backward() : W=tensor([0.9769], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0526], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9769], requires_grad=True), W.grad=tensor([-0.0056]), b=tensor([0.0526], requires_grad=True), b.grad=tensor([0.0126])\n",
      "after  step() : W=tensor([0.9769], requires_grad=True), W.grad=tensor([-0.0056]), b=tensor([0.0525], requires_grad=True), b.grad=tensor([0.0126])\n",
      "=======================810==================\n",
      "hypothesis=\n",
      "tensor([[1.0294],\n",
      "        [2.0063],\n",
      "        [2.9832]], grad_fn=<AddBackward0>)\n",
      "loss=0.000394853443140164\n",
      "before backward() : W=tensor([0.9769], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0525], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9769], requires_grad=True), W.grad=tensor([-0.0055]), b=tensor([0.0525], requires_grad=True), b.grad=tensor([0.0126])\n",
      "after  step() : W=tensor([0.9770], requires_grad=True), W.grad=tensor([-0.0055]), b=tensor([0.0523], requires_grad=True), b.grad=tensor([0.0126])\n",
      "=======================811==================\n",
      "hypothesis=\n",
      "tensor([[1.0293],\n",
      "        [2.0063],\n",
      "        [2.9833]], grad_fn=<AddBackward0>)\n",
      "loss=0.00039295852184295654\n",
      "before backward() : W=tensor([0.9770], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0523], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9770], requires_grad=True), W.grad=tensor([-0.0055]), b=tensor([0.0523], requires_grad=True), b.grad=tensor([0.0126])\n",
      "after  step() : W=tensor([0.9770], requires_grad=True), W.grad=tensor([-0.0055]), b=tensor([0.0522], requires_grad=True), b.grad=tensor([0.0126])\n",
      "=======================812==================\n",
      "hypothesis=\n",
      "tensor([[1.0292],\n",
      "        [2.0063],\n",
      "        [2.9833]], grad_fn=<AddBackward0>)\n",
      "loss=0.00039106886833906174\n",
      "before backward() : W=tensor([0.9770], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0522], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9770], requires_grad=True), W.grad=tensor([-0.0055]), b=tensor([0.0522], requires_grad=True), b.grad=tensor([0.0126])\n",
      "after  step() : W=tensor([0.9771], requires_grad=True), W.grad=tensor([-0.0055]), b=tensor([0.0521], requires_grad=True), b.grad=tensor([0.0126])\n",
      "=======================813==================\n",
      "hypothesis=\n",
      "tensor([[1.0292],\n",
      "        [2.0063],\n",
      "        [2.9833]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003891939704772085\n",
      "before backward() : W=tensor([0.9771], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0521], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9771], requires_grad=True), W.grad=tensor([-0.0055]), b=tensor([0.0521], requires_grad=True), b.grad=tensor([0.0125])\n",
      "after  step() : W=tensor([0.9771], requires_grad=True), W.grad=tensor([-0.0055]), b=tensor([0.0520], requires_grad=True), b.grad=tensor([0.0125])\n",
      "=======================814==================\n",
      "hypothesis=\n",
      "tensor([[1.0291],\n",
      "        [2.0062],\n",
      "        [2.9834]], grad_fn=<AddBackward0>)\n",
      "loss=0.00038732230314053595\n",
      "before backward() : W=tensor([0.9771], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0520], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9771], requires_grad=True), W.grad=tensor([-0.0055]), b=tensor([0.0520], requires_grad=True), b.grad=tensor([0.0125])\n",
      "after  step() : W=tensor([0.9772], requires_grad=True), W.grad=tensor([-0.0055]), b=tensor([0.0518], requires_grad=True), b.grad=tensor([0.0125])\n",
      "=======================815==================\n",
      "hypothesis=\n",
      "tensor([[1.0290],\n",
      "        [2.0062],\n",
      "        [2.9834]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003854640235658735\n",
      "before backward() : W=tensor([0.9772], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0518], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9772], requires_grad=True), W.grad=tensor([-0.0055]), b=tensor([0.0518], requires_grad=True), b.grad=tensor([0.0125])\n",
      "after  step() : W=tensor([0.9773], requires_grad=True), W.grad=tensor([-0.0055]), b=tensor([0.0517], requires_grad=True), b.grad=tensor([0.0125])\n",
      "=======================816==================\n",
      "hypothesis=\n",
      "tensor([[1.0290],\n",
      "        [2.0062],\n",
      "        [2.9835]], grad_fn=<AddBackward0>)\n",
      "loss=0.00038361185579560697\n",
      "before backward() : W=tensor([0.9773], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0517], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9773], requires_grad=True), W.grad=tensor([-0.0055]), b=tensor([0.0517], requires_grad=True), b.grad=tensor([0.0124])\n",
      "after  step() : W=tensor([0.9773], requires_grad=True), W.grad=tensor([-0.0055]), b=tensor([0.0516], requires_grad=True), b.grad=tensor([0.0124])\n",
      "=======================817==================\n",
      "hypothesis=\n",
      "tensor([[1.0289],\n",
      "        [2.0062],\n",
      "        [2.9835]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003817713586613536\n",
      "before backward() : W=tensor([0.9773], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0516], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9773], requires_grad=True), W.grad=tensor([-0.0055]), b=tensor([0.0516], requires_grad=True), b.grad=tensor([0.0124])\n",
      "after  step() : W=tensor([0.9774], requires_grad=True), W.grad=tensor([-0.0055]), b=tensor([0.0515], requires_grad=True), b.grad=tensor([0.0124])\n",
      "=======================818==================\n",
      "hypothesis=\n",
      "tensor([[1.0288],\n",
      "        [2.0062],\n",
      "        [2.9835]], grad_fn=<AddBackward0>)\n",
      "loss=0.00037993499427102506\n",
      "before backward() : W=tensor([0.9774], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0515], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9774], requires_grad=True), W.grad=tensor([-0.0054]), b=tensor([0.0515], requires_grad=True), b.grad=tensor([0.0124])\n",
      "after  step() : W=tensor([0.9774], requires_grad=True), W.grad=tensor([-0.0054]), b=tensor([0.0513], requires_grad=True), b.grad=tensor([0.0124])\n",
      "=======================819==================\n",
      "hypothesis=\n",
      "tensor([[1.0288],\n",
      "        [2.0062],\n",
      "        [2.9836]], grad_fn=<AddBackward0>)\n",
      "loss=0.00037811114452779293\n",
      "before backward() : W=tensor([0.9774], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0513], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9774], requires_grad=True), W.grad=tensor([-0.0054]), b=tensor([0.0513], requires_grad=True), b.grad=tensor([0.0123])\n",
      "after  step() : W=tensor([0.9775], requires_grad=True), W.grad=tensor([-0.0054]), b=tensor([0.0512], requires_grad=True), b.grad=tensor([0.0123])\n",
      "=======================820==================\n",
      "hypothesis=\n",
      "tensor([[1.0287],\n",
      "        [2.0062],\n",
      "        [2.9836]], grad_fn=<AddBackward0>)\n",
      "loss=0.00037629660801030695\n",
      "before backward() : W=tensor([0.9775], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0512], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9775], requires_grad=True), W.grad=tensor([-0.0054]), b=tensor([0.0512], requires_grad=True), b.grad=tensor([0.0123])\n",
      "after  step() : W=tensor([0.9775], requires_grad=True), W.grad=tensor([-0.0054]), b=tensor([0.0511], requires_grad=True), b.grad=tensor([0.0123])\n",
      "=======================821==================\n",
      "hypothesis=\n",
      "tensor([[1.0286],\n",
      "        [2.0061],\n",
      "        [2.9837]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003744910063687712\n",
      "before backward() : W=tensor([0.9775], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0511], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9775], requires_grad=True), W.grad=tensor([-0.0054]), b=tensor([0.0511], requires_grad=True), b.grad=tensor([0.0123])\n",
      "after  step() : W=tensor([0.9776], requires_grad=True), W.grad=tensor([-0.0054]), b=tensor([0.0510], requires_grad=True), b.grad=tensor([0.0123])\n",
      "=======================822==================\n",
      "hypothesis=\n",
      "tensor([[1.0285],\n",
      "        [2.0061],\n",
      "        [2.9837]], grad_fn=<AddBackward0>)\n",
      "loss=0.00037268942105583847\n",
      "before backward() : W=tensor([0.9776], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0510], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9776], requires_grad=True), W.grad=tensor([-0.0054]), b=tensor([0.0510], requires_grad=True), b.grad=tensor([0.0123])\n",
      "after  step() : W=tensor([0.9776], requires_grad=True), W.grad=tensor([-0.0054]), b=tensor([0.0508], requires_grad=True), b.grad=tensor([0.0123])\n",
      "=======================823==================\n",
      "hypothesis=\n",
      "tensor([[1.0285],\n",
      "        [2.0061],\n",
      "        [2.9837]], grad_fn=<AddBackward0>)\n",
      "loss=0.00037089959369041026\n",
      "before backward() : W=tensor([0.9776], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0508], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9776], requires_grad=True), W.grad=tensor([-0.0054]), b=tensor([0.0508], requires_grad=True), b.grad=tensor([0.0122])\n",
      "after  step() : W=tensor([0.9777], requires_grad=True), W.grad=tensor([-0.0054]), b=tensor([0.0507], requires_grad=True), b.grad=tensor([0.0122])\n",
      "=======================824==================\n",
      "hypothesis=\n",
      "tensor([[1.0284],\n",
      "        [2.0061],\n",
      "        [2.9838]], grad_fn=<AddBackward0>)\n",
      "loss=0.00036912213545292616\n",
      "before backward() : W=tensor([0.9777], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0507], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9777], requires_grad=True), W.grad=tensor([-0.0054]), b=tensor([0.0507], requires_grad=True), b.grad=tensor([0.0122])\n",
      "after  step() : W=tensor([0.9777], requires_grad=True), W.grad=tensor([-0.0054]), b=tensor([0.0506], requires_grad=True), b.grad=tensor([0.0122])\n",
      "=======================825==================\n",
      "hypothesis=\n",
      "tensor([[1.0283],\n",
      "        [2.0061],\n",
      "        [2.9838]], grad_fn=<AddBackward0>)\n",
      "loss=0.00036734770401380956\n",
      "before backward() : W=tensor([0.9777], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0506], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9777], requires_grad=True), W.grad=tensor([-0.0054]), b=tensor([0.0506], requires_grad=True), b.grad=tensor([0.0122])\n",
      "after  step() : W=tensor([0.9778], requires_grad=True), W.grad=tensor([-0.0054]), b=tensor([0.0505], requires_grad=True), b.grad=tensor([0.0122])\n",
      "=======================826==================\n",
      "hypothesis=\n",
      "tensor([[1.0283],\n",
      "        [2.0061],\n",
      "        [2.9839]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003655832551885396\n",
      "before backward() : W=tensor([0.9778], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0505], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9778], requires_grad=True), W.grad=tensor([-0.0053]), b=tensor([0.0505], requires_grad=True), b.grad=tensor([0.0121])\n",
      "after  step() : W=tensor([0.9778], requires_grad=True), W.grad=tensor([-0.0053]), b=tensor([0.0504], requires_grad=True), b.grad=tensor([0.0121])\n",
      "=======================827==================\n",
      "hypothesis=\n",
      "tensor([[1.0282],\n",
      "        [2.0061],\n",
      "        [2.9839]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003638275957200676\n",
      "before backward() : W=tensor([0.9778], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0504], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9778], requires_grad=True), W.grad=tensor([-0.0053]), b=tensor([0.0504], requires_grad=True), b.grad=tensor([0.0121])\n",
      "after  step() : W=tensor([0.9779], requires_grad=True), W.grad=tensor([-0.0053]), b=tensor([0.0502], requires_grad=True), b.grad=tensor([0.0121])\n",
      "=======================828==================\n",
      "hypothesis=\n",
      "tensor([[1.0281],\n",
      "        [2.0060],\n",
      "        [2.9839]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003620783390942961\n",
      "before backward() : W=tensor([0.9779], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0502], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9779], requires_grad=True), W.grad=tensor([-0.0053]), b=tensor([0.0502], requires_grad=True), b.grad=tensor([0.0121])\n",
      "after  step() : W=tensor([0.9780], requires_grad=True), W.grad=tensor([-0.0053]), b=tensor([0.0501], requires_grad=True), b.grad=tensor([0.0121])\n",
      "=======================829==================\n",
      "hypothesis=\n",
      "tensor([[1.0281],\n",
      "        [2.0060],\n",
      "        [2.9840]], grad_fn=<AddBackward0>)\n",
      "loss=0.00036034287768416107\n",
      "before backward() : W=tensor([0.9780], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0501], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9780], requires_grad=True), W.grad=tensor([-0.0053]), b=tensor([0.0501], requires_grad=True), b.grad=tensor([0.0120])\n",
      "after  step() : W=tensor([0.9780], requires_grad=True), W.grad=tensor([-0.0053]), b=tensor([0.0500], requires_grad=True), b.grad=tensor([0.0120])\n",
      "=======================830==================\n",
      "hypothesis=\n",
      "tensor([[1.0280],\n",
      "        [2.0060],\n",
      "        [2.9840]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003586138191167265\n",
      "before backward() : W=tensor([0.9780], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0500], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9780], requires_grad=True), W.grad=tensor([-0.0053]), b=tensor([0.0500], requires_grad=True), b.grad=tensor([0.0120])\n",
      "after  step() : W=tensor([0.9781], requires_grad=True), W.grad=tensor([-0.0053]), b=tensor([0.0499], requires_grad=True), b.grad=tensor([0.0120])\n",
      "=======================831==================\n",
      "hypothesis=\n",
      "tensor([[1.0279],\n",
      "        [2.0060],\n",
      "        [2.9841]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003568886313587427\n",
      "before backward() : W=tensor([0.9781], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0499], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9781], requires_grad=True), W.grad=tensor([-0.0053]), b=tensor([0.0499], requires_grad=True), b.grad=tensor([0.0120])\n",
      "after  step() : W=tensor([0.9781], requires_grad=True), W.grad=tensor([-0.0053]), b=tensor([0.0498], requires_grad=True), b.grad=tensor([0.0120])\n",
      "=======================832==================\n",
      "hypothesis=\n",
      "tensor([[1.0279],\n",
      "        [2.0060],\n",
      "        [2.9841]], grad_fn=<AddBackward0>)\n",
      "loss=0.00035517546348273754\n",
      "before backward() : W=tensor([0.9781], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0498], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9781], requires_grad=True), W.grad=tensor([-0.0053]), b=tensor([0.0498], requires_grad=True), b.grad=tensor([0.0120])\n",
      "after  step() : W=tensor([0.9782], requires_grad=True), W.grad=tensor([-0.0053]), b=tensor([0.0496], requires_grad=True), b.grad=tensor([0.0120])\n",
      "=======================833==================\n",
      "hypothesis=\n",
      "tensor([[1.0278],\n",
      "        [2.0060],\n",
      "        [2.9841]], grad_fn=<AddBackward0>)\n",
      "loss=0.00035347117227502167\n",
      "before backward() : W=tensor([0.9782], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0496], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9782], requires_grad=True), W.grad=tensor([-0.0052]), b=tensor([0.0496], requires_grad=True), b.grad=tensor([0.0119])\n",
      "after  step() : W=tensor([0.9782], requires_grad=True), W.grad=tensor([-0.0052]), b=tensor([0.0495], requires_grad=True), b.grad=tensor([0.0119])\n",
      "=======================834==================\n",
      "hypothesis=\n",
      "tensor([[1.0277],\n",
      "        [2.0060],\n",
      "        [2.9842]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003517731965985149\n",
      "before backward() : W=tensor([0.9782], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0495], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9782], requires_grad=True), W.grad=tensor([-0.0052]), b=tensor([0.0495], requires_grad=True), b.grad=tensor([0.0119])\n",
      "after  step() : W=tensor([0.9783], requires_grad=True), W.grad=tensor([-0.0052]), b=tensor([0.0494], requires_grad=True), b.grad=tensor([0.0119])\n",
      "=======================835==================\n",
      "hypothesis=\n",
      "tensor([[1.0277],\n",
      "        [2.0059],\n",
      "        [2.9842]], grad_fn=<AddBackward0>)\n",
      "loss=0.00035008369013667107\n",
      "before backward() : W=tensor([0.9783], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0494], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9783], requires_grad=True), W.grad=tensor([-0.0052]), b=tensor([0.0494], requires_grad=True), b.grad=tensor([0.0119])\n",
      "after  step() : W=tensor([0.9783], requires_grad=True), W.grad=tensor([-0.0052]), b=tensor([0.0493], requires_grad=True), b.grad=tensor([0.0119])\n",
      "=======================836==================\n",
      "hypothesis=\n",
      "tensor([[1.0276],\n",
      "        [2.0059],\n",
      "        [2.9842]], grad_fn=<AddBackward0>)\n",
      "loss=0.00034840297303162515\n",
      "before backward() : W=tensor([0.9783], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0493], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9783], requires_grad=True), W.grad=tensor([-0.0052]), b=tensor([0.0493], requires_grad=True), b.grad=tensor([0.0118])\n",
      "after  step() : W=tensor([0.9784], requires_grad=True), W.grad=tensor([-0.0052]), b=tensor([0.0492], requires_grad=True), b.grad=tensor([0.0118])\n",
      "=======================837==================\n",
      "hypothesis=\n",
      "tensor([[1.0275],\n",
      "        [2.0059],\n",
      "        [2.9843]], grad_fn=<AddBackward0>)\n",
      "loss=0.00034673092886805534\n",
      "before backward() : W=tensor([0.9784], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0492], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9784], requires_grad=True), W.grad=tensor([-0.0052]), b=tensor([0.0492], requires_grad=True), b.grad=tensor([0.0118])\n",
      "after  step() : W=tensor([0.9784], requires_grad=True), W.grad=tensor([-0.0052]), b=tensor([0.0490], requires_grad=True), b.grad=tensor([0.0118])\n",
      "=======================838==================\n",
      "hypothesis=\n",
      "tensor([[1.0275],\n",
      "        [2.0059],\n",
      "        [2.9843]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003450648218858987\n",
      "before backward() : W=tensor([0.9784], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0490], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9784], requires_grad=True), W.grad=tensor([-0.0052]), b=tensor([0.0490], requires_grad=True), b.grad=tensor([0.0118])\n",
      "after  step() : W=tensor([0.9785], requires_grad=True), W.grad=tensor([-0.0052]), b=tensor([0.0489], requires_grad=True), b.grad=tensor([0.0118])\n",
      "=======================839==================\n",
      "hypothesis=\n",
      "tensor([[1.0274],\n",
      "        [2.0059],\n",
      "        [2.9844]], grad_fn=<AddBackward0>)\n",
      "loss=0.00034340834827162325\n",
      "before backward() : W=tensor([0.9785], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0489], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9785], requires_grad=True), W.grad=tensor([-0.0052]), b=tensor([0.0489], requires_grad=True), b.grad=tensor([0.0118])\n",
      "after  step() : W=tensor([0.9785], requires_grad=True), W.grad=tensor([-0.0052]), b=tensor([0.0488], requires_grad=True), b.grad=tensor([0.0118])\n",
      "=======================840==================\n",
      "hypothesis=\n",
      "tensor([[1.0273],\n",
      "        [2.0059],\n",
      "        [2.9844]], grad_fn=<AddBackward0>)\n",
      "loss=0.00034175708424299955\n",
      "before backward() : W=tensor([0.9785], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0488], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9785], requires_grad=True), W.grad=tensor([-0.0052]), b=tensor([0.0488], requires_grad=True), b.grad=tensor([0.0117])\n",
      "after  step() : W=tensor([0.9786], requires_grad=True), W.grad=tensor([-0.0052]), b=tensor([0.0487], requires_grad=True), b.grad=tensor([0.0117])\n",
      "=======================841==================\n",
      "hypothesis=\n",
      "tensor([[1.0273],\n",
      "        [2.0059],\n",
      "        [2.9844]], grad_fn=<AddBackward0>)\n",
      "loss=0.000340117490850389\n",
      "before backward() : W=tensor([0.9786], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0487], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9786], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0487], requires_grad=True), b.grad=tensor([0.0117])\n",
      "after  step() : W=tensor([0.9786], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0486], requires_grad=True), b.grad=tensor([0.0117])\n",
      "=======================842==================\n",
      "hypothesis=\n",
      "tensor([[1.0272],\n",
      "        [2.0058],\n",
      "        [2.9845]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003384840674698353\n",
      "before backward() : W=tensor([0.9786], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0486], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9786], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0486], requires_grad=True), b.grad=tensor([0.0117])\n",
      "after  step() : W=tensor([0.9787], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0485], requires_grad=True), b.grad=tensor([0.0117])\n",
      "=======================843==================\n",
      "hypothesis=\n",
      "tensor([[1.0271],\n",
      "        [2.0058],\n",
      "        [2.9845]], grad_fn=<AddBackward0>)\n",
      "loss=0.00033685669768601656\n",
      "before backward() : W=tensor([0.9787], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0485], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9787], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0485], requires_grad=True), b.grad=tensor([0.0116])\n",
      "after  step() : W=tensor([0.9787], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0483], requires_grad=True), b.grad=tensor([0.0116])\n",
      "=======================844==================\n",
      "hypothesis=\n",
      "tensor([[1.0271],\n",
      "        [2.0058],\n",
      "        [2.9845]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003352409403305501\n",
      "before backward() : W=tensor([0.9787], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0483], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9787], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0483], requires_grad=True), b.grad=tensor([0.0116])\n",
      "after  step() : W=tensor([0.9788], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0482], requires_grad=True), b.grad=tensor([0.0116])\n",
      "=======================845==================\n",
      "hypothesis=\n",
      "tensor([[1.0270],\n",
      "        [2.0058],\n",
      "        [2.9846]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003336302761454135\n",
      "before backward() : W=tensor([0.9788], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0482], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9788], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0482], requires_grad=True), b.grad=tensor([0.0116])\n",
      "after  step() : W=tensor([0.9788], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0481], requires_grad=True), b.grad=tensor([0.0116])\n",
      "=======================846==================\n",
      "hypothesis=\n",
      "tensor([[1.0269],\n",
      "        [2.0058],\n",
      "        [2.9846]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003320287214592099\n",
      "before backward() : W=tensor([0.9788], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0481], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9788], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0481], requires_grad=True), b.grad=tensor([0.0116])\n",
      "after  step() : W=tensor([0.9789], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0480], requires_grad=True), b.grad=tensor([0.0116])\n",
      "=======================847==================\n",
      "hypothesis=\n",
      "tensor([[1.0269],\n",
      "        [2.0058],\n",
      "        [2.9847]], grad_fn=<AddBackward0>)\n",
      "loss=0.000330433453200385\n",
      "before backward() : W=tensor([0.9789], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0480], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9789], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0480], requires_grad=True), b.grad=tensor([0.0115])\n",
      "after  step() : W=tensor([0.9789], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0479], requires_grad=True), b.grad=tensor([0.0115])\n",
      "=======================848==================\n",
      "hypothesis=\n",
      "tensor([[1.0268],\n",
      "        [2.0058],\n",
      "        [2.9847]], grad_fn=<AddBackward0>)\n",
      "loss=0.00032884778920561075\n",
      "before backward() : W=tensor([0.9789], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0479], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9789], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0479], requires_grad=True), b.grad=tensor([0.0115])\n",
      "after  step() : W=tensor([0.9790], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0478], requires_grad=True), b.grad=tensor([0.0115])\n",
      "=======================849==================\n",
      "hypothesis=\n",
      "tensor([[1.0268],\n",
      "        [2.0057],\n",
      "        [2.9847]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003272689937148243\n",
      "before backward() : W=tensor([0.9790], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0478], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9790], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0478], requires_grad=True), b.grad=tensor([0.0115])\n",
      "after  step() : W=tensor([0.9790], requires_grad=True), W.grad=tensor([-0.0051]), b=tensor([0.0476], requires_grad=True), b.grad=tensor([0.0115])\n",
      "=======================850==================\n",
      "hypothesis=\n",
      "tensor([[1.0267],\n",
      "        [2.0057],\n",
      "        [2.9848]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003256985219195485\n",
      "before backward() : W=tensor([0.9790], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0476], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9790], requires_grad=True), W.grad=tensor([-0.0050]), b=tensor([0.0476], requires_grad=True), b.grad=tensor([0.0115])\n",
      "after  step() : W=tensor([0.9791], requires_grad=True), W.grad=tensor([-0.0050]), b=tensor([0.0475], requires_grad=True), b.grad=tensor([0.0115])\n",
      "=======================851==================\n",
      "hypothesis=\n",
      "tensor([[1.0266],\n",
      "        [2.0057],\n",
      "        [2.9848]], grad_fn=<AddBackward0>)\n",
      "loss=0.00032413125154562294\n",
      "before backward() : W=tensor([0.9791], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0475], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9791], requires_grad=True), W.grad=tensor([-0.0050]), b=tensor([0.0475], requires_grad=True), b.grad=tensor([0.0114])\n",
      "after  step() : W=tensor([0.9791], requires_grad=True), W.grad=tensor([-0.0050]), b=tensor([0.0474], requires_grad=True), b.grad=tensor([0.0114])\n",
      "=======================852==================\n",
      "hypothesis=\n",
      "tensor([[1.0266],\n",
      "        [2.0057],\n",
      "        [2.9848]], grad_fn=<AddBackward0>)\n",
      "loss=0.00032257556449621916\n",
      "before backward() : W=tensor([0.9791], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0474], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9791], requires_grad=True), W.grad=tensor([-0.0050]), b=tensor([0.0474], requires_grad=True), b.grad=tensor([0.0114])\n",
      "after  step() : W=tensor([0.9792], requires_grad=True), W.grad=tensor([-0.0050]), b=tensor([0.0473], requires_grad=True), b.grad=tensor([0.0114])\n",
      "=======================853==================\n",
      "hypothesis=\n",
      "tensor([[1.0265],\n",
      "        [2.0057],\n",
      "        [2.9849]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003210269787814468\n",
      "before backward() : W=tensor([0.9792], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0473], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9792], requires_grad=True), W.grad=tensor([-0.0050]), b=tensor([0.0473], requires_grad=True), b.grad=tensor([0.0114])\n",
      "after  step() : W=tensor([0.9792], requires_grad=True), W.grad=tensor([-0.0050]), b=tensor([0.0472], requires_grad=True), b.grad=tensor([0.0114])\n",
      "=======================854==================\n",
      "hypothesis=\n",
      "tensor([[1.0264],\n",
      "        [2.0057],\n",
      "        [2.9849]], grad_fn=<AddBackward0>)\n",
      "loss=0.00031948540708981454\n",
      "before backward() : W=tensor([0.9792], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0472], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9792], requires_grad=True), W.grad=tensor([-0.0050]), b=tensor([0.0472], requires_grad=True), b.grad=tensor([0.0113])\n",
      "after  step() : W=tensor([0.9793], requires_grad=True), W.grad=tensor([-0.0050]), b=tensor([0.0471], requires_grad=True), b.grad=tensor([0.0113])\n",
      "=======================855==================\n",
      "hypothesis=\n",
      "tensor([[1.0264],\n",
      "        [2.0057],\n",
      "        [2.9849]], grad_fn=<AddBackward0>)\n",
      "loss=0.00031795079121366143\n",
      "before backward() : W=tensor([0.9793], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0471], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9793], requires_grad=True), W.grad=tensor([-0.0050]), b=tensor([0.0471], requires_grad=True), b.grad=tensor([0.0113])\n",
      "after  step() : W=tensor([0.9793], requires_grad=True), W.grad=tensor([-0.0050]), b=tensor([0.0470], requires_grad=True), b.grad=tensor([0.0113])\n",
      "=======================856==================\n",
      "hypothesis=\n",
      "tensor([[1.0263],\n",
      "        [2.0056],\n",
      "        [2.9850]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003164252848364413\n",
      "before backward() : W=tensor([0.9793], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0470], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9793], requires_grad=True), W.grad=tensor([-0.0050]), b=tensor([0.0470], requires_grad=True), b.grad=tensor([0.0113])\n",
      "after  step() : W=tensor([0.9794], requires_grad=True), W.grad=tensor([-0.0050]), b=tensor([0.0469], requires_grad=True), b.grad=tensor([0.0113])\n",
      "=======================857==================\n",
      "hypothesis=\n",
      "tensor([[1.0262],\n",
      "        [2.0056],\n",
      "        [2.9850]], grad_fn=<AddBackward0>)\n",
      "loss=0.00031490522087551653\n",
      "before backward() : W=tensor([0.9794], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0469], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9794], requires_grad=True), W.grad=tensor([-0.0050]), b=tensor([0.0469], requires_grad=True), b.grad=tensor([0.0113])\n",
      "after  step() : W=tensor([0.9794], requires_grad=True), W.grad=tensor([-0.0050]), b=tensor([0.0467], requires_grad=True), b.grad=tensor([0.0113])\n",
      "=======================858==================\n",
      "hypothesis=\n",
      "tensor([[1.0262],\n",
      "        [2.0056],\n",
      "        [2.9851]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003133935679215938\n",
      "before backward() : W=tensor([0.9794], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0467], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9794], requires_grad=True), W.grad=tensor([-0.0049]), b=tensor([0.0467], requires_grad=True), b.grad=tensor([0.0112])\n",
      "after  step() : W=tensor([0.9795], requires_grad=True), W.grad=tensor([-0.0049]), b=tensor([0.0466], requires_grad=True), b.grad=tensor([0.0112])\n",
      "=======================859==================\n",
      "hypothesis=\n",
      "tensor([[1.0261],\n",
      "        [2.0056],\n",
      "        [2.9851]], grad_fn=<AddBackward0>)\n",
      "loss=0.00031188823049888015\n",
      "before backward() : W=tensor([0.9795], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0466], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9795], requires_grad=True), W.grad=tensor([-0.0049]), b=tensor([0.0466], requires_grad=True), b.grad=tensor([0.0112])\n",
      "after  step() : W=tensor([0.9795], requires_grad=True), W.grad=tensor([-0.0049]), b=tensor([0.0465], requires_grad=True), b.grad=tensor([0.0112])\n",
      "=======================860==================\n",
      "hypothesis=\n",
      "tensor([[1.0261],\n",
      "        [2.0056],\n",
      "        [2.9851]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003103921189904213\n",
      "before backward() : W=tensor([0.9795], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0465], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9795], requires_grad=True), W.grad=tensor([-0.0049]), b=tensor([0.0465], requires_grad=True), b.grad=tensor([0.0112])\n",
      "after  step() : W=tensor([0.9796], requires_grad=True), W.grad=tensor([-0.0049]), b=tensor([0.0464], requires_grad=True), b.grad=tensor([0.0112])\n",
      "=======================861==================\n",
      "hypothesis=\n",
      "tensor([[1.0260],\n",
      "        [2.0056],\n",
      "        [2.9852]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003088996163569391\n",
      "before backward() : W=tensor([0.9796], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0464], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9796], requires_grad=True), W.grad=tensor([-0.0049]), b=tensor([0.0464], requires_grad=True), b.grad=tensor([0.0112])\n",
      "after  step() : W=tensor([0.9796], requires_grad=True), W.grad=tensor([-0.0049]), b=tensor([0.0463], requires_grad=True), b.grad=tensor([0.0112])\n",
      "=======================862==================\n",
      "hypothesis=\n",
      "tensor([[1.0259],\n",
      "        [2.0056],\n",
      "        [2.9852]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003074186679441482\n",
      "before backward() : W=tensor([0.9796], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0463], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9796], requires_grad=True), W.grad=tensor([-0.0049]), b=tensor([0.0463], requires_grad=True), b.grad=tensor([0.0111])\n",
      "after  step() : W=tensor([0.9797], requires_grad=True), W.grad=tensor([-0.0049]), b=tensor([0.0462], requires_grad=True), b.grad=tensor([0.0111])\n",
      "=======================863==================\n",
      "hypothesis=\n",
      "tensor([[1.0259],\n",
      "        [2.0056],\n",
      "        [2.9852]], grad_fn=<AddBackward0>)\n",
      "loss=0.0003059418231714517\n",
      "before backward() : W=tensor([0.9797], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0462], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9797], requires_grad=True), W.grad=tensor([-0.0049]), b=tensor([0.0462], requires_grad=True), b.grad=tensor([0.0111])\n",
      "after  step() : W=tensor([0.9797], requires_grad=True), W.grad=tensor([-0.0049]), b=tensor([0.0461], requires_grad=True), b.grad=tensor([0.0111])\n",
      "=======================864==================\n",
      "hypothesis=\n",
      "tensor([[1.0258],\n",
      "        [2.0055],\n",
      "        [2.9853]], grad_fn=<AddBackward0>)\n",
      "loss=0.00030447239987552166\n",
      "before backward() : W=tensor([0.9797], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0461], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9797], requires_grad=True), W.grad=tensor([-0.0049]), b=tensor([0.0461], requires_grad=True), b.grad=tensor([0.0111])\n",
      "after  step() : W=tensor([0.9798], requires_grad=True), W.grad=tensor([-0.0049]), b=tensor([0.0460], requires_grad=True), b.grad=tensor([0.0111])\n",
      "=======================865==================\n",
      "hypothesis=\n",
      "tensor([[1.0257],\n",
      "        [2.0055],\n",
      "        [2.9853]], grad_fn=<AddBackward0>)\n",
      "loss=0.00030300969956442714\n",
      "before backward() : W=tensor([0.9798], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0460], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9798], requires_grad=True), W.grad=tensor([-0.0049]), b=tensor([0.0460], requires_grad=True), b.grad=tensor([0.0110])\n",
      "after  step() : W=tensor([0.9798], requires_grad=True), W.grad=tensor([-0.0049]), b=tensor([0.0458], requires_grad=True), b.grad=tensor([0.0110])\n",
      "=======================866==================\n",
      "hypothesis=\n",
      "tensor([[1.0257],\n",
      "        [2.0055],\n",
      "        [2.9853]], grad_fn=<AddBackward0>)\n",
      "loss=0.000301555497571826\n",
      "before backward() : W=tensor([0.9798], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0458], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9798], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0458], requires_grad=True), b.grad=tensor([0.0110])\n",
      "after  step() : W=tensor([0.9799], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0457], requires_grad=True), b.grad=tensor([0.0110])\n",
      "=======================867==================\n",
      "hypothesis=\n",
      "tensor([[1.0256],\n",
      "        [2.0055],\n",
      "        [2.9854]], grad_fn=<AddBackward0>)\n",
      "loss=0.000300106214126572\n",
      "before backward() : W=tensor([0.9799], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0457], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9799], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0457], requires_grad=True), b.grad=tensor([0.0110])\n",
      "after  step() : W=tensor([0.9799], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0456], requires_grad=True), b.grad=tensor([0.0110])\n",
      "=======================868==================\n",
      "hypothesis=\n",
      "tensor([[1.0256],\n",
      "        [2.0055],\n",
      "        [2.9854]], grad_fn=<AddBackward0>)\n",
      "loss=0.00029866708791814744\n",
      "before backward() : W=tensor([0.9799], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0456], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9799], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0456], requires_grad=True), b.grad=tensor([0.0110])\n",
      "after  step() : W=tensor([0.9800], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0455], requires_grad=True), b.grad=tensor([0.0110])\n",
      "=======================869==================\n",
      "hypothesis=\n",
      "tensor([[1.0255],\n",
      "        [2.0055],\n",
      "        [2.9854]], grad_fn=<AddBackward0>)\n",
      "loss=0.00029723174520768225\n",
      "before backward() : W=tensor([0.9800], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0455], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9800], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0455], requires_grad=True), b.grad=tensor([0.0109])\n",
      "after  step() : W=tensor([0.9800], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0454], requires_grad=True), b.grad=tensor([0.0109])\n",
      "=======================870==================\n",
      "hypothesis=\n",
      "tensor([[1.0254],\n",
      "        [2.0055],\n",
      "        [2.9855]], grad_fn=<AddBackward0>)\n",
      "loss=0.000295804173219949\n",
      "before backward() : W=tensor([0.9800], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0454], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9800], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0454], requires_grad=True), b.grad=tensor([0.0109])\n",
      "after  step() : W=tensor([0.9801], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0453], requires_grad=True), b.grad=tensor([0.0109])\n",
      "=======================871==================\n",
      "hypothesis=\n",
      "tensor([[1.0254],\n",
      "        [2.0054],\n",
      "        [2.9855]], grad_fn=<AddBackward0>)\n",
      "loss=0.00029438294586725533\n",
      "before backward() : W=tensor([0.9801], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0453], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9801], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0453], requires_grad=True), b.grad=tensor([0.0109])\n",
      "after  step() : W=tensor([0.9801], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0452], requires_grad=True), b.grad=tensor([0.0109])\n",
      "=======================872==================\n",
      "hypothesis=\n",
      "tensor([[1.0253],\n",
      "        [2.0054],\n",
      "        [2.9856]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002929705660790205\n",
      "before backward() : W=tensor([0.9801], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0452], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9801], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0452], requires_grad=True), b.grad=tensor([0.0109])\n",
      "after  step() : W=tensor([0.9802], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0451], requires_grad=True), b.grad=tensor([0.0109])\n",
      "=======================873==================\n",
      "hypothesis=\n",
      "tensor([[1.0253],\n",
      "        [2.0054],\n",
      "        [2.9856]], grad_fn=<AddBackward0>)\n",
      "loss=0.00029156386153772473\n",
      "before backward() : W=tensor([0.9802], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0451], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9802], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0451], requires_grad=True), b.grad=tensor([0.0108])\n",
      "after  step() : W=tensor([0.9802], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0450], requires_grad=True), b.grad=tensor([0.0108])\n",
      "=======================874==================\n",
      "hypothesis=\n",
      "tensor([[1.0252],\n",
      "        [2.0054],\n",
      "        [2.9856]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002901631232816726\n",
      "before backward() : W=tensor([0.9802], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0450], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9802], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0450], requires_grad=True), b.grad=tensor([0.0108])\n",
      "after  step() : W=tensor([0.9803], requires_grad=True), W.grad=tensor([-0.0048]), b=tensor([0.0449], requires_grad=True), b.grad=tensor([0.0108])\n",
      "=======================875==================\n",
      "hypothesis=\n",
      "tensor([[1.0251],\n",
      "        [2.0054],\n",
      "        [2.9857]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002887692244257778\n",
      "before backward() : W=tensor([0.9803], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0449], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9803], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0449], requires_grad=True), b.grad=tensor([0.0108])\n",
      "after  step() : W=tensor([0.9803], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0448], requires_grad=True), b.grad=tensor([0.0108])\n",
      "=======================876==================\n",
      "hypothesis=\n",
      "tensor([[1.0251],\n",
      "        [2.0054],\n",
      "        [2.9857]], grad_fn=<AddBackward0>)\n",
      "loss=0.00028738146647810936\n",
      "before backward() : W=tensor([0.9803], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0448], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9803], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0448], requires_grad=True), b.grad=tensor([0.0108])\n",
      "after  step() : W=tensor([0.9804], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0447], requires_grad=True), b.grad=tensor([0.0108])\n",
      "=======================877==================\n",
      "hypothesis=\n",
      "tensor([[1.0250],\n",
      "        [2.0054],\n",
      "        [2.9857]], grad_fn=<AddBackward0>)\n",
      "loss=0.00028600249788723886\n",
      "before backward() : W=tensor([0.9804], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0447], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9804], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0447], requires_grad=True), b.grad=tensor([0.0107])\n",
      "after  step() : W=tensor([0.9804], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0445], requires_grad=True), b.grad=tensor([0.0107])\n",
      "=======================878==================\n",
      "hypothesis=\n",
      "tensor([[1.0249],\n",
      "        [2.0054],\n",
      "        [2.9858]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002846276911441237\n",
      "before backward() : W=tensor([0.9804], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0445], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9804], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0445], requires_grad=True), b.grad=tensor([0.0107])\n",
      "after  step() : W=tensor([0.9805], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0444], requires_grad=True), b.grad=tensor([0.0107])\n",
      "=======================879==================\n",
      "hypothesis=\n",
      "tensor([[1.0249],\n",
      "        [2.0053],\n",
      "        [2.9858]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002832623722497374\n",
      "before backward() : W=tensor([0.9805], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0444], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9805], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0444], requires_grad=True), b.grad=tensor([0.0107])\n",
      "after  step() : W=tensor([0.9805], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0443], requires_grad=True), b.grad=tensor([0.0107])\n",
      "=======================880==================\n",
      "hypothesis=\n",
      "tensor([[1.0248],\n",
      "        [2.0053],\n",
      "        [2.9858]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002819023502524942\n",
      "before backward() : W=tensor([0.9805], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0443], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9805], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0443], requires_grad=True), b.grad=tensor([0.0107])\n",
      "after  step() : W=tensor([0.9805], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0442], requires_grad=True), b.grad=tensor([0.0107])\n",
      "=======================881==================\n",
      "hypothesis=\n",
      "tensor([[1.0248],\n",
      "        [2.0053],\n",
      "        [2.9859]], grad_fn=<AddBackward0>)\n",
      "loss=0.00028055033180862665\n",
      "before backward() : W=tensor([0.9805], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0442], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9805], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0442], requires_grad=True), b.grad=tensor([0.0106])\n",
      "after  step() : W=tensor([0.9806], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0441], requires_grad=True), b.grad=tensor([0.0106])\n",
      "=======================882==================\n",
      "hypothesis=\n",
      "tensor([[1.0247],\n",
      "        [2.0053],\n",
      "        [2.9859]], grad_fn=<AddBackward0>)\n",
      "loss=0.00027920075808651745\n",
      "before backward() : W=tensor([0.9806], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0441], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9806], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0441], requires_grad=True), b.grad=tensor([0.0106])\n",
      "after  step() : W=tensor([0.9806], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0440], requires_grad=True), b.grad=tensor([0.0106])\n",
      "=======================883==================\n",
      "hypothesis=\n",
      "tensor([[1.0247],\n",
      "        [2.0053],\n",
      "        [2.9859]], grad_fn=<AddBackward0>)\n",
      "loss=0.000277861428912729\n",
      "before backward() : W=tensor([0.9806], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0440], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9806], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0440], requires_grad=True), b.grad=tensor([0.0106])\n",
      "after  step() : W=tensor([0.9807], requires_grad=True), W.grad=tensor([-0.0047]), b=tensor([0.0439], requires_grad=True), b.grad=tensor([0.0106])\n",
      "=======================884==================\n",
      "hypothesis=\n",
      "tensor([[1.0246],\n",
      "        [2.0053],\n",
      "        [2.9860]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002765250392258167\n",
      "before backward() : W=tensor([0.9807], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0439], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9807], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0439], requires_grad=True), b.grad=tensor([0.0106])\n",
      "after  step() : W=tensor([0.9807], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0438], requires_grad=True), b.grad=tensor([0.0106])\n",
      "=======================885==================\n",
      "hypothesis=\n",
      "tensor([[1.0245],\n",
      "        [2.0053],\n",
      "        [2.9860]], grad_fn=<AddBackward0>)\n",
      "loss=0.00027519743889570236\n",
      "before backward() : W=tensor([0.9807], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0438], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9807], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0438], requires_grad=True), b.grad=tensor([0.0105])\n",
      "after  step() : W=tensor([0.9808], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0437], requires_grad=True), b.grad=tensor([0.0105])\n",
      "=======================886==================\n",
      "hypothesis=\n",
      "tensor([[1.0245],\n",
      "        [2.0053],\n",
      "        [2.9860]], grad_fn=<AddBackward0>)\n",
      "loss=0.00027387772570364177\n",
      "before backward() : W=tensor([0.9808], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0437], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9808], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0437], requires_grad=True), b.grad=tensor([0.0105])\n",
      "after  step() : W=tensor([0.9808], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0436], requires_grad=True), b.grad=tensor([0.0105])\n",
      "=======================887==================\n",
      "hypothesis=\n",
      "tensor([[1.0244],\n",
      "        [2.0052],\n",
      "        [2.9861]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002725612430367619\n",
      "before backward() : W=tensor([0.9808], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0436], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9808], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0436], requires_grad=True), b.grad=tensor([0.0105])\n",
      "after  step() : W=tensor([0.9809], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0435], requires_grad=True), b.grad=tensor([0.0105])\n",
      "=======================888==================\n",
      "hypothesis=\n",
      "tensor([[1.0244],\n",
      "        [2.0052],\n",
      "        [2.9861]], grad_fn=<AddBackward0>)\n",
      "loss=0.00027125398628413677\n",
      "before backward() : W=tensor([0.9809], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0435], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9809], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0435], requires_grad=True), b.grad=tensor([0.0105])\n",
      "after  step() : W=tensor([0.9809], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0434], requires_grad=True), b.grad=tensor([0.0105])\n",
      "=======================889==================\n",
      "hypothesis=\n",
      "tensor([[1.0243],\n",
      "        [2.0052],\n",
      "        [2.9861]], grad_fn=<AddBackward0>)\n",
      "loss=0.000269949872745201\n",
      "before backward() : W=tensor([0.9809], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0434], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9809], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0434], requires_grad=True), b.grad=tensor([0.0104])\n",
      "after  step() : W=tensor([0.9810], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0433], requires_grad=True), b.grad=tensor([0.0104])\n",
      "=======================890==================\n",
      "hypothesis=\n",
      "tensor([[1.0242],\n",
      "        [2.0052],\n",
      "        [2.9862]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002686535590328276\n",
      "before backward() : W=tensor([0.9810], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0433], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9810], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0433], requires_grad=True), b.grad=tensor([0.0104])\n",
      "after  step() : W=tensor([0.9810], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0432], requires_grad=True), b.grad=tensor([0.0104])\n",
      "=======================891==================\n",
      "hypothesis=\n",
      "tensor([[1.0242],\n",
      "        [2.0052],\n",
      "        [2.9862]], grad_fn=<AddBackward0>)\n",
      "loss=0.00026736423023976386\n",
      "before backward() : W=tensor([0.9810], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0432], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9810], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0432], requires_grad=True), b.grad=tensor([0.0104])\n",
      "after  step() : W=tensor([0.9811], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0431], requires_grad=True), b.grad=tensor([0.0104])\n",
      "=======================892==================\n",
      "hypothesis=\n",
      "tensor([[1.0241],\n",
      "        [2.0052],\n",
      "        [2.9862]], grad_fn=<AddBackward0>)\n",
      "loss=0.00026607935433276\n",
      "before backward() : W=tensor([0.9811], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0431], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9811], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0431], requires_grad=True), b.grad=tensor([0.0104])\n",
      "after  step() : W=tensor([0.9811], requires_grad=True), W.grad=tensor([-0.0046]), b=tensor([0.0430], requires_grad=True), b.grad=tensor([0.0104])\n",
      "=======================893==================\n",
      "hypothesis=\n",
      "tensor([[1.0241],\n",
      "        [2.0052],\n",
      "        [2.9863]], grad_fn=<AddBackward0>)\n",
      "loss=0.00026480224914848804\n",
      "before backward() : W=tensor([0.9811], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0430], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9811], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0430], requires_grad=True), b.grad=tensor([0.0103])\n",
      "after  step() : W=tensor([0.9811], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0429], requires_grad=True), b.grad=tensor([0.0103])\n",
      "=======================894==================\n",
      "hypothesis=\n",
      "tensor([[1.0240],\n",
      "        [2.0052],\n",
      "        [2.9863]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002635303826536983\n",
      "before backward() : W=tensor([0.9811], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0429], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9811], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0429], requires_grad=True), b.grad=tensor([0.0103])\n",
      "after  step() : W=tensor([0.9812], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0428], requires_grad=True), b.grad=tensor([0.0103])\n",
      "=======================895==================\n",
      "hypothesis=\n",
      "tensor([[1.0239],\n",
      "        [2.0051],\n",
      "        [2.9863]], grad_fn=<AddBackward0>)\n",
      "loss=0.00026226535555906594\n",
      "before backward() : W=tensor([0.9812], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0428], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9812], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0428], requires_grad=True), b.grad=tensor([0.0103])\n",
      "after  step() : W=tensor([0.9812], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0427], requires_grad=True), b.grad=tensor([0.0103])\n",
      "=======================896==================\n",
      "hypothesis=\n",
      "tensor([[1.0239],\n",
      "        [2.0051],\n",
      "        [2.9864]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002610052761156112\n",
      "before backward() : W=tensor([0.9812], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0427], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9812], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0427], requires_grad=True), b.grad=tensor([0.0103])\n",
      "after  step() : W=tensor([0.9813], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0426], requires_grad=True), b.grad=tensor([0.0103])\n",
      "=======================897==================\n",
      "hypothesis=\n",
      "tensor([[1.0238],\n",
      "        [2.0051],\n",
      "        [2.9864]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002597501443233341\n",
      "before backward() : W=tensor([0.9813], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0426], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9813], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0426], requires_grad=True), b.grad=tensor([0.0102])\n",
      "after  step() : W=tensor([0.9813], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0424], requires_grad=True), b.grad=tensor([0.0102])\n",
      "=======================898==================\n",
      "hypothesis=\n",
      "tensor([[1.0238],\n",
      "        [2.0051],\n",
      "        [2.9864]], grad_fn=<AddBackward0>)\n",
      "loss=0.00025850479141809046\n",
      "before backward() : W=tensor([0.9813], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0424], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9813], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0424], requires_grad=True), b.grad=tensor([0.0102])\n",
      "after  step() : W=tensor([0.9814], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0423], requires_grad=True), b.grad=tensor([0.0102])\n",
      "=======================899==================\n",
      "hypothesis=\n",
      "tensor([[1.0237],\n",
      "        [2.0051],\n",
      "        [2.9865]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002572615921963006\n",
      "before backward() : W=tensor([0.9814], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0423], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9814], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0423], requires_grad=True), b.grad=tensor([0.0102])\n",
      "after  step() : W=tensor([0.9814], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0422], requires_grad=True), b.grad=tensor([0.0102])\n",
      "=======================900==================\n",
      "hypothesis=\n",
      "tensor([[1.0237],\n",
      "        [2.0051],\n",
      "        [2.9865]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002560281427577138\n",
      "before backward() : W=tensor([0.9814], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0422], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9814], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0422], requires_grad=True), b.grad=tensor([0.0102])\n",
      "after  step() : W=tensor([0.9815], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0421], requires_grad=True), b.grad=tensor([0.0102])\n",
      "=======================901==================\n",
      "hypothesis=\n",
      "tensor([[1.0236],\n",
      "        [2.0051],\n",
      "        [2.9865]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002547968178987503\n",
      "before backward() : W=tensor([0.9815], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0421], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9815], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0421], requires_grad=True), b.grad=tensor([0.0101])\n",
      "after  step() : W=tensor([0.9815], requires_grad=True), W.grad=tensor([-0.0045]), b=tensor([0.0420], requires_grad=True), b.grad=tensor([0.0101])\n",
      "=======================902==================\n",
      "hypothesis=\n",
      "tensor([[1.0235],\n",
      "        [2.0051],\n",
      "        [2.9866]], grad_fn=<AddBackward0>)\n",
      "loss=0.00025357570848427713\n",
      "before backward() : W=tensor([0.9815], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0420], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9815], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0420], requires_grad=True), b.grad=tensor([0.0101])\n",
      "after  step() : W=tensor([0.9815], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0419], requires_grad=True), b.grad=tensor([0.0101])\n",
      "=======================903==================\n",
      "hypothesis=\n",
      "tensor([[1.0235],\n",
      "        [2.0050],\n",
      "        [2.9866]], grad_fn=<AddBackward0>)\n",
      "loss=0.00025235884822905064\n",
      "before backward() : W=tensor([0.9815], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0419], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9815], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0419], requires_grad=True), b.grad=tensor([0.0101])\n",
      "after  step() : W=tensor([0.9816], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0418], requires_grad=True), b.grad=tensor([0.0101])\n",
      "=======================904==================\n",
      "hypothesis=\n",
      "tensor([[1.0234],\n",
      "        [2.0050],\n",
      "        [2.9866]], grad_fn=<AddBackward0>)\n",
      "loss=0.00025114676100201905\n",
      "before backward() : W=tensor([0.9816], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0418], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9816], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0418], requires_grad=True), b.grad=tensor([0.0101])\n",
      "after  step() : W=tensor([0.9816], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0417], requires_grad=True), b.grad=tensor([0.0101])\n",
      "=======================905==================\n",
      "hypothesis=\n",
      "tensor([[1.0234],\n",
      "        [2.0050],\n",
      "        [2.9867]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002499394759070128\n",
      "before backward() : W=tensor([0.9816], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0417], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9816], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0417], requires_grad=True), b.grad=tensor([0.0100])\n",
      "after  step() : W=tensor([0.9817], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0416], requires_grad=True), b.grad=tensor([0.0100])\n",
      "=======================906==================\n",
      "hypothesis=\n",
      "tensor([[1.0233],\n",
      "        [2.0050],\n",
      "        [2.9867]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002487374877091497\n",
      "before backward() : W=tensor([0.9817], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0416], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9817], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0416], requires_grad=True), b.grad=tensor([0.0100])\n",
      "after  step() : W=tensor([0.9817], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0415], requires_grad=True), b.grad=tensor([0.0100])\n",
      "=======================907==================\n",
      "hypothesis=\n",
      "tensor([[1.0233],\n",
      "        [2.0050],\n",
      "        [2.9867]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002475436485838145\n",
      "before backward() : W=tensor([0.9817], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0415], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9817], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0415], requires_grad=True), b.grad=tensor([0.0100])\n",
      "after  step() : W=tensor([0.9818], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0414], requires_grad=True), b.grad=tensor([0.0100])\n",
      "=======================908==================\n",
      "hypothesis=\n",
      "tensor([[1.0232],\n",
      "        [2.0050],\n",
      "        [2.9868]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002463574637658894\n",
      "before backward() : W=tensor([0.9818], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0414], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9818], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0414], requires_grad=True), b.grad=tensor([0.0100])\n",
      "after  step() : W=tensor([0.9818], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0413], requires_grad=True), b.grad=tensor([0.0100])\n",
      "=======================909==================\n",
      "hypothesis=\n",
      "tensor([[1.0232],\n",
      "        [2.0050],\n",
      "        [2.9868]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002451712207403034\n",
      "before backward() : W=tensor([0.9818], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0413], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9818], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0413], requires_grad=True), b.grad=tensor([0.0099])\n",
      "after  step() : W=tensor([0.9819], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0412], requires_grad=True), b.grad=tensor([0.0099])\n",
      "=======================910==================\n",
      "hypothesis=\n",
      "tensor([[1.0231],\n",
      "        [2.0050],\n",
      "        [2.9868]], grad_fn=<AddBackward0>)\n",
      "loss=0.00024399436370003968\n",
      "before backward() : W=tensor([0.9819], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0412], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9819], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0412], requires_grad=True), b.grad=tensor([0.0099])\n",
      "after  step() : W=tensor([0.9819], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0411], requires_grad=True), b.grad=tensor([0.0099])\n",
      "=======================911==================\n",
      "hypothesis=\n",
      "tensor([[1.0230],\n",
      "        [2.0049],\n",
      "        [2.9868]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002428261359455064\n",
      "before backward() : W=tensor([0.9819], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0411], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9819], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0411], requires_grad=True), b.grad=tensor([0.0099])\n",
      "after  step() : W=tensor([0.9819], requires_grad=True), W.grad=tensor([-0.0044]), b=tensor([0.0410], requires_grad=True), b.grad=tensor([0.0099])\n",
      "=======================912==================\n",
      "hypothesis=\n",
      "tensor([[1.0230],\n",
      "        [2.0049],\n",
      "        [2.9869]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002416572970105335\n",
      "before backward() : W=tensor([0.9819], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0410], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9819], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0410], requires_grad=True), b.grad=tensor([0.0099])\n",
      "after  step() : W=tensor([0.9820], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0409], requires_grad=True), b.grad=tensor([0.0099])\n",
      "=======================913==================\n",
      "hypothesis=\n",
      "tensor([[1.0229],\n",
      "        [2.0049],\n",
      "        [2.9869]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002404964907327667\n",
      "before backward() : W=tensor([0.9820], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0409], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9820], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0409], requires_grad=True), b.grad=tensor([0.0098])\n",
      "after  step() : W=tensor([0.9820], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0408], requires_grad=True), b.grad=tensor([0.0098])\n",
      "=======================914==================\n",
      "hypothesis=\n",
      "tensor([[1.0229],\n",
      "        [2.0049],\n",
      "        [2.9869]], grad_fn=<AddBackward0>)\n",
      "loss=0.00023934418277349323\n",
      "before backward() : W=tensor([0.9820], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0408], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9820], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0408], requires_grad=True), b.grad=tensor([0.0098])\n",
      "after  step() : W=tensor([0.9821], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0407], requires_grad=True), b.grad=tensor([0.0098])\n",
      "=======================915==================\n",
      "hypothesis=\n",
      "tensor([[1.0228],\n",
      "        [2.0049],\n",
      "        [2.9870]], grad_fn=<AddBackward0>)\n",
      "loss=0.00023819257330615073\n",
      "before backward() : W=tensor([0.9821], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0407], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9821], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0407], requires_grad=True), b.grad=tensor([0.0098])\n",
      "after  step() : W=tensor([0.9821], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0406], requires_grad=True), b.grad=tensor([0.0098])\n",
      "=======================916==================\n",
      "hypothesis=\n",
      "tensor([[1.0228],\n",
      "        [2.0049],\n",
      "        [2.9870]], grad_fn=<AddBackward0>)\n",
      "loss=0.00023705018975306302\n",
      "before backward() : W=tensor([0.9821], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0406], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9821], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0406], requires_grad=True), b.grad=tensor([0.0098])\n",
      "after  step() : W=tensor([0.9822], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0406], requires_grad=True), b.grad=tensor([0.0098])\n",
      "=======================917==================\n",
      "hypothesis=\n",
      "tensor([[1.0227],\n",
      "        [2.0049],\n",
      "        [2.9870]], grad_fn=<AddBackward0>)\n",
      "loss=0.00023591237550135702\n",
      "before backward() : W=tensor([0.9822], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0406], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9822], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0406], requires_grad=True), b.grad=tensor([0.0097])\n",
      "after  step() : W=tensor([0.9822], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0405], requires_grad=True), b.grad=tensor([0.0097])\n",
      "=======================918==================\n",
      "hypothesis=\n",
      "tensor([[1.0227],\n",
      "        [2.0049],\n",
      "        [2.9871]], grad_fn=<AddBackward0>)\n",
      "loss=0.00023477908689528704\n",
      "before backward() : W=tensor([0.9822], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0405], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9822], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0405], requires_grad=True), b.grad=tensor([0.0097])\n",
      "after  step() : W=tensor([0.9822], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0404], requires_grad=True), b.grad=tensor([0.0097])\n",
      "=======================919==================\n",
      "hypothesis=\n",
      "tensor([[1.0226],\n",
      "        [2.0049],\n",
      "        [2.9871]], grad_fn=<AddBackward0>)\n",
      "loss=0.00023365010565612465\n",
      "before backward() : W=tensor([0.9822], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0404], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9822], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0404], requires_grad=True), b.grad=tensor([0.0097])\n",
      "after  step() : W=tensor([0.9823], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0403], requires_grad=True), b.grad=tensor([0.0097])\n",
      "=======================920==================\n",
      "hypothesis=\n",
      "tensor([[1.0225],\n",
      "        [2.0048],\n",
      "        [2.9871]], grad_fn=<AddBackward0>)\n",
      "loss=0.00023252790560945868\n",
      "before backward() : W=tensor([0.9823], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0403], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9823], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0403], requires_grad=True), b.grad=tensor([0.0097])\n",
      "after  step() : W=tensor([0.9823], requires_grad=True), W.grad=tensor([-0.0043]), b=tensor([0.0402], requires_grad=True), b.grad=tensor([0.0097])\n",
      "=======================921==================\n",
      "hypothesis=\n",
      "tensor([[1.0225],\n",
      "        [2.0048],\n",
      "        [2.9872]], grad_fn=<AddBackward0>)\n",
      "loss=0.00023141223937273026\n",
      "before backward() : W=tensor([0.9823], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0402], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9823], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0402], requires_grad=True), b.grad=tensor([0.0097])\n",
      "after  step() : W=tensor([0.9824], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0401], requires_grad=True), b.grad=tensor([0.0097])\n",
      "=======================922==================\n",
      "hypothesis=\n",
      "tensor([[1.0224],\n",
      "        [2.0048],\n",
      "        [2.9872]], grad_fn=<AddBackward0>)\n",
      "loss=0.00023029976000543684\n",
      "before backward() : W=tensor([0.9824], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0401], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9824], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0401], requires_grad=True), b.grad=tensor([0.0096])\n",
      "after  step() : W=tensor([0.9824], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0400], requires_grad=True), b.grad=tensor([0.0096])\n",
      "=======================923==================\n",
      "hypothesis=\n",
      "tensor([[1.0224],\n",
      "        [2.0048],\n",
      "        [2.9872]], grad_fn=<AddBackward0>)\n",
      "loss=0.00022919480397831649\n",
      "before backward() : W=tensor([0.9824], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0400], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9824], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0400], requires_grad=True), b.grad=tensor([0.0096])\n",
      "after  step() : W=tensor([0.9825], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0399], requires_grad=True), b.grad=tensor([0.0096])\n",
      "=======================924==================\n",
      "hypothesis=\n",
      "tensor([[1.0223],\n",
      "        [2.0048],\n",
      "        [2.9873]], grad_fn=<AddBackward0>)\n",
      "loss=0.00022809325309935957\n",
      "before backward() : W=tensor([0.9825], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0399], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9825], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0399], requires_grad=True), b.grad=tensor([0.0096])\n",
      "after  step() : W=tensor([0.9825], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0398], requires_grad=True), b.grad=tensor([0.0096])\n",
      "=======================925==================\n",
      "hypothesis=\n",
      "tensor([[1.0223],\n",
      "        [2.0048],\n",
      "        [2.9873]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002269991673529148\n",
      "before backward() : W=tensor([0.9825], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0398], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9825], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0398], requires_grad=True), b.grad=tensor([0.0096])\n",
      "after  step() : W=tensor([0.9825], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0397], requires_grad=True), b.grad=tensor([0.0096])\n",
      "=======================926==================\n",
      "hypothesis=\n",
      "tensor([[1.0222],\n",
      "        [2.0048],\n",
      "        [2.9873]], grad_fn=<AddBackward0>)\n",
      "loss=0.00022590819571632892\n",
      "before backward() : W=tensor([0.9825], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0397], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9825], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0397], requires_grad=True), b.grad=tensor([0.0095])\n",
      "after  step() : W=tensor([0.9826], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0396], requires_grad=True), b.grad=tensor([0.0095])\n",
      "=======================927==================\n",
      "hypothesis=\n",
      "tensor([[1.0222],\n",
      "        [2.0048],\n",
      "        [2.9873]], grad_fn=<AddBackward0>)\n",
      "loss=0.00022482390340883285\n",
      "before backward() : W=tensor([0.9826], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0396], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9826], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0396], requires_grad=True), b.grad=tensor([0.0095])\n",
      "after  step() : W=tensor([0.9826], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0395], requires_grad=True), b.grad=tensor([0.0095])\n",
      "=======================928==================\n",
      "hypothesis=\n",
      "tensor([[1.0221],\n",
      "        [2.0047],\n",
      "        [2.9874]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002237457229057327\n",
      "before backward() : W=tensor([0.9826], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0395], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9826], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0395], requires_grad=True), b.grad=tensor([0.0095])\n",
      "after  step() : W=tensor([0.9827], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0394], requires_grad=True), b.grad=tensor([0.0095])\n",
      "=======================929==================\n",
      "hypothesis=\n",
      "tensor([[1.0221],\n",
      "        [2.0047],\n",
      "        [2.9874]], grad_fn=<AddBackward0>)\n",
      "loss=0.00022266989981289953\n",
      "before backward() : W=tensor([0.9827], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0394], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9827], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0394], requires_grad=True), b.grad=tensor([0.0095])\n",
      "after  step() : W=tensor([0.9827], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0393], requires_grad=True), b.grad=tensor([0.0095])\n",
      "=======================930==================\n",
      "hypothesis=\n",
      "tensor([[1.0220],\n",
      "        [2.0047],\n",
      "        [2.9874]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002216004068031907\n",
      "before backward() : W=tensor([0.9827], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0393], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9827], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0393], requires_grad=True), b.grad=tensor([0.0094])\n",
      "after  step() : W=tensor([0.9828], requires_grad=True), W.grad=tensor([-0.0042]), b=tensor([0.0392], requires_grad=True), b.grad=tensor([0.0094])\n",
      "=======================931==================\n",
      "hypothesis=\n",
      "tensor([[1.0220],\n",
      "        [2.0047],\n",
      "        [2.9875]], grad_fn=<AddBackward0>)\n",
      "loss=0.00022053597785998136\n",
      "before backward() : W=tensor([0.9828], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0392], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9828], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0392], requires_grad=True), b.grad=tensor([0.0094])\n",
      "after  step() : W=tensor([0.9828], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0391], requires_grad=True), b.grad=tensor([0.0094])\n",
      "=======================932==================\n",
      "hypothesis=\n",
      "tensor([[1.0219],\n",
      "        [2.0047],\n",
      "        [2.9875]], grad_fn=<AddBackward0>)\n",
      "loss=0.00021947710774838924\n",
      "before backward() : W=tensor([0.9828], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0391], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9828], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0391], requires_grad=True), b.grad=tensor([0.0094])\n",
      "after  step() : W=tensor([0.9828], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0390], requires_grad=True), b.grad=tensor([0.0094])\n",
      "=======================933==================\n",
      "hypothesis=\n",
      "tensor([[1.0219],\n",
      "        [2.0047],\n",
      "        [2.9875]], grad_fn=<AddBackward0>)\n",
      "loss=0.00021842301066499203\n",
      "before backward() : W=tensor([0.9828], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0390], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9828], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0390], requires_grad=True), b.grad=tensor([0.0094])\n",
      "after  step() : W=tensor([0.9829], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0389], requires_grad=True), b.grad=tensor([0.0094])\n",
      "=======================934==================\n",
      "hypothesis=\n",
      "tensor([[1.0218],\n",
      "        [2.0047],\n",
      "        [2.9876]], grad_fn=<AddBackward0>)\n",
      "loss=0.00021737662609666586\n",
      "before backward() : W=tensor([0.9829], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0389], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9829], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0389], requires_grad=True), b.grad=tensor([0.0094])\n",
      "after  step() : W=tensor([0.9829], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0388], requires_grad=True), b.grad=tensor([0.0094])\n",
      "=======================935==================\n",
      "hypothesis=\n",
      "tensor([[1.0218],\n",
      "        [2.0047],\n",
      "        [2.9876]], grad_fn=<AddBackward0>)\n",
      "loss=0.00021632931020576507\n",
      "before backward() : W=tensor([0.9829], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0388], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9829], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0388], requires_grad=True), b.grad=tensor([0.0093])\n",
      "after  step() : W=tensor([0.9830], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0387], requires_grad=True), b.grad=tensor([0.0093])\n",
      "=======================936==================\n",
      "hypothesis=\n",
      "tensor([[1.0217],\n",
      "        [2.0047],\n",
      "        [2.9876]], grad_fn=<AddBackward0>)\n",
      "loss=0.00021529193327296525\n",
      "before backward() : W=tensor([0.9830], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0387], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9830], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0387], requires_grad=True), b.grad=tensor([0.0093])\n",
      "after  step() : W=tensor([0.9830], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0386], requires_grad=True), b.grad=tensor([0.0093])\n",
      "=======================937==================\n",
      "hypothesis=\n",
      "tensor([[1.0216],\n",
      "        [2.0046],\n",
      "        [2.9876]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002142570447176695\n",
      "before backward() : W=tensor([0.9830], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0386], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9830], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0386], requires_grad=True), b.grad=tensor([0.0093])\n",
      "after  step() : W=tensor([0.9830], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0386], requires_grad=True), b.grad=tensor([0.0093])\n",
      "=======================938==================\n",
      "hypothesis=\n",
      "tensor([[1.0216],\n",
      "        [2.0046],\n",
      "        [2.9877]], grad_fn=<AddBackward0>)\n",
      "loss=0.00021323077089618891\n",
      "before backward() : W=tensor([0.9830], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0386], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9830], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0386], requires_grad=True), b.grad=tensor([0.0093])\n",
      "after  step() : W=tensor([0.9831], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0385], requires_grad=True), b.grad=tensor([0.0093])\n",
      "=======================939==================\n",
      "hypothesis=\n",
      "tensor([[1.0215],\n",
      "        [2.0046],\n",
      "        [2.9877]], grad_fn=<AddBackward0>)\n",
      "loss=0.00021220622875262052\n",
      "before backward() : W=tensor([0.9831], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0385], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9831], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0385], requires_grad=True), b.grad=tensor([0.0092])\n",
      "after  step() : W=tensor([0.9831], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0384], requires_grad=True), b.grad=tensor([0.0092])\n",
      "=======================940==================\n",
      "hypothesis=\n",
      "tensor([[1.0215],\n",
      "        [2.0046],\n",
      "        [2.9877]], grad_fn=<AddBackward0>)\n",
      "loss=0.00021118561562616378\n",
      "before backward() : W=tensor([0.9831], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0384], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9831], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0384], requires_grad=True), b.grad=tensor([0.0092])\n",
      "after  step() : W=tensor([0.9832], requires_grad=True), W.grad=tensor([-0.0041]), b=tensor([0.0383], requires_grad=True), b.grad=tensor([0.0092])\n",
      "=======================941==================\n",
      "hypothesis=\n",
      "tensor([[1.0214],\n",
      "        [2.0046],\n",
      "        [2.9878]], grad_fn=<AddBackward0>)\n",
      "loss=0.00021017207473050803\n",
      "before backward() : W=tensor([0.9832], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0383], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9832], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0383], requires_grad=True), b.grad=tensor([0.0092])\n",
      "after  step() : W=tensor([0.9832], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0382], requires_grad=True), b.grad=tensor([0.0092])\n",
      "=======================942==================\n",
      "hypothesis=\n",
      "tensor([[1.0214],\n",
      "        [2.0046],\n",
      "        [2.9878]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002091617207042873\n",
      "before backward() : W=tensor([0.9832], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0382], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9832], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0382], requires_grad=True), b.grad=tensor([0.0092])\n",
      "after  step() : W=tensor([0.9832], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0381], requires_grad=True), b.grad=tensor([0.0092])\n",
      "=======================943==================\n",
      "hypothesis=\n",
      "tensor([[1.0213],\n",
      "        [2.0046],\n",
      "        [2.9878]], grad_fn=<AddBackward0>)\n",
      "loss=0.00020815645984839648\n",
      "before backward() : W=tensor([0.9832], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0381], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9832], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0381], requires_grad=True), b.grad=tensor([0.0092])\n",
      "after  step() : W=tensor([0.9833], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0380], requires_grad=True), b.grad=tensor([0.0092])\n",
      "=======================944==================\n",
      "hypothesis=\n",
      "tensor([[1.0213],\n",
      "        [2.0046],\n",
      "        [2.9879]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002071589115075767\n",
      "before backward() : W=tensor([0.9833], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0380], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9833], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0380], requires_grad=True), b.grad=tensor([0.0091])\n",
      "after  step() : W=tensor([0.9833], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0379], requires_grad=True), b.grad=tensor([0.0091])\n",
      "=======================945==================\n",
      "hypothesis=\n",
      "tensor([[1.0212],\n",
      "        [2.0046],\n",
      "        [2.9879]], grad_fn=<AddBackward0>)\n",
      "loss=0.00020616354595404118\n",
      "before backward() : W=tensor([0.9833], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0379], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9833], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0379], requires_grad=True), b.grad=tensor([0.0091])\n",
      "after  step() : W=tensor([0.9834], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0378], requires_grad=True), b.grad=tensor([0.0091])\n",
      "=======================946==================\n",
      "hypothesis=\n",
      "tensor([[1.0212],\n",
      "        [2.0045],\n",
      "        [2.9879]], grad_fn=<AddBackward0>)\n",
      "loss=0.00020517416123766452\n",
      "before backward() : W=tensor([0.9834], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0378], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9834], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0378], requires_grad=True), b.grad=tensor([0.0091])\n",
      "after  step() : W=tensor([0.9834], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0377], requires_grad=True), b.grad=tensor([0.0091])\n",
      "=======================947==================\n",
      "hypothesis=\n",
      "tensor([[1.0211],\n",
      "        [2.0045],\n",
      "        [2.9879]], grad_fn=<AddBackward0>)\n",
      "loss=0.00020418886560946703\n",
      "before backward() : W=tensor([0.9834], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0377], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9834], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0377], requires_grad=True), b.grad=tensor([0.0091])\n",
      "after  step() : W=tensor([0.9834], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0376], requires_grad=True), b.grad=tensor([0.0091])\n",
      "=======================948==================\n",
      "hypothesis=\n",
      "tensor([[1.0211],\n",
      "        [2.0045],\n",
      "        [2.9880]], grad_fn=<AddBackward0>)\n",
      "loss=0.0002032076008617878\n",
      "before backward() : W=tensor([0.9834], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0376], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9834], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0376], requires_grad=True), b.grad=tensor([0.0090])\n",
      "after  step() : W=tensor([0.9835], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0375], requires_grad=True), b.grad=tensor([0.0090])\n",
      "=======================949==================\n",
      "hypothesis=\n",
      "tensor([[1.0210],\n",
      "        [2.0045],\n",
      "        [2.9880]], grad_fn=<AddBackward0>)\n",
      "loss=0.00020223298633936793\n",
      "before backward() : W=tensor([0.9835], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0375], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9835], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0375], requires_grad=True), b.grad=tensor([0.0090])\n",
      "after  step() : W=tensor([0.9835], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0375], requires_grad=True), b.grad=tensor([0.0090])\n",
      "=======================950==================\n",
      "hypothesis=\n",
      "tensor([[1.0210],\n",
      "        [2.0045],\n",
      "        [2.9880]], grad_fn=<AddBackward0>)\n",
      "loss=0.00020126190793234855\n",
      "before backward() : W=tensor([0.9835], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0375], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9835], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0375], requires_grad=True), b.grad=tensor([0.0090])\n",
      "after  step() : W=tensor([0.9836], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0374], requires_grad=True), b.grad=tensor([0.0090])\n",
      "=======================951==================\n",
      "hypothesis=\n",
      "tensor([[1.0209],\n",
      "        [2.0045],\n",
      "        [2.9881]], grad_fn=<AddBackward0>)\n",
      "loss=0.00020029507868457586\n",
      "before backward() : W=tensor([0.9836], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0374], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9836], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0374], requires_grad=True), b.grad=tensor([0.0090])\n",
      "after  step() : W=tensor([0.9836], requires_grad=True), W.grad=tensor([-0.0040]), b=tensor([0.0373], requires_grad=True), b.grad=tensor([0.0090])\n",
      "=======================952==================\n",
      "hypothesis=\n",
      "tensor([[1.0209],\n",
      "        [2.0045],\n",
      "        [2.9881]], grad_fn=<AddBackward0>)\n",
      "loss=0.00019933439034502953\n",
      "before backward() : W=tensor([0.9836], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0373], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9836], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0373], requires_grad=True), b.grad=tensor([0.0090])\n",
      "after  step() : W=tensor([0.9836], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0372], requires_grad=True), b.grad=tensor([0.0090])\n",
      "=======================953==================\n",
      "hypothesis=\n",
      "tensor([[1.0208],\n",
      "        [2.0045],\n",
      "        [2.9881]], grad_fn=<AddBackward0>)\n",
      "loss=0.00019837525906041265\n",
      "before backward() : W=tensor([0.9836], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0372], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9836], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0372], requires_grad=True), b.grad=tensor([0.0089])\n",
      "after  step() : W=tensor([0.9837], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0371], requires_grad=True), b.grad=tensor([0.0089])\n",
      "=======================954==================\n",
      "hypothesis=\n",
      "tensor([[1.0208],\n",
      "        [2.0045],\n",
      "        [2.9881]], grad_fn=<AddBackward0>)\n",
      "loss=0.0001974241022253409\n",
      "before backward() : W=tensor([0.9837], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0371], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9837], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0371], requires_grad=True), b.grad=tensor([0.0089])\n",
      "after  step() : W=tensor([0.9837], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0370], requires_grad=True), b.grad=tensor([0.0089])\n",
      "=======================955==================\n",
      "hypothesis=\n",
      "tensor([[1.0207],\n",
      "        [2.0044],\n",
      "        [2.9882]], grad_fn=<AddBackward0>)\n",
      "loss=0.00019647549197543412\n",
      "before backward() : W=tensor([0.9837], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0370], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9837], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0370], requires_grad=True), b.grad=tensor([0.0089])\n",
      "after  step() : W=tensor([0.9838], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0369], requires_grad=True), b.grad=tensor([0.0089])\n",
      "=======================956==================\n",
      "hypothesis=\n",
      "tensor([[1.0207],\n",
      "        [2.0044],\n",
      "        [2.9882]], grad_fn=<AddBackward0>)\n",
      "loss=0.00019553292077034712\n",
      "before backward() : W=tensor([0.9838], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0369], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9838], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0369], requires_grad=True), b.grad=tensor([0.0089])\n",
      "after  step() : W=tensor([0.9838], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0368], requires_grad=True), b.grad=tensor([0.0089])\n",
      "=======================957==================\n",
      "hypothesis=\n",
      "tensor([[1.0206],\n",
      "        [2.0044],\n",
      "        [2.9882]], grad_fn=<AddBackward0>)\n",
      "loss=0.00019459375471342355\n",
      "before backward() : W=tensor([0.9838], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0368], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9838], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0368], requires_grad=True), b.grad=tensor([0.0089])\n",
      "after  step() : W=tensor([0.9838], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0367], requires_grad=True), b.grad=tensor([0.0089])\n",
      "=======================958==================\n",
      "hypothesis=\n",
      "tensor([[1.0206],\n",
      "        [2.0044],\n",
      "        [2.9883]], grad_fn=<AddBackward0>)\n",
      "loss=0.0001936587505042553\n",
      "before backward() : W=tensor([0.9838], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0367], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9838], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0367], requires_grad=True), b.grad=tensor([0.0088])\n",
      "after  step() : W=tensor([0.9839], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0367], requires_grad=True), b.grad=tensor([0.0088])\n",
      "=======================959==================\n",
      "hypothesis=\n",
      "tensor([[1.0205],\n",
      "        [2.0044],\n",
      "        [2.9883]], grad_fn=<AddBackward0>)\n",
      "loss=0.00019272971258033067\n",
      "before backward() : W=tensor([0.9839], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0367], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9839], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0367], requires_grad=True), b.grad=tensor([0.0088])\n",
      "after  step() : W=tensor([0.9839], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0366], requires_grad=True), b.grad=tensor([0.0088])\n",
      "=======================960==================\n",
      "hypothesis=\n",
      "tensor([[1.0205],\n",
      "        [2.0044],\n",
      "        [2.9883]], grad_fn=<AddBackward0>)\n",
      "loss=0.00019180246454197913\n",
      "before backward() : W=tensor([0.9839], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0366], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9839], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0366], requires_grad=True), b.grad=tensor([0.0088])\n",
      "after  step() : W=tensor([0.9840], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0365], requires_grad=True), b.grad=tensor([0.0088])\n",
      "=======================961==================\n",
      "hypothesis=\n",
      "tensor([[1.0204],\n",
      "        [2.0044],\n",
      "        [2.9883]], grad_fn=<AddBackward0>)\n",
      "loss=0.00019088113913312554\n",
      "before backward() : W=tensor([0.9840], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0365], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9840], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0365], requires_grad=True), b.grad=tensor([0.0088])\n",
      "after  step() : W=tensor([0.9840], requires_grad=True), W.grad=tensor([-0.0039]), b=tensor([0.0364], requires_grad=True), b.grad=tensor([0.0088])\n",
      "=======================962==================\n",
      "hypothesis=\n",
      "tensor([[1.0204],\n",
      "        [2.0044],\n",
      "        [2.9884]], grad_fn=<AddBackward0>)\n",
      "loss=0.00018996343715116382\n",
      "before backward() : W=tensor([0.9840], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0364], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9840], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0364], requires_grad=True), b.grad=tensor([0.0087])\n",
      "after  step() : W=tensor([0.9840], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0363], requires_grad=True), b.grad=tensor([0.0087])\n",
      "=======================963==================\n",
      "hypothesis=\n",
      "tensor([[1.0203],\n",
      "        [2.0044],\n",
      "        [2.9884]], grad_fn=<AddBackward0>)\n",
      "loss=0.00018905416072811931\n",
      "before backward() : W=tensor([0.9840], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0363], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9840], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0363], requires_grad=True), b.grad=tensor([0.0087])\n",
      "after  step() : W=tensor([0.9841], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0362], requires_grad=True), b.grad=tensor([0.0087])\n",
      "=======================964==================\n",
      "hypothesis=\n",
      "tensor([[1.0203],\n",
      "        [2.0044],\n",
      "        [2.9884]], grad_fn=<AddBackward0>)\n",
      "loss=0.0001881463686004281\n",
      "before backward() : W=tensor([0.9841], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0362], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9841], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0362], requires_grad=True), b.grad=tensor([0.0087])\n",
      "after  step() : W=tensor([0.9841], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0361], requires_grad=True), b.grad=tensor([0.0087])\n",
      "=======================965==================\n",
      "hypothesis=\n",
      "tensor([[1.0202],\n",
      "        [2.0043],\n",
      "        [2.9884]], grad_fn=<AddBackward0>)\n",
      "loss=0.00018724008987192065\n",
      "before backward() : W=tensor([0.9841], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0361], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9841], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0361], requires_grad=True), b.grad=tensor([0.0087])\n",
      "after  step() : W=tensor([0.9841], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0360], requires_grad=True), b.grad=tensor([0.0087])\n",
      "=======================966==================\n",
      "hypothesis=\n",
      "tensor([[1.0202],\n",
      "        [2.0043],\n",
      "        [2.9885]], grad_fn=<AddBackward0>)\n",
      "loss=0.00018634241132531315\n",
      "before backward() : W=tensor([0.9841], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0360], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9841], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0360], requires_grad=True), b.grad=tensor([0.0087])\n",
      "after  step() : W=tensor([0.9842], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0360], requires_grad=True), b.grad=tensor([0.0087])\n",
      "=======================967==================\n",
      "hypothesis=\n",
      "tensor([[1.0201],\n",
      "        [2.0043],\n",
      "        [2.9885]], grad_fn=<AddBackward0>)\n",
      "loss=0.00018544780323281884\n",
      "before backward() : W=tensor([0.9842], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0360], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9842], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0360], requires_grad=True), b.grad=tensor([0.0086])\n",
      "after  step() : W=tensor([0.9842], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0359], requires_grad=True), b.grad=tensor([0.0086])\n",
      "=======================968==================\n",
      "hypothesis=\n",
      "tensor([[1.0201],\n",
      "        [2.0043],\n",
      "        [2.9885]], grad_fn=<AddBackward0>)\n",
      "loss=0.00018455693498253822\n",
      "before backward() : W=tensor([0.9842], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0359], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9842], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0359], requires_grad=True), b.grad=tensor([0.0086])\n",
      "after  step() : W=tensor([0.9843], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0358], requires_grad=True), b.grad=tensor([0.0086])\n",
      "=======================969==================\n",
      "hypothesis=\n",
      "tensor([[1.0200],\n",
      "        [2.0043],\n",
      "        [2.9886]], grad_fn=<AddBackward0>)\n",
      "loss=0.00018367233860772103\n",
      "before backward() : W=tensor([0.9843], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0358], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9843], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0358], requires_grad=True), b.grad=tensor([0.0086])\n",
      "after  step() : W=tensor([0.9843], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0357], requires_grad=True), b.grad=tensor([0.0086])\n",
      "=======================970==================\n",
      "hypothesis=\n",
      "tensor([[1.0200],\n",
      "        [2.0043],\n",
      "        [2.9886]], grad_fn=<AddBackward0>)\n",
      "loss=0.00018278893548995256\n",
      "before backward() : W=tensor([0.9843], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0357], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9843], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0357], requires_grad=True), b.grad=tensor([0.0086])\n",
      "after  step() : W=tensor([0.9843], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0356], requires_grad=True), b.grad=tensor([0.0086])\n",
      "=======================971==================\n",
      "hypothesis=\n",
      "tensor([[1.0199],\n",
      "        [2.0043],\n",
      "        [2.9886]], grad_fn=<AddBackward0>)\n",
      "loss=0.00018191193521488458\n",
      "before backward() : W=tensor([0.9843], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0356], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9843], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0356], requires_grad=True), b.grad=tensor([0.0086])\n",
      "after  step() : W=tensor([0.9844], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0355], requires_grad=True), b.grad=tensor([0.0086])\n",
      "=======================972==================\n",
      "hypothesis=\n",
      "tensor([[1.0199],\n",
      "        [2.0043],\n",
      "        [2.9886]], grad_fn=<AddBackward0>)\n",
      "loss=0.00018103844195138663\n",
      "before backward() : W=tensor([0.9844], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0355], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9844], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0355], requires_grad=True), b.grad=tensor([0.0085])\n",
      "after  step() : W=tensor([0.9844], requires_grad=True), W.grad=tensor([-0.0038]), b=tensor([0.0354], requires_grad=True), b.grad=tensor([0.0085])\n",
      "=======================973==================\n",
      "hypothesis=\n",
      "tensor([[1.0198],\n",
      "        [2.0043],\n",
      "        [2.9887]], grad_fn=<AddBackward0>)\n",
      "loss=0.00018016861577052623\n",
      "before backward() : W=tensor([0.9844], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0354], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9844], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0354], requires_grad=True), b.grad=tensor([0.0085])\n",
      "after  step() : W=tensor([0.9844], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0354], requires_grad=True), b.grad=tensor([0.0085])\n",
      "=======================974==================\n",
      "hypothesis=\n",
      "tensor([[1.0198],\n",
      "        [2.0042],\n",
      "        [2.9887]], grad_fn=<AddBackward0>)\n",
      "loss=0.0001793042611097917\n",
      "before backward() : W=tensor([0.9844], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0354], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9844], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0354], requires_grad=True), b.grad=tensor([0.0085])\n",
      "after  step() : W=tensor([0.9845], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0353], requires_grad=True), b.grad=tensor([0.0085])\n",
      "=======================975==================\n",
      "hypothesis=\n",
      "tensor([[1.0198],\n",
      "        [2.0042],\n",
      "        [2.9887]], grad_fn=<AddBackward0>)\n",
      "loss=0.0001784419728210196\n",
      "before backward() : W=tensor([0.9845], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0353], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9845], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0353], requires_grad=True), b.grad=tensor([0.0085])\n",
      "after  step() : W=tensor([0.9845], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0352], requires_grad=True), b.grad=tensor([0.0085])\n",
      "=======================976==================\n",
      "hypothesis=\n",
      "tensor([[1.0197],\n",
      "        [2.0042],\n",
      "        [2.9888]], grad_fn=<AddBackward0>)\n",
      "loss=0.00017758489411789924\n",
      "before backward() : W=tensor([0.9845], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0352], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9845], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0352], requires_grad=True), b.grad=tensor([0.0085])\n",
      "after  step() : W=tensor([0.9846], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0351], requires_grad=True), b.grad=tensor([0.0085])\n",
      "=======================977==================\n",
      "hypothesis=\n",
      "tensor([[1.0197],\n",
      "        [2.0042],\n",
      "        [2.9888]], grad_fn=<AddBackward0>)\n",
      "loss=0.00017673389811534435\n",
      "before backward() : W=tensor([0.9846], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0351], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9846], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0351], requires_grad=True), b.grad=tensor([0.0084])\n",
      "after  step() : W=tensor([0.9846], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0350], requires_grad=True), b.grad=tensor([0.0084])\n",
      "=======================978==================\n",
      "hypothesis=\n",
      "tensor([[1.0196],\n",
      "        [2.0042],\n",
      "        [2.9888]], grad_fn=<AddBackward0>)\n",
      "loss=0.00017588249465916306\n",
      "before backward() : W=tensor([0.9846], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0350], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9846], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0350], requires_grad=True), b.grad=tensor([0.0084])\n",
      "after  step() : W=tensor([0.9846], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0349], requires_grad=True), b.grad=tensor([0.0084])\n",
      "=======================979==================\n",
      "hypothesis=\n",
      "tensor([[1.0196],\n",
      "        [2.0042],\n",
      "        [2.9888]], grad_fn=<AddBackward0>)\n",
      "loss=0.0001750404917402193\n",
      "before backward() : W=tensor([0.9846], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0349], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9846], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0349], requires_grad=True), b.grad=tensor([0.0084])\n",
      "after  step() : W=tensor([0.9847], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0348], requires_grad=True), b.grad=tensor([0.0084])\n",
      "=======================980==================\n",
      "hypothesis=\n",
      "tensor([[1.0195],\n",
      "        [2.0042],\n",
      "        [2.9889]], grad_fn=<AddBackward0>)\n",
      "loss=0.0001741980668157339\n",
      "before backward() : W=tensor([0.9847], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0348], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9847], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0348], requires_grad=True), b.grad=tensor([0.0084])\n",
      "after  step() : W=tensor([0.9847], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0348], requires_grad=True), b.grad=tensor([0.0084])\n",
      "=======================981==================\n",
      "hypothesis=\n",
      "tensor([[1.0195],\n",
      "        [2.0042],\n",
      "        [2.9889]], grad_fn=<AddBackward0>)\n",
      "loss=0.00017336143355350941\n",
      "before backward() : W=tensor([0.9847], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0348], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9847], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0348], requires_grad=True), b.grad=tensor([0.0084])\n",
      "after  step() : W=tensor([0.9847], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0347], requires_grad=True), b.grad=tensor([0.0084])\n",
      "=======================982==================\n",
      "hypothesis=\n",
      "tensor([[1.0194],\n",
      "        [2.0042],\n",
      "        [2.9889]], grad_fn=<AddBackward0>)\n",
      "loss=0.00017253031546715647\n",
      "before backward() : W=tensor([0.9847], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0347], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9847], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0347], requires_grad=True), b.grad=tensor([0.0083])\n",
      "after  step() : W=tensor([0.9848], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0346], requires_grad=True), b.grad=tensor([0.0083])\n",
      "=======================983==================\n",
      "hypothesis=\n",
      "tensor([[1.0194],\n",
      "        [2.0042],\n",
      "        [2.9889]], grad_fn=<AddBackward0>)\n",
      "loss=0.00017170100181829184\n",
      "before backward() : W=tensor([0.9848], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0346], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9848], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0346], requires_grad=True), b.grad=tensor([0.0083])\n",
      "after  step() : W=tensor([0.9848], requires_grad=True), W.grad=tensor([-0.0037]), b=tensor([0.0345], requires_grad=True), b.grad=tensor([0.0083])\n",
      "=======================984==================\n",
      "hypothesis=\n",
      "tensor([[1.0193],\n",
      "        [2.0041],\n",
      "        [2.9890]], grad_fn=<AddBackward0>)\n",
      "loss=0.00017087673768401146\n",
      "before backward() : W=tensor([0.9848], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0345], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9848], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0345], requires_grad=True), b.grad=tensor([0.0083])\n",
      "after  step() : W=tensor([0.9849], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0344], requires_grad=True), b.grad=tensor([0.0083])\n",
      "=======================985==================\n",
      "hypothesis=\n",
      "tensor([[1.0193],\n",
      "        [2.0041],\n",
      "        [2.9890]], grad_fn=<AddBackward0>)\n",
      "loss=0.00017005622794386\n",
      "before backward() : W=tensor([0.9849], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0344], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9849], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0344], requires_grad=True), b.grad=tensor([0.0083])\n",
      "after  step() : W=tensor([0.9849], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0343], requires_grad=True), b.grad=tensor([0.0083])\n",
      "=======================986==================\n",
      "hypothesis=\n",
      "tensor([[1.0192],\n",
      "        [2.0041],\n",
      "        [2.9890]], grad_fn=<AddBackward0>)\n",
      "loss=0.0001692381192697212\n",
      "before backward() : W=tensor([0.9849], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0343], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9849], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0343], requires_grad=True), b.grad=tensor([0.0083])\n",
      "after  step() : W=tensor([0.9849], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0343], requires_grad=True), b.grad=tensor([0.0083])\n",
      "=======================987==================\n",
      "hypothesis=\n",
      "tensor([[1.0192],\n",
      "        [2.0041],\n",
      "        [2.9890]], grad_fn=<AddBackward0>)\n",
      "loss=0.0001684281014604494\n",
      "before backward() : W=tensor([0.9849], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0343], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9849], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0343], requires_grad=True), b.grad=tensor([0.0082])\n",
      "after  step() : W=tensor([0.9850], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0342], requires_grad=True), b.grad=tensor([0.0082])\n",
      "=======================988==================\n",
      "hypothesis=\n",
      "tensor([[1.0191],\n",
      "        [2.0041],\n",
      "        [2.9891]], grad_fn=<AddBackward0>)\n",
      "loss=0.0001676180399954319\n",
      "before backward() : W=tensor([0.9850], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0342], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9850], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0342], requires_grad=True), b.grad=tensor([0.0082])\n",
      "after  step() : W=tensor([0.9850], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0341], requires_grad=True), b.grad=tensor([0.0082])\n",
      "=======================989==================\n",
      "hypothesis=\n",
      "tensor([[1.0191],\n",
      "        [2.0041],\n",
      "        [2.9891]], grad_fn=<AddBackward0>)\n",
      "loss=0.00016681211127433926\n",
      "before backward() : W=tensor([0.9850], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0341], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9850], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0341], requires_grad=True), b.grad=tensor([0.0082])\n",
      "after  step() : W=tensor([0.9850], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0340], requires_grad=True), b.grad=tensor([0.0082])\n",
      "=======================990==================\n",
      "hypothesis=\n",
      "tensor([[1.0191],\n",
      "        [2.0041],\n",
      "        [2.9891]], grad_fn=<AddBackward0>)\n",
      "loss=0.0001660130947129801\n",
      "before backward() : W=tensor([0.9850], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0340], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9850], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0340], requires_grad=True), b.grad=tensor([0.0082])\n",
      "after  step() : W=tensor([0.9851], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0339], requires_grad=True), b.grad=tensor([0.0082])\n",
      "=======================991==================\n",
      "hypothesis=\n",
      "tensor([[1.0190],\n",
      "        [2.0041],\n",
      "        [2.9892]], grad_fn=<AddBackward0>)\n",
      "loss=0.0001652158098295331\n",
      "before backward() : W=tensor([0.9851], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0339], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9851], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0339], requires_grad=True), b.grad=tensor([0.0082])\n",
      "after  step() : W=tensor([0.9851], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0339], requires_grad=True), b.grad=tensor([0.0082])\n",
      "=======================992==================\n",
      "hypothesis=\n",
      "tensor([[1.0190],\n",
      "        [2.0041],\n",
      "        [2.9892]], grad_fn=<AddBackward0>)\n",
      "loss=0.00016442191554233432\n",
      "before backward() : W=tensor([0.9851], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0339], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9851], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0339], requires_grad=True), b.grad=tensor([0.0081])\n",
      "after  step() : W=tensor([0.9851], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0338], requires_grad=True), b.grad=tensor([0.0081])\n",
      "=======================993==================\n",
      "hypothesis=\n",
      "tensor([[1.0189],\n",
      "        [2.0041],\n",
      "        [2.9892]], grad_fn=<AddBackward0>)\n",
      "loss=0.00016363167378585786\n",
      "before backward() : W=tensor([0.9851], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0338], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9851], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0338], requires_grad=True), b.grad=tensor([0.0081])\n",
      "after  step() : W=tensor([0.9852], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0337], requires_grad=True), b.grad=tensor([0.0081])\n",
      "=======================994==================\n",
      "hypothesis=\n",
      "tensor([[1.0189],\n",
      "        [2.0040],\n",
      "        [2.9892]], grad_fn=<AddBackward0>)\n",
      "loss=0.0001628463069209829\n",
      "before backward() : W=tensor([0.9852], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0337], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9852], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0337], requires_grad=True), b.grad=tensor([0.0081])\n",
      "after  step() : W=tensor([0.9852], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0336], requires_grad=True), b.grad=tensor([0.0081])\n",
      "=======================995==================\n",
      "hypothesis=\n",
      "tensor([[1.0188],\n",
      "        [2.0040],\n",
      "        [2.9893]], grad_fn=<AddBackward0>)\n",
      "loss=0.0001620645634829998\n",
      "before backward() : W=tensor([0.9852], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0336], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9852], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0336], requires_grad=True), b.grad=tensor([0.0081])\n",
      "after  step() : W=tensor([0.9852], requires_grad=True), W.grad=tensor([-0.0036]), b=tensor([0.0335], requires_grad=True), b.grad=tensor([0.0081])\n",
      "=======================996==================\n",
      "hypothesis=\n",
      "tensor([[1.0188],\n",
      "        [2.0040],\n",
      "        [2.9893]], grad_fn=<AddBackward0>)\n",
      "loss=0.00016128596325870603\n",
      "before backward() : W=tensor([0.9852], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0335], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9852], requires_grad=True), W.grad=tensor([-0.0035]), b=tensor([0.0335], requires_grad=True), b.grad=tensor([0.0081])\n",
      "after  step() : W=tensor([0.9853], requires_grad=True), W.grad=tensor([-0.0035]), b=tensor([0.0334], requires_grad=True), b.grad=tensor([0.0081])\n",
      "=======================997==================\n",
      "hypothesis=\n",
      "tensor([[1.0187],\n",
      "        [2.0040],\n",
      "        [2.9893]], grad_fn=<AddBackward0>)\n",
      "loss=0.000160509895067662\n",
      "before backward() : W=tensor([0.9853], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0334], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9853], requires_grad=True), W.grad=tensor([-0.0035]), b=tensor([0.0334], requires_grad=True), b.grad=tensor([0.0080])\n",
      "after  step() : W=tensor([0.9853], requires_grad=True), W.grad=tensor([-0.0035]), b=tensor([0.0334], requires_grad=True), b.grad=tensor([0.0080])\n",
      "=======================998==================\n",
      "hypothesis=\n",
      "tensor([[1.0187],\n",
      "        [2.0040],\n",
      "        [2.9893]], grad_fn=<AddBackward0>)\n",
      "loss=0.00015974161215126514\n",
      "before backward() : W=tensor([0.9853], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0334], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9853], requires_grad=True), W.grad=tensor([-0.0035]), b=tensor([0.0334], requires_grad=True), b.grad=tensor([0.0080])\n",
      "after  step() : W=tensor([0.9854], requires_grad=True), W.grad=tensor([-0.0035]), b=tensor([0.0333], requires_grad=True), b.grad=tensor([0.0080])\n",
      "=======================999==================\n",
      "hypothesis=\n",
      "tensor([[1.0186],\n",
      "        [2.0040],\n",
      "        [2.9894]], grad_fn=<AddBackward0>)\n",
      "loss=0.0001589732855791226\n",
      "before backward() : W=tensor([0.9854], requires_grad=True), W.grad=tensor([0.]), b=tensor([0.0333], requires_grad=True), b.grad=tensor([0.])\n",
      "after  backward() : W=tensor([0.9854], requires_grad=True), W.grad=tensor([-0.0035]), b=tensor([0.0333], requires_grad=True), b.grad=tensor([0.0080])\n",
      "after  step() : W=tensor([0.9854], requires_grad=True), W.grad=tensor([-0.0035]), b=tensor([0.0332], requires_grad=True), b.grad=tensor([0.0080])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "print(x_train)\n",
    "print(y_train)\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "print(f'W={W}\\nb={b}')\n",
    "\n",
    "W_col = []\n",
    "b_col = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    print(f\"======================={epoch}==================\")\n",
    "\n",
    "    # forward( Hypothesis )\n",
    "    hypothesis = x_train * W + b\n",
    "    print(f'hypothesis=\\n{hypothesis}')\n",
    "\n",
    "    # Loss = MSE\n",
    "    loss = torch.mean((hypothesis - y_train)**2)\n",
    "    learning_rate = 0.01\n",
    "    print(f'loss={loss}')\n",
    "\n",
    "    # backward\n",
    "    print(f'before backward() : W={W}, W.grad={W.grad}, b={b}, b.grad={b.grad}')\n",
    "    loss.backward()\n",
    "    print(f'after  backward() : W={W}, W.grad={W.grad}, b={b}, b.grad={b.grad}')\n",
    "\n",
    "    # step\n",
    "    with torch.no_grad():\n",
    "        W -= learning_rate*W.grad\n",
    "        b -= learning_rate*b.grad\n",
    "    print(f'after  step() : W={W}, W.grad={W.grad}, b={b}, b.grad={b.grad}')\n",
    "    W_col.append(W.item())\n",
    "    b_col.append(b.item())\n",
    "\n",
    "    # Gradient 초기화\n",
    "    if W.grad is not None:\n",
    "        W.grad.zero_()\n",
    "    if b.grad is not None:\n",
    "        b.grad.zero_()\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(W_col)\n",
    "plt.xlabel('EPoch')\n",
    "plt.ylabel('W')\n",
    "plt.grid()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(b_col)\n",
    "plt.xlabel('EPoch')\n",
    "plt.ylabel('b')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs/linear_regression')\n",
    "\n",
    "x_train = torch.FloatTensor([[1.], [2.], [3.]])\n",
    "y_train = torch.FloatTensor([[1.], [2.], [3.]])\n",
    "\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "lr = 0.01\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    # forward\n",
    "    hypothesis = x_train*W + b\n",
    "    cost = torch.mean((hypothesis-y_train)**2) # MSE\n",
    "\n",
    "    #backward\n",
    "    if W.grad is not None:\n",
    "        W.grad.zero_()\n",
    "    if b.grad is not None:\n",
    "        b.grad.zero_()\n",
    "    cost.backward()\n",
    "\n",
    "    # step\n",
    "    with torch.no_grad():\n",
    "        W -= lr*W.grad\n",
    "        b -= lr*b.grad\n",
    "\n",
    "    # log\n",
    "    writer.add_scalar('Loss', cost.item(), epoch)\n",
    "    writer.add_scalars('Parameters', {\n",
    "        'W' : W.item(),\n",
    "        'b' : b.item(),\n",
    "    }, epoch)\n",
    "    writer.add_histogram('Parameters/W_hist', W, epoch)\n",
    "\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ae2b2d5f27b72743\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ae2b2d5f27b72743\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "점점 $H(x)$ 의 $W$ 와 $b$ 를 조정해서 cost가 줄어드는 것을 볼 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
